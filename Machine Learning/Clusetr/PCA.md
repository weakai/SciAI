---
title: PCA
---

 降维算法由很多，比如 PCA ，ICA，SOM，MDS， ISOMAP，LLE 等，在此不一一列举。PCA 是一种无监督降维算法，它是最常用的降维算法之一，可以很好的解决因变量太多而复杂性，计算量增大的弊端。

## PCA 算法思路

1. 去掉数据的类别特征（label），将去掉后的 d 维数据作为样本
2. 计算 d 维的均值向量（即所有数据的每一维向量的均值）
3. 计算所有数据的散布矩阵（或者协方差矩阵）
4. 计算特征值（e1 , e2 , e3 , .... ed）以及相应的特征向量（lambda1，lambda2，...lambda d）
5. 按照特征值的大小对特征向量降序排序，选择前 k 个最大的特征向量，组成 d*k 维的矩阵W（其中每一列代表一个特征向量）
6. 运行 d*K 的特征向量矩阵W将样本数据变换成新的子空间。

- 注意1：虽然 PCA 有降维的效果，也许对避免过拟合有作用，但是最好不要用 PCA 去作用于过拟合。
- 注意2：在训练集中找出 PCA 的主成分，（可以看做为映射mapping），然后应用到测试集和交叉验证集中，而不是对所有数据集使用 PCA 然后再划分训练集，测试集和交叉验证集。

## PCA 算法优缺点总结

仅仅需要以方差衡量信息量，不受数据集以外的因素影响
各主成分之间正交，可消除原始数据成分间的相互影响的因素
计算方法简单，主要运算时特征值分解，易于实现

主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强
方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响

#### 4.2  PCA算法缺点

-   1，主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强
-   2，方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

## 参考

[Python机器学习笔记：主成分分析（PCA）算法 ](https://www.cnblogs.com/wj-1314/p/8032780.html)
[Python机器学习笔记：使用scikit-learn工具进行PCA降维](https://www.cnblogs.com/wj-1314/p/10144700.html)