---
title: 贝叶斯优化
subtitle: BayesianOptimization
---

穷举搜索 Grid Search 效率太低
随机搜索比穷举搜索好一点
目前比较好的解决方案是贝叶斯优化

贝叶斯优化的核心是将概率思想融入到代理优化思想之中。

代理优化是使用一个代理函数或近似函数来通过采样估计目标函数。

贝叶斯优化是通过将代理函数表示成概率分布而将代理优化放入一个概率框架中，然后再使用新信息更新这个分布。

获取函数则是用于基于已知的先验，评估利用空间中的某个特定点得到「好」结果的概率。其关键在于探索与利用的平衡。

贝叶斯优化的主要使用场景是目标函数评估成本高的任务，比如超参数调节。有一些用于该任务的软件库，比如 HyperOpt。

## 背景

贝叶斯优化（Bayesian Optimization，以下简称 BO）开始被好多人用来调神经网络的超参，在这方面 BO 最大的优势是 sample efficiency，也就是 BO 可以用非常少的步数（每一步可以想成用一组超参数来训练你的神经网络）就能找到比较好的超参数组合。

另一个原因是 BO 不需要求导数（gradient），而正好一般情况下神经网络超参的导数是求不出来的。这两个原因导致 BO 成为了如今世界上最好的调超参的方法。

其实 BO 不是只能用来调超参的，因为他是一个非常general 的 gradient-free global optimization 的方法，所以他的适用场景一般有两个特点

1. 需要优化的 function 计算起来非常费时费力，比如上面提到的神经网络的超参问题，每一次训练神经网络都是燃烧好多GPU的
2. 优化的 function 没有导数信息。所以如果你遇到的问题有以上两个特点的话直接闭着眼睛用 BO 就行了。当然了这么说还是有点太暴力了，因为有一些特殊的问题结构也会影响 BO 的效果，比如需要调的参数太多的话（对应 high-dimensional BO 的问题），或者参数里面有太多 discrete parameter 的话 BO 的效果都会受影响，当然了这两种场景也是 BO 目前的 open problems 之二。

## 贝叶斯优化算法

特点
- 采用高斯过程，考虑之前的参数信息，不断地更新先验；网格搜索未考虑之前的参数信息
- 贝叶斯调参迭代次数少，速度快；网格搜索速度慢,参数多时易导致维度爆炸
- 贝叶斯调参针对非凸问题依然稳健；网格搜索针对非凸问题易得到局部最优

https://github.com/fmfn/BayesianOptimization

## 基于代理的优化方法

这个代理函数，其通常表示成高斯过程，这可被看作是一种掷骰子过程。
为什么要使用高斯过程来建模代理函数，而不是使用其它曲线拟合方法？这是因为高斯过程本质上就是贝叶斯模式的。高斯过程是一种概率分布，就像一个事件的最终结果分布一样（比如掷硬币的 1/2 概率），只不过高斯过程是在所有可能的函数上的分布。

举个例子，我们也许可以定义当前的数据点集可由函数 a(x) 表示 40%、由函数 b(x) 表示 10% 等等。通过将代理函数表示成概率分布，可使用新信息，通过固有的概率贝叶斯过程来完成更新。也许当新信息被引入时，a(x) 函数又只能表示 20% 的数据了。这样的变化受贝叶斯公式的约束。

这会使得类似于新数据点的多项式回归拟合这样的目标难以完成甚至不可能完成。

表示成先验概率分布的代理函数会通过一个「获取函数（acquisition function）」而更新。这个函数负责在探索与利用权衡的基础上，对提议的新点进行测试。

利用的目标是采样代理模型能很好地预测目标函数的地方。这会用到已知的有潜力的位置。但是，如果我们已经充分探索了某个特定的区域，再继续利用已知信息也收益不大了。

探索的目标是采样不确定度较高的位置。这能确保空间中不留下未探索的主要区域——全局最小值可能就藏在此处。

太过重视利用而不太重视探索的获取函数会让模型驻留于其发现的第一个最小值（通常是局部最小值）。反过来，重探索而轻利用的获取函数则一开始就不会留在某个最小值，不管是局部最小值还是全局最小值。因此，为了得到很好的结果，需要达到微妙精巧的平衡。

## 参考

- [贝叶斯优化(Bayesian Optimization)深入理解](https://www.cnblogs.com/marsggbo/p/9866764.html)

- [用简单术语让你看到贝叶斯优化之美](https://zhuanlan.zhihu.com/p/262259364)

