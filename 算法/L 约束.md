# L 约束

L 约束，并不只是 WGAN。它可以用在生成模型中，也可以用在一般的监督学习中。

我们揭示了 l2 正则化（也称为weight decay）与 L 约束的联系，表明 l2 正则化能使得模型更好地满足 L 约束，从而降低模型对输入扰动的敏感性，增强模型的泛化性能。

#### 谱正则化

前面我们已经表明了Frobenius范数与l2正则化的关系，而我们已经说明了Frobenius范数是一个更强（更粗糙）的条件，更准确的范数应该是谱范数。虽然谱范数没有Frobenius范数那么容易计算，但依然可以通过式(15)迭代几步来做近似。

#### 谱归一化

梯度惩罚的问题在于它只是一个惩罚，只能在局部生效。真正妙的方案是构造法：构建特殊的f，使得不管f里边的参数是什么，f都满足L约束。

事实上，WGAN首次提出时用的是参数裁剪——将所有参数的绝对值裁剪到不超过某个常数，这样一来参数的Frobenius范数不会超过某个常数，从而|f|L不会超过某个常数，虽然没有准确地实现|f|L=1，但这只会让loss放大常数倍，因此不影响优化结果。参数裁剪就是一种构造法，这不过这种构造法对优化并不友好。

简单来看，这种裁剪的方案优化空间有很大，比如改为将所有参数的Frobenius范数裁剪到不超过某个常数，这样模型的灵活性比直接参数裁剪要好。如果觉得裁剪太粗暴，换成参数惩罚也是可以的，即对所有范数超过Frobenius范数的参数施加一个大惩罚，我也试验过，基本有效，但是收敛速度比较慢。

然而，上面这些方案都只是某种近似，现在我们已经有了谱范数，那么可以用最精准的方案了：将f中所有的参数都替换为w/∥w∥2。这就是谱归一化（Spectral Normalization），在《Spectral Normalization for Generative Adversarial Networks》一文中被提出并实验。这样一来，如果f所用的激活函数的导数绝对值都不超过1，那么我们就有|f|L≤1，从而用最精准的方案实现了所需要的L约束。



## 参考

- [苏神 | 深度学习中的 Lipschitz 约束：泛化与生成模型](https://kexue.fm/archives/6051)