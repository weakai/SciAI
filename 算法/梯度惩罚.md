# 梯度惩罚

- [输入梯度惩罚与参数梯度惩罚的一个不等式](https://kexue.fm/archives/8796)

- 从形式上来看，梯度惩罚项分为两种，一种是关于输入的梯度惩罚∥∇xf(x;θ)∥2
- 另一种则是关于参数的梯度惩罚∥∇θf(x;θ)∥2

两种梯度惩罚都声称有着提高模型泛化性能的能力，那么两者有没有什么联系呢？

参数的梯度惩罚一定程度上包含了输入的梯度惩罚

SGD隐式地包含了对参数的梯度惩罚项，而式(2)则说明对参数的梯度惩罚隐式地包含了对输入的梯度惩罚，而对输入的梯度惩罚又跟Dirichlet能量有关，Dirichlet能量则可以作为模型复杂度的表征。所以总的一串推理下来，结论就是：SGD本身会倾向于选择复杂度比较小的模型。

论文提供的解决方案非常简单，假设原来的损失函数是L(θ)，现在改为L~(θ)：
L~(θ)=|L(θ)−b|+b(1)

其中b是预先设定的阈值。当L(θ)>b时L~(θ)=L(θ)，这时候就是执行普通的梯度下降；而L(θ)<b时L~(θ)=2b−L(θ)，注意到损失函数变号了，所以这时候是梯度上升。因此，总的来说就是以b为阈值，低于阈值时反而希望损失函数变大。论文把这个改动称为“Flooding”。

这样做有什么效果呢？论文显示，在某些任务中，训练集的损失函数经过这样处理后，验证集的损失能出现“二次下降（Double Descent）”，如下图：

![](https://kexue.fm/usr/uploads/2020/07/2776542712.png)

