---
title:
subtitle:
post:
modified:
tags: []
---

三种思想

- Bagging：独立(并行)集成多个模型，每个模型有一定的差异，最终综合有差异的模型的结果，获得学习的最终的结果；
- Boosting（增强集成学习）：串行集成多个模型，每个模型都在尝试增强（Boosting）整体的效果；
- Stacking（堆叠）：集成 k 个模型，得到 k 个预测结果，将 k 个预测结果再传给一个新的算法，得到的结果为集成系统最终的预测结果；

## 增强集成学习（Boosting）

- Boosting 类的集成学习，主要有：Ada Boosting 和 Gradient Boosting 两类；
- 由于每个子模型要使用全部的数据集进行训练，因此 Ada Boosting 算法中没有 oob 数据集，在使用 Ada Boosting 算法前，需要划分数据集：train_test_split；
- 每个 Ada Boosting 集成学习算法的模型只使用一个基本算法进行训练子模型；
- 相对于集成学习方法，决策树算法、SVM 算法、逻辑回归算法等，称为基本的学习方法；

## 参考

[机器学习：集成学习（Ada Boosting 和 Gradient Boosting）](https://www.cnblogs.com/volcao/p/9490651.html)
