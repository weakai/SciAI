---
title: transfomerCPI
url: https://github.com/lifanchen-simm/transformerCPI
---

![](https://github.com/lifanchen-simm/transformerCPI/raw/master/model.png)

## 1.介绍

识别化合物蛋白相互作用(CPI)是药物发现和化学基因组学研究中的一项关键任务，但三维(3D)结构缺失的蛋白质占潜在生物靶点的很大一部分，这就需要研究仅利用蛋白质序列信息预测CPI的方法。

**然而，基于序列的 CPI 模型可能会面临一些特定的陷阱，包括使用不恰当的数据集、隐藏的配体偏差以及不恰当地分割数据集，从而导致高估了模型的预测性能。**

为了解决这些问题，本文作者构建了专门用于 CPI 预测的新数据集，提出了一种新型的 transformer 神经网络称为 TransformerCPI，并引入了更严格的标签反转实验来测试模型是否学习了真实的相互作用特征并对预测结果进行了可视化操作及分析。

### 1.1 数据集陷阱

最近，谷歌的研究人员提出了机器学习应避免的三个陷阱

1. Using inappropriate datasets（使用不合适的数据集）

数据是深度学习模型的核心基础，在某种程度上，模型学习的内容主要取决于所输入的数据集，而不合适的数据集会使模型容易偏离目标。CPI 建模的总体目标是用蛋白质和配体特征的抽象表示形式，预测不同蛋白质与不同化合物之间的相互作用。因此，相互作用的信息是模型应从数据集中学习的关键要素。以前基于化学基因组学的 CPI 预测模型使用不合适的数据集来构建深度学习模型，例如 DUD-E 数据集和 Human 数据集，收集 DUD-E 数据集的目的是训练基于结构的虚拟筛选。此外，DUD-E，MUV，Human 和 BindingDB 中的大多数配体仅在一类中出现，并且阴性样本的生成可能引入了无法检测到的噪声。这些数据集可以只通过配体信息分开，无法不能保证模型学习到蛋白质信息或相互作用的特征。

2. Hidden ligand bias（隐藏的配体偏差）

深度学习系统通常被称为黑匣子模型，因此很难解释该模型确切学习了什么以及该模型基于什么进行预测。在验证集和测试集上获得更好的性能通常意味着研究结束了，很少人会花费精力来进一步研究模型是否以预期的方式学习。在 DUD-E 和 MUV 数据集中已经报道了隐藏的配体偏差问题，这引起了药物设计领域的广泛关注。隐藏的配体偏差是指模型主要根据配体 pattern 进行预测而不是根据相互作用的特征，从而导致理论建模和实际应用之间有偏差。比如，基于结构的虚拟筛选，基于 3D-CNN 的模型以及在 DUD-E 数据集上训练的其他模型。我们想知道基于化学基因组学的 CPI 建模是否面临类似的问题，从而以 Human 数据集为例，重新研究了先前的 CPI-GNN 典型模型，以研究隐藏的配体偏差的潜在影响。下图 A 显示了在人类数据集上训练的 CPI-GNN 模型的权重分布图。
![](https://img-blog.csdnimg.cn/20210303212614987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTUwNTgyMA==,size_16,color_FFFFFF,t_70#pic_center)

用于提取蛋白质特征的 CNN 块的权重明显集中在零，这表明在进行预测时很少考虑蛋白质信息。相反，用于提取复合特征的 GNN 块的权重分布宽而平坦。因此，作者认为配体信息比蛋白质信息更重要。在上图 B 中阐明了仅配体信息的进一步训练及其与原始模型的比较，其中将数据集随机划分了 10 次，并在 10 次划分中对两个模型进行了评估。在两个样本的 t 检验中，AUC 分布差异的 p 值大于 0.05，这表明单独使用配体信息可能会比使用配体和蛋白质信息的原始 CPI-GNN 模型取得更具有竞争力的性能。因此，CPI-GNN 模型主要学习如何对不同的配体进行分类，而不是对不同的 CPI 对进行分类，这增加了预测错误的风险。这些结果突出了配体 pattern 可能误导模型的可能性。

3. Splitting dataset inappropriately（不恰当地划分数据集)

隐藏配体偏差的风险很难消除，但可以降低。通常，机器学习研究人员会将数据随机分为训练集和测试集。然而，在随机划分的测试集上使用传统的分类测量，我们并不清楚模型是否学习了真实的相互作用特征或其他的隐藏变量。因此，测试集应该根据建模的实际目标和应用场景进行设计，而不是简单地随机划分。

为了解决上面 3 个问题，作者提出了一个名为 TransformerCPI 的新型 Transformer 神经网络，构建了针对 CPI 建模的新数据集，并引入了更为严格的标签反转实验，以评估数据驱动模型是否陷入 AI 的常见陷阱里。结果，TransformerCPI 在三个公共数据集和两个标签反转数据集上均取得了最佳性能。此外，作者还通过将注意力权重映射回蛋白质序列和化合物分子，进一步研究了 TransformerCPI 的可解释性，以揭示其潜在的预测机制，结果还证实了 TransformerCPI 的自注意力机制可用于捕获所需的相互作用特征。作者希望这些发现可以引起研究者的注意，以提高 CPI 建模的泛化和解释能力。

## 2.方法

#### 蛋白质的处理

n-gram: 3-mer, 有重叠的划分

-> word2vec (Skip-Gram + CBOW): 单词转为实值嵌入

-> word2vec 在 UniProt 中所有的人类蛋白序列作为语料库: 得到一个 100 维的实值嵌入表示向量

#### 化合物的处理

RDKit: 原子的特征, dim=34

-> GCN: 分子的表示。

#### TansformerCPI 模型

蛋白质序列是编码器的输入，而原子序列是解码器的输入，解码器的输出是包含有与原子序列相同长度的相互作用特征向量。

考虑到原子特征向量的顺序对 CPI 建模没有影响，因此在 TansformerCPI 模型中删除了原本存在于 Transformer 模型中的位置嵌入。

编码器的部分，由于传统的 Transformer 结构通常需要大量语料库进行训练，并且在规模较小的数据集上很容易出现过拟合，因此TransfermorCPI 使用一维卷积的门控卷积网络 Conv1D 和门控线性单元(GLU)代替了传统 Transformer 编码器中的 self-attention 层，这样的改变使得 TransformerCPI 显示出更好的性能。

TransformerCPI 的解码器由 self-attention 层和 feed farword 层组成，这点和传统的 Transformer 模型一致。TransformerCPI 通过解码器学习相互作用特征，解码器的关键技术是 multi-headed self-attention（多头自注意力），用于提取编码器和解码器之间的交互信息。

值得注意的是，原 Transformer 模型用于解决序列预测的任务，并利用掩码操作遮挡解码器中单词的下游上下文，这一点不利于解决 CPI 问题，因此，论文作者对解码器的掩码操作进行了修改，以确保 TransformerCPI 对整个序列是可访问的，这是将 Transformer 模型从自回归任务转换为分类任务的最关键的修改之一。



解码器的输出是包含有与化合物序列相同长度的相互作用特征的相互作用序列
$$
x_1, x_2, \cdots,x_a
$$
，然后通过公式计算权重：
$$
\alpha_i \frac{e^{x^\prime_i}}{\sum_{i=1}^{a}e{x^\prime_i}}
$$
再通过加权求和得到预测的相互作用向量:

最后将相互作用向量输入到一系列的全连接层和非线性激活函数得到预测值，以此判断化合物-蛋白质之间是否会发生相互作用。作为一种传统的二分类任务，使用二元交叉熵损失来训练 TransformerCPI 模型。



## 3.数据集

### 3.1 公开数据集

作者在之前的三个基准数据集——Human 数据集，C.elegans 数据集和 BindingDB 数据集上对模型进行了比较。Human 数据集和 C.elegans 数据集包括 DrugBank 和 Matador 的正 CPI 对以及通过系统筛选框架获得的高度可信的负 CPI 样本。

1. Human 数据集包含 1052 种独特化合物与 852 种独特蛋白质之间的 3369 种正的相互作用
2. C.elegans 数据集包含 1434 个独特化合物和 2504 个独特蛋白质之间的 4000 种正相互作用，并且训练集，验证集和测试集被随机拆分。
3. BindingDB 数据集包含来自公共数据库的 39747 个阳性示例和 31218 个阴性示例。 BindingDB 的训练集，验证集和测试集都经过了精心设计，并且测试集包括 CPI 对，其中测试集中的蛋白质或配体在训练集中没有出现过。因此，BindingDB 数据集可以评估模型对未知配体和蛋白质的泛化能力。

### 3.2 标签反转数据集

为了构建专门用于基于化学基因组学的 CPI 建模的数据集，作者遵循两个规则：

1. 从经过实验验证的数据库中收集 CPI 数据
2. 每个配体应同时存在于两类中。

之前的许多研究都是通过 CPI 对的随机交叉组合或使用基于相似度的方法来生成负样本的，这可能会引入意想不到的噪声和不被注意的偏差。
首先，作者从 GLASS 数据库构建了一个 GPCR 数据集。 GLASS 数据库提供了大量经过实验验证的 GPCR-配体关联，这满足了第一个规则。 
其次，作者基于 KIBA 数据集构建了 Kinase 数据集。KIBA 数据集结合了各种生物活性类型，并消除不同生物活性类型之间的不一致，这大大降低了数据集中的偏差。KIBA 数据集包含从 ChEMBL 和 STITCH 收集的 467 个靶标和 52498 个配体，那确保了 KIBA 中的数据是经过实验验证的。鉴于大多数配体只出现一次，作者效仿一项工作（SimBoost）对原始 KIBA 数据集进行过滤，获得 229 种蛋白质和 2111 种化合物。然后，作者使用 KIBA 的阈值 12.1 将数据集分为正样本集和负样本集，选择同时存在于正样本集和负样本集的化合物，共产生 1644 种化合物，229 种蛋白质和 111237 CPI。

![img](https://pic3.zhimg.com/80/v2-72439d1232e3a1e9f7b92f097edac846_720w.jpg)

标签反转实验具体而言是这样的，在训练集中的配体只出现在一个类的样本(正样本或负样本)中，而在测试集中该配体只能出现在相反的一类中。通过这种方式，迫使模型利用蛋白质信息理解相互作用模式并通过学习（在测试集中）做出相反的预测。作者认为，如果一个模型只学习到配体的特征，它不太可能在测试集中做出正确的预测，因为对于训练集中的配体而言，它在测试集中的标签是相反的。因此，这个标签反转实验是专门设计用来评估基于化学基因组学的 CPI 模型并反映隐藏的配体偏置在模型训练过程中产生了多大的影响。

![img](https://pic3.zhimg.com/80/v2-7d38a7150bb5518eb40111f46340175e_720w.jpg)



## 4.对比实验及结果

在 Human 数据集上的实验对比：

![img](https://pic2.zhimg.com/80/v2-6d9fc2d9bb67a47ef097bdd0fdefecb5_720w.jpg)



在 C.elegans 数据集上的实验对比：

![img](https://pic1.zhimg.com/80/v2-537e9e2bf0143f70990d304c70abb2d8_720w.jpg)



在 BindingDB 数据集上的实验对比：

![img](https://pic3.zhimg.com/80/v2-1cbc4eab1e26d32b914f44ce641ccdc6_720w.jpg)



**在标签反转数据集上的实验对比：**

在反转实验中作者选择了 AUC 和 PRC 作为评价指标。公平起见，每个对比模型都进行了微调以达到较佳效果。通过实验结果可以看到，所有模型在验证集上都取得了相似的性能，但在测试集上，这些模型之间存在较大的性能差距。在 GPCR 数据集上，TransformerCPI 在 AUC 和 PRC 方面都优于 CPI-GNN、GraphDTA 和 GCN，表现出更强的捕获化合物和蛋白质之间相互作用特征的能力。在 Kinase 数据集上，TransformerCPI 的表现也很好，其他对比模型的 AUC 均小于 0.5。根据这些实验结果，作者认为在所有参考模型中，GPCR和Kinase数据集的配体偏置可能带来不可忽视的影响。GraphDTA 和 GCN 在 GPCR 数据集上表现良好，接近于 TransformerCPI，但在 Kinase 数据集上表现较差。而 TransformerCPI 在这两个数据集上表现最好，显示了其鲁棒性和泛化能力。以往 CPI 模型的通常做法是，分别独立提取配体和蛋白质的特征，然后将这两个特征向量拼接起来作为输入特征。**为了验证 Transformer 编码器-解码器结构的作用，作者接下来评估了 TransformerCPI-ablation 模型，该模型利用传统的拼接向量替代了 Transformer 的解码器。通过结果可以看出，TransformerCPI-ablation 模型显著降低了 TransformerCPI 的性能，这说明 self-attention 机制和编码器-解码器架构在提取相互作用特征中确实起到了关键作用。**



![img](https://pic4.zhimg.com/80/v2-5f17d3bed1adae3959bc5ab97dfe9327_720w.jpg)



在比较 GPCR 数据集和 Kinase 数据集的结果时，同样值得注意的是 TransformerCPI、GraphDTA 和 GCN 在 GPCR 上的表现要比 Kinase 好得多，作者认为，产生这种差异的原因可能有以下两个：一是 GPCR 和 Kinase 的数据分布不同，导致两个数据集之间存在性能差距。二是 GPCR 的序列特征更容易被 TransformerCPI 学习。

#### 预测结果可视化及分析

将注意力权重映射到化合物原子上，揭示了 TransformerCPI 学习到的知识，模型在面对不同的化合物蛋白质对时，会关注不同的原子，然后学习如何正确地将化合物蛋白质对分为相互作用和非相互作用两类。

为了进一步验证原子注意力权重的意义，实验团队选择了复方吩噻嗪来解释 TransformerCPI。吩噻嗪是一种以多巴胺受体(DA)为靶点的经典抗精神病药物，其构效关系（SAR）已被深入研究。如图所示，通过注意力权重突出的吩噻嗪的原子与吩噻嗪的 SAR 一致，证实了 TransformerCPI 能够捕捉到真实的相互作用特征，并找出与蛋白质相互作用的关键原子。



![img](https://pic1.zhimg.com/80/v2-cf8ef22d82f1ef239e85db69970b43b4_720w.jpg)



在解释了原子的注意机制之后，实验团队还研究了蛋白质序列的注意力权重，以确定蛋白质序列的哪些部分成为注意焦点。通过注意力权重 TransformerCPI 可以大致推测配体与 GPCR 家族的结合位点是在胞外区域还是在跨膜区域，并检测激酶家族的 ATP 结合口袋。作者以组胺 H1 受体、5-HT1B 受体和丝裂原活化蛋白激酶 8 (MAPK8)及其相应的活性物质为例，蛋白质中具有从 TransformerCPI 中提取的高注意力权重的区域用紫色标出。这些结果表明 TransformerCPI 可以推测新的化合物是结合到 GPCR 靶标的胞外区域还是在跨膜区域，这在药物设计中是有用的，特别是当 GPCR 靶标的三维结构未知时。同时，作者也注意到，高亮显示的区域可能会包含更广泛的区域，并不对应于准确的结合位点残基。为了解决这一问题，作者认为需要加入更多具有精确标注的高质量数据，新的基于序列的深度表示学习也可能有助于更好地对结构信息进行编码和解码。



![img](https://pic2.zhimg.com/80/v2-ad16f3d712102feedb9ac1e8e00f8901_720w.jpg)



## 5.总结

在这项工作中，作者修改了传统的 transformer 架构使其更适合处理基于序列的 CPI 分类任务，该模型在三个基准数据集上表现出了高性能，并设计了更严格的标签反转实验作为基于化学基因组的 CPI 建模的新测量方法。与其他模型相比，TransformerCPI 在新测量实验中显著提高了性能，表明它可以学习到期望的相互作用特征，降低隐藏配体偏差的风险。最后，通过将注意力权重映射到蛋白质序列和化合物原子，作者还研究了模型的可解释性，这可以帮助确定预测是否可靠。作者在文章中有一段话说的很好，"实验设计在深度学习中起着重要的作用，应该更多地关注于评估深度学习模型真正学习到了什么。因此，在未来深度学习的发展中，不仅需要新的深度学习方法，还需要新的验证策略和实验设计。"