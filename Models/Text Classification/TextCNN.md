# TextCNN

2014 年，Yoon Kim 针对 CNN 的输入层做了一些变形，提出了文本分类模型 textCNN。
与传统图像的 CNN 网络相比, textCNN 在网络结构上没有任何变化(甚至更加简单了), 从图一可以看出 textCNN 其实只有一层卷积,一层 max-pooling, 最后将输出外接 softmax 来 n 分类。

与图像当中 CNN 的网络相比，textCNN 最大的不同便是在输入数据的不同：
1）图像是二维数据, 图像的卷积核是从左到右, 从上到下进行滑动来进行特征抽取。
2）自然语言是一维数据, 虽然经过 word-embedding 生成了二维向量，但是对词向量做从左到右滑动来进行卷积没有意义.
比如 "今天" 对应的向量[0, 0, 0, 0, 1], 按窗口大小为 1* 2 从左到右滑动得到[0,0], [0,0], [0,0], [0, 1]这四个向量, 对应的都是"今天"这个词汇, 这种滑动没有帮助.

TextCNN 的成功, 不是网络结构的成功, 而是通过引入已经训练好的词向量来在多个数据集上达到了超越 benchmark 的表现，进一步证明了构造更好的 embedding, 是提升 nlp 各项任务的关键能力。