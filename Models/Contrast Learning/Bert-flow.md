---
title: Bert-flow
---

## 0 Base

### 各向异性

举个例子，一些电阻原件，正接是良导体，反接是绝缘体或者电阻很大，沿不同方向差异很大。在 Bert 出来的向量中表现为，用不同的方式去衡量它，他表现出不同的语义，差别很大，也就是不能完整的衡量出 Bert 向量中全部语义信息。

## 1 Introduction

Bert 预训练模型问世以来，改变了很多 NLP 任务的习惯。很多 NLP 任务都会直接在 Bert 的基础上做一个 fine-tune，构建一个 baseline 模型，既快捷也使用，但 Bert 在做文本表达任务时也有一些明显的缺点。

### 1.1 Bert 存在的两个问题

1. Bert encode 出来的向量表达具有各向异性。
   - 表现: 向量会不均匀分布，且充斥在一个狭窄的锥形空间
   - 限制了句子向量的语义表达能力: 因此当采用计算 Bert encode句子相似度，采用 cos 或 dot 是无法很好的衡量出两个句子的相似度的，因为Bert向量不是基于一个标准正交基得到的。
2. 分布不均匀，低频词分布稀疏，高频词分布紧密。
   - 高频词与低频词之间的相识度也就没法计算
   - 高频词，频次高，主宰句子表达

既然问题已经定义清楚了，那这么卷的NLP领域肯定会有解决方法。

### 1.2 解决方法

#### 1.2.1 流式变换 Bert-flow

Bert-flow 在文中详细总结并分析了以上两个问题之后，思想也比较直接，既然Bert 出来的 embedding 向量存在各向异性，进而也产生了分布不均匀问题，那我就采用一个变换，将 Bert encode 的句子表达转换到一个各向同性且分布较均匀的空间。而标准的高斯分布刚好是一个各向同性的空间，且是一个凸函数，语义分布也更平滑均匀。

![](https://pic1.zhimg.com/80/v2-cad5e8e71ad9b325ae01d430383f2e9c_720w.jpg)

于是,变换就来了。Bert-flow 采用一种流式可逆变换，记为

![](https://pic2.zhimg.com/80/v2-1174aca26ef24aadfe844f96954a0139_720w.png)

其中 $u$ 就是 Bert 空间向量（observe space），$z$ 即是高斯空间向量（latent space）， $f: Z \rightarrow U$ 则是一个可逆变换，这个变换也是模型需要学出来的。

最后，通过无监督的方式 maximize 这个优化目标，得到可逆的映射变换 $f$，这其实就是在 Bert 预训练的模型后接了一个f low 变换的模型，让其继续预训练，学出 flow 变换，从而完成向量空间的 transform。

虽然这种 Bert-flow 方法站在现在的角度看还是复杂了些，但 Bert-flow 这篇文章完整的梳理出了问题脉络，并对这些问题通过实验做出了完整的剖析与实验佐证，读起来很顺畅，推荐大家看语义向量表达的问题时可以从这篇看起。

#### 1.2.2 带白化处理的 Bert-whitening

在解决问题的路上，方法总是会朝着简单的方向进行迭代更新。于是 Bert-whitening 带着更简单的变换方法来了。

既然是要做一个向量的转换，那有没有简单一点的方法，直接校正句向量？ Bert-whitening 提出通过一个白化的操作直接校正局向量的协方差矩阵，简单粗暴，也达到了与 Bert-flow 差不多的效果。

方法很简单，思路也很直接，就是对现存的 Bert 向量空间分布进行白化操作。

在工程上应用节省了内存，也提升了性能。

## Links

- [文本表达进击:从 Bert-flow 到 Bert-white、SimCSE](https://zhuanlan.zhihu.com/p/430580960)

