# Bert

- BERT = ELMo 的双向结构（编码器中，更好的抽取信息） + GPT Transformer
- 通过训练 Masked Language Model 和预测下一句任务得到的模型。
- Google 在 2018 年 10 月发布的语言表示模型
- 在 NLP 领域横扫了 11 项任务的最优结果，可以说是现今最近 NLP 中最重要的突破。
- 位置编码可以学习
- 输入采用句子对
- 针对微调设计

发起军备竞赛，数据集往无限大去。

两种训练方式

1. 带掩码的语言模型，每次随机将一些词元换成`<mask>`。因为微调任务中不出现 `<mask>`。
   80% 变成 `<mask>`
   10% 变成随机
   10% 保持原样
2. 下一个句子预测
   50% 句子对不变
   50% 下半句从其他句子中复制过来，作为负样本

## BERT 模型介绍

Devlin 等人提出了一种**预训练**语言模型 BERT。**该模型在 NLP 的多个任务上的评估表现均比当时最优模型有较大提升，可以说 BERT 的出现使得 NLP 的研究进入到一个全新的阶段。**当然，BERT 的出现也不是天马行空，而是有迹可循的。前文提到 ELMo 预训练语言模型作为词向量工具表现十分不错。但是从 GPT 模型的实验结果来看，Transformer 结构的特征提取能力比 LSTM 更强。GPT 模型本身为文本生成任务而设计，它采用的是单向 Transformer 结构，因此 GPT 的适用范围存在局限。

除此，与 ELMo 的根据上下文信息预测当前词的训练目标不同，BERT 有两个训练目标。第一个是掩码语言模型，BERT 会随机将一段文本中 15%的词替换为 [Mask] 标记，然后让模型预测被替换位子上原来是什么词。同时这 15% 的替换工作中又只有 80%的词会被真正替换为 [Mask]，10%的词会被其他词替代，10% 的词保持不变。BERT 的另一个训练目标是下一个句子预测，它指的是 BERT 在预训练阶段，需要将给定的两个句子 A 和 B 做一个判断，即 B 是否为 A 的下个句子。训练时，有 50% 的数据中 A 和 B 是两个顺序连贯的句子，而另外 50%的数据中 B 被随机替换为其他句子。BERT 之所以设计这样一个任务，是考虑到 NLP 中有一些任务涉及到句子关系匹配，而大多数关于 NLP 的神经网络模型的特征提取工作都是基于字词粒度级别的，而无法到达句子级别(除非特别设计)。因此，BERT 通过引入该任务使得它兼顾了字词粒度及句子级别粒度的特征提取，这也促使了它能够胜任 NLP 中大多数任务。

![BERT 预训练模型结构图](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2021-11-08_13-59.png)

BERT 结构中虽然使用了 Transformer 结构，但是只使用了 Transformer 中的编码器部分，因为 Transformer 完整结构是为机器
翻译任务设计的，它包括了双向编码的编码器和单向解码的解码器。与 ELMo 一样，BERT 的预训练阶段只是为了具备更强的特征提取能力及更准确的词向量表示，它只使用了编码器部分。

BERT 版本

1. Base
   满足基本要求，训练时间短，显存需求小，对实验环境要求低。 
2. Large
   更深的网络，支持更多的预训练数据，因此提取的文本特征更多，实验表现好。

## 从 Word Embedding 到 Bert 模型—自然语言处理中的预训练技术发展史

<https://zhuanlan.zhihu.com/p/49271699>

### 为什么会 Bert 会火

把Bert当做最近两年NLP重大进展的集大成者

- 没有有重大的理论或者模型创新
- 关键：效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。
- 另一个关键：Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果

自从深度学习火起来后，预训练过程成为图像或者视频领域的常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。

预训练的好处，大大降低计算代价，通用性好，万金油，一个跑好了哪哪都能用。

预训练的可行性：底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。

借鉴图像处理，NLP 的预训练：
word embedding 其实就是 NLP 里的早期预训练技术。当然也不能说 word embedding 不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。



