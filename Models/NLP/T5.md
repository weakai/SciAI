---
title: T5
date: 2022-4-17
update: 2022-7-19
---

T5 的基本思想是将每个 NLP 问题都视为 “text-to-text” 问题，即将文本作为输入并生成新的文本作为输出，这允许将相同的模型、目标、训练步骤和解码过程，直接应用于每个任务。

## 位置编码

- Transformer: 正余弦函数的位置编码
- Bert: 学习到的位置嵌入
- T5: 相对位置嵌入
  - 根据 self-attention 机制中的 key 和 query 之间的偏移量生成不同的学习嵌入。

## Links
