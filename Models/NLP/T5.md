---
title: T5
date: 2022-4-17
update: 2022-7-19
---

T5 的基本思想是将每个 NLP 问题都视为 “text-to-text” 问题，即将文本作为输入并生成新的文本作为输出，这允许将相同的模型、目标、训练步骤和解码过程，直接应用于每个任务。

> ALBERT 凳子还没坐热，GLUE 就又换了老大，Google T5 (Text-to-Text Transfer Transformer)大力出奇迹，威震天在角落默不作声。

#### 写在前面

谷歌用一篇诚意满满(财大气粗)的基于实验的综述，试图帮助研究者们「拨开云雾见光明」。论文十分适合该领域的初学者通读，写的十分友好，不过由于涉及到的模型/技术很多，所以遇到不熟悉的部分还是需要自行了解。

论文的摘要以及 1-2 部分介绍了 Motivation、数据集的预处理操作、待评估的下游任务以及这些任务为了适应 Text-to-text 框架所需的相关处理。

第 3 部分是最长的实验部分，3.1 介绍了 Text-to-text  框架下使用的基线模型；3.2 介绍了现有的不同模型架构，并对无监督预训练的降噪目标和传统语言建模目标进行实验比较；3.3  对无监督降噪目标进行了细致研究，如图 5 所示，先对前缀语言建模、BERT-style 和 Deshuffling 三种方法进行选择(表 3  提供了各种方法的输入与输出)，接着对 BERT-style 的降噪目标进行修改，对 MASS-style、Replace corrupted  spans 和 Drop corrupted spans 进行实验，最后对 corruption rate 和 corruption span  length 依次实验；3.4 比较了使用不同方法过滤后的 C4 数据集以及常用的预训练数据集，并对预训练数据集(是否重复)进行试验；3.5  对训练方式进行了探究，Baseline  是在无监督降噪任务上对模型的所有参数进行预训练，然后对在每个下游任务上分别对其进行了微调，并使用模型的不同参数设置(检查点)来评估性能，本节进一步的对微调方法、多任务学习方法以及两者的结合进行了实验探究；3.6 对增加计算成本获取性能提升的各种方式进行了探究；3.7 对第 3 部分进行了总结与细节分析。

第 4 部分是基于本文的实验研究以及Text-to-text 框架，对系统实验进行了总结，并对未来的研究进行展望。

#### 数据

Colossal Clean Crawled Corpus (C4)

作者选取了Common Crawl数据集，这个数据集每周大约爬取 20 TB 的 WEB 数据。虽然数据集已经抽取了文本，但实际上并不干净，里面还包含了很多非自然语言的东西，比如错误消息、菜单、重复文本，用过脏数据的同学一定深有体会。于是本文对数据进行了比较细致的处理：

- 只取结尾有标点的句子
- 去掉包含脏话的网页
- 有很多页面包含"enable Javascript"的提示，去掉包含 Javascript 的句子
- "lorem ipsum"是一个测试网页排版的拉丁文，去掉包含这个占位符的网页
- 去掉包含代码片段的网页
- 以三句为一个片段进行去重
- 去掉非英文的网页

经过上述处理后最终生成了 750 GB 的数据集 C4，并且在 TensorFlow Datasets开源了。

#### 任务及数据格式

**任务**

机器翻译、问答、生成式摘要、文本分类(单句&双句)

**数据格式**

输入：参考 GPT2，直接把任务名称当作 prefix 和输入拼在一起

输出：分类任务(如推断)，需要输出"entailment", "neutral", "contradiction"这三种文本，否则都算错；回归任务输出 str 类型的浮点数。

#### 训练

**预训练**

- 参考 SpanBERT，mask 掉 15%，平均长度为 3 的 span
- 训练更长步数，1 百万步*1024个样本
- 使用 Multi-task 预训练，即混入在无监督数据中混入一定比例的任务数据

**精调**

- 也是 Multi-task，将所有 GLUE/SuperGLUE 的数据拼在一起变成精调一个 task，减少过拟合，但同时也会牺牲一些精度
- batch size 减小到 8
- 其实最后同时进行了多任务精调和单独精调，根据 dev 集选择最好的结果

**解码**

大部分使用 Greedy decoding，对于输出句子较长的任务使用beam search

## 0. Abstract

迁移学习是一种在自然语言处理中强大的技术，模型首先要针对数据丰富的任务进行**预训练**，然后再针对下游任务进行**微调**。本文通过引入统一的框架来探索NLP迁移学习技术的前景：将问题都转换为 text-to-text 格式，并在数十种语言理解任务研究比较了预训练目标，架构，未标记的数据集，迁移方法和其他因素。结合实验所得以及 C4 数据集，在许多基准上获得了最新的结果，这些基准涵盖了摘要，问题回答，文本分类等等。



## 位置编码

- Transformer: 正余弦函数的位置编码
- Bert: 学习到的位置嵌入
- T5: 相对位置嵌入
  - 根据 self-attention 机制中的 key 和 query 之间的偏移量生成不同的学习嵌入。

## Links

- [【长文详解】T5: Text-to-Text Transfer Transformer 阅读笔记](https://cloud.tencent.com/developer/article/1537682	)
- [【NLP】Google T5速读](https://jishuin.proginn.com/p/763bfbd31391)
