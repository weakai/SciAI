---
title: transformer
---

## NLP

NLP 两个热点任务：文本摘要和机器翻译

其他任务：如文本分类（垃圾邮件分类）、舆情分析、自动文摘、机器翻译、智能问答等等。

NLP 属于一个较大的研究领域，其下的子任务有许多。按照输出结构的不同
可以划分以下几类： 
1.序列标注：分词、词性标注、命名体识别等； 
2.分类任务：文本分类、情感分析等； 
3.句子关系判断：机器问答、文本相似度计算等； 
4.生成式任务：文本摘要、机器翻译等。 

生成目标文本的过程类似于人类创作的过程，该任务更具挑战性。
同时生成式任务相关的文本摘要、机器翻译两个应用的实用性更强。

### 国外研究

国外早期对于自动文本摘要技术的研究主要以统计学基础为主。

Luhn 在 1958 年发表的论文中提出，根据句子中词的频率高低给句子打分，选出得分较高的几句话组成摘要文本。

Baxendal 等人则通过句子特征入手，一段文本中的不同位置的句子的重要性是不同的，如文章开头及文末通常会给出一些总结性的信息。因此他希望以此为依据从原文中抽取句子组成摘要。 

Salton 等人提出了 TF-IDF 关键词提取算法，消除停用词对关键词提取的影响。

> TF 是指每个词在文档中出现的频率，频率越高表示该词在文档中越重要；IDF 是指语料库中所有文档数量除以出现过该词的文档数量，然后再取对数。IDF 意味着该词虽然在当前文档出现频率较高，但是在其他文档中也多次出现，那么该词可能为一些对句子无意义的停用词。通过 TF、IDF 两个权重相乘来计算一个词的重要性，TF-IDF 相较于直接使用词频更加合理，因为它可以排除一些停用词的影响。TF-IDF 关键词提取算法的提出，也间接改善了前面提到的通过关键词提取摘要的方法。 

Kupiec 等人提出使用机器学习中的朴素贝叶斯分类模型对源文本中的每个句子做二分类，即该句子属于摘要或不属于，再将分类标签为属于摘要的句子进行组合，得到最终的摘要。

Osborne 等人基于文本分类器思想，使用线性判别器代替朴素贝叶斯进行是否属于摘要的二分类判断工作。

Svore 等人则使用神经网络模型作为摘要句子分类器。 

 Cho 等人在 2014 年提出 Seq2seq 模型，又称作 Encoder-Decoder 用于解决神经网络模型中输入与输出序列长度不等问题的模型。该模型最早用于机器翻译任务。

Rush 等人将此结构应用于自动文本摘要任务，他们使用卷积神经网络(CNN)结构并结合 attention 机制完成源文本的编码解码工作，并使用 Beam Search 方法解析得到最终摘要。

Chopra 等人在 Rush 方法的基础上，使用了循环神经网络(RNN)，因为 RNN 更适合编码序列化的数据。

Collins 等人使用了长短时记忆网络(LSTM)，解决 RNN 的梯度消失、梯度爆炸等缺陷。

> NLP 任务存在的一个普遍问题是 OOV(Out of Vocabulary)。
> 在模型训练前我们需要明确一个固定词汇数量的词汇表，并对其进行编号。虽然这个词汇表可以涵盖到所有训练数据中出现的词汇，但是在给定的新文本中，如果出现了未出现在词汇表中的词时(这类词通常称为未登录词)，我们通常进行特殊标记。但如果新文本中未登录词出现过多，则不利于文本生成。这也导致了尽管模型在训练集上的表现很好，但是在测试集上的表现却不太理想。

Gulcehre 等人为了解决 OOV 问题，提出一种指针生成网络，通过对解码器的***每个时间步长计算生成概率*** ，来确定当前待解析词来自源文本的权重和来自模型输出的权重，最终通过对词分布表示加权再累加后，得到最终待解析词的分布表示。

Gu 等人也提出了一种复制机制来解决 OOV 问题。

### 思路

预训练语言模型是近年来 NLP 领域发展的一个重要成果。其思想是将训练任务划分为预训练和微调两个阶段。在预训练阶段，我们将大量无监督数据丢给模型训练，让模型尽可能多地学习文本特征表达。在微调阶段，我们只需在之前已训练好的模型上根据下游任务的不同连接上针对下游任务的网络结构进行解码输出，得到最终结果。预训练语言模型不同于 word2vec及 GloVe 一个词对应一个固定训练好的向量，它会根据输入文本中每个词的上下文信息来推断该词对应的向量表示，这样可以较好地改善固定词向量的**一词多义问题**。由于预训练语言模型在训练阶段接触到了大量数据，**因此训练完成后的语言模型也具备较强的特征编码能力**。

Peters 等人在 2018 年公布的 ELMo 模型中提出预训练语言模型的概念。

Radford 等人将 ELMo 中的 LSTM 结构替换为了更为复杂的 Transformer 结构得到了 GPT(Generative Pre-Training Model) 模型，该结构具备更强的特征抽取能力，同时将双向编码改为单向。

2018 年 Devlin 等人提出的 BERT 模型的实验结果更加验证了预训练语言模型的优势，该模型在 NLP 的诸多任务中表现优异。可以说，BERT 的出现让 NLP 的发展进入到了一个新的阶段。**之后，NLP 的研究也大多围绕 BERT 模型及其变体展开**。

Liu 等人将 BERT 模型稍作改造后应用在了抽取式文本摘要任务上，并取得了不错的表现。

### 国内研究现状

中文不同于英文，中文有字和词之分。字本身具备语义信息，但词能够表达更丰富的语义信息。但每个字本身又具备多层语义，而词的含义相对清晰。**因此，早期对于中文文本的 NLP 研究都需要进行中文分词处理**。

国内的 jieba 分词工具便是一款优秀的中文分词工具，它促进了中文 NLP 研究的进展。受限于当时的计算环境，并且中文本身语法复杂、词义复杂，国内 NLP 研究进展相对缓慢。

关于自动文本摘要技术的研究，王永成教授于 1997 年建立了基于中文的 OA 文本摘要系统，该系统集成了多种方法。该系统仍需要人为定义规则，而不是让机器学习人类语言表达的机器摘要系统。这类系统人为干预更多，无法适应一些复杂文本的摘要获取场景。

在 1999 年，刘挺等人实现了一个名为 CAAS 的中文摘要系统，该系统利用词权计算、意义段划分、句间指代关系识别、句子位置分析等多个特征结果进行加权计算得到每个句子的得分，最后筛选出摘要句。

随着国内计算机、互联网等基础设施的飞速发展，国内计算机领域、人工智能领域的研究拥有了更好的实验环境。**国内研究人员关于文本摘要的研究也多从统计学、机器学习等转向深度学习**。**当下，使用深度学习理论的 NLP 研究中，针对每个特定任务，国外的机构组织都会构建一个公开的数据集，供研究人员学习使用，同时这些公开数据集及评估标准的出现，也让研究者的成果有了数值可比性，而不再是人为评估实验结果的好坏。**国外公开了多个摘要数据集以供训练测试及评估如 CNN/Daily Mail、NYT 及 Gigaword 等。

而国内公开的中文数据集相对较少。具有代表性的便是哈尔滨工业大学的团队通过收集大量新浪微博上的文本语料，并进行人工处理后构建了中文摘要数据集 LCSTS。除此，该团队还使用了 RNN 结合 attention 机制构建了神经网络模型进行训练测试，并给出了 ROUGE 评估得分。该中文摘要数据集的出现为之后中文摘要的研究提供了极大帮助。 

近几年，国内关于中文摘要的研究也多数与深度学习方法相关。华南师范大学的周才东等人提出将局部注意力与卷积神经网络相结合的方法应用于中文摘要任务的研究。北京大学计算语言学实验室的研究团队提出了一个基于语义关联的神经网络模型来增强文本和摘要之间的语义相似性。 

## 经典时序网络结构

图像处理：CNN，无法捕捉与时序有关的数据中的信息。因此早期的，基于深度学习的 NLP 研究大多使用 RNN 结构作为模型基础。RNN 结构本身存在梯度爆炸、梯度消失问题。LSTM，解决上述问题。但 LSTM 本身结构更加复杂，计算复杂度也更高。作为 LSTM 的变种，门限循环单元(GRU)通过对 LSTM 结构进行简化，降低了计算复杂度，提升了计算效率。下面将详细介绍这三种常见时序结构的构成及计算过程。 

## Seq2seq 模型与 Attention 机制

$c$ 为上下文向量 

![seq2seq network structure](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2021-11-08_13-42.png)

缺陷

在 seq2seq 模型中，Encoder 将输入信息编码成一个上下文向量 ，并且在每一个解码时刻用同样的$c$去解码，那么就必须尽可能完整地包含输入序列中的信息。但是如果输入序列过长，那么上下文向量$c$存储的信息可能就不够完整，从而导致实际效果变差。除此，不同时刻的输出与输入序列的关联程度是不同的，比如一般位于句首的词都会包含更多的信息，而位于句尾的词包含的信息相对较少。因此，从句法结构来讲，解码时每个时刻都用相同的上下文向量$c$进行解码是不合理的。 

Bahdanau 等人于 2015 年提出了 attention 机制，该机制希望通过在编码时使用不同时刻的输出计算与输入序列不同关联程度的上下文向量$c$来改善上述问题。引入 Attention 机制的 seq2seq 模型如下： 

![引入 attention 机制的 seq2seq 架构 ](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2021-11-08_13-46.png)

 attention 权重可视化：清晰地看到模型在学习过程中，每个输出与每个输入的关联程度。

## Bert 的改造

BERT 在自动文本摘要任务中的问题

1. BERT 非原生支持文本生成。因为，其结构只使用了编码器部分。
2. 下游任务缺乏细粒度的语义表示能力。尽管 BERT 在预训练阶段通过大量无监督数据训练学习，从而具备了很强的词法、句法表征能力。
3. 容易产生语义偏差问题。因为，编码器层数多。

解决方案首先

1. Dong 等人改造了 BERT-base 的掩码矩阵以适应文本摘要任务。
2. 提出一种基于胶囊网络的语义监督方法，以改善输入经过多层编码器后出现的**语义偏差问题**及 BERT 在文本摘要任务
   中**缺乏细粒度语义**表示的问题。

### 掩码矩阵改造

Song 等人提出了基于 Bert 改造的 MASS 模型，用来处理文本生成任务。

Dong 等人提出了 UNILM 的语言模型，改造方式简单，且能够适应各种 NLP 任务。
UNILM 模型包含三个训练目标：Unidirectional LM(left-to-right and right-to-left)，Bidirectional LM 及 Seq2seqLM。有了这三个目标，UNILM 才能适应各种各样的 NLP 任务，因此 UNILM 又称统一语言模型。 

思路

ELMo：对从左至右及从右至左两个单向模型进行叠加。
GPT：只考虑从左至右模型，因为这样可以防止做预测时当前时刻关注到后文的信息。
BERT：双向语言模型，即每个词都可以看到其他词提供的信息。
UNILM = Bert + seq2seqLM，而该模块便是为了适应生成式任务而新增的。

作者是如何修改的？

模型主体是基于 BERT(也就是图中的 Embedding Layer  和  Transformer Layer 两部分)，在此基础上增加了输出层 Output Layer 来解码我们需要的输出摘要。下面将详细介绍这三部分。 

#### Embedding Layer 

BERT 的 Embedding 层包含了 Token Embedding、Positional Embedding 和 Segment Embedding。

Token Embedding，每个 token 的向量表示，根据 token Id 查询词嵌入矩阵得到。
Segment Embedding，段嵌入，通常用来表示当前 Token 来自哪一个段。段的划分通常用于区分不同句子或者不同的文本段。
Positional Embedding，表示当前 Token 所在的位置。

我们对模型输入的处理遵循了 BERT 的处理方式：在输入的起始位置增加特殊标记符 [CLS]，并在每个文本段结束时增加特殊标记符[SEP]。在文本摘要任务中，我们将源文本记为一个段，对应的摘要记录为另一个段。图中的$T$为源文本序列，为对应的$S$摘要文本序列。我们将$T$和$S$及特殊字符进行拼接得到模型的输入 $X=\{[CLS], T, [SEP], S, [SEP]\}$。 而为对应的输出序列。在解码摘要过程中，我们取$Y_k$对应的字符或单词作为当前的输出，然后再与之前的输入进行拼接一步一步地迭代式解码，从而得到最终的输出摘要内容。 

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2021-11-08_16-32.png)

#### Transformer Layer 

BERT 的核心模块，该模块由多个结构相同 Transformer Block 堆叠而成。seq2seqLM 主要在 Transformer Block 中的 multi-head attention 计算中引入了掩码矩阵，这样可以防止在模型做预测过程中看到后文信息。掩码矩阵的设计如图 3-3 所示，源文本中的每个 token 可以互相关注，而它们无法
关注摘要文本中的 token。摘要文本中的 token 只能关注所有源文本中的 token 及位于它之前的摘要 token，而位于它之后的 token 无法关注。

#### Output Layer

输出层主要用于将 Transformer Layer 提取到的词法、句法等特征进行解码输出。我们将最后一个 Transformer Block 的输出作为 Output Layer 的输入。Output Layer 主要包含两层全连接和一层 Layer Normlization。第一层全连接主要用于为 BERT 的输出添加线性操作，它包含 768(768 为 BERT-base 的词嵌入维度)个神经元，并且我们使用 BERT 中广泛使用的 GELU 作为激活函数。我们添加的 Layer Normlization 主要用于避免梯度消失。不同于 Batch Normlization，它不依赖于 Batch Size 的大小及输入序列的长度，即输入的长度允许不固定。最后一层全连接主要用于解析输出，它包含
个神经元( 为词汇表包含的词汇个数)，并且我们使用 softmax 作为激活函数。 

## 基于胶囊网络的语义监督方法

在应用到具体任务时，**BERT 会缺乏细粒度的语义表示。**

而文本摘要任务需要对整个文本的信息进行抽取、总结、聚合，因此文本摘要任务需要细粒度的语义表示才能保证生成的摘要语义尽可能的贴近原文。考虑到胶囊网络具备很强的特征聚合能力，为此我们希望通过胶囊网络对 BERT 输入与输出的语义向量进行特征聚类从而获取细粒度语义表示。除此，模型输入经过多层编码器后的结果与原始输入会产生语义偏差问题。

为解决该问题我们提出一种**语义监督方法**，即通过计算 BERT 输入与输出聚类后的两个语义向量之间欧式距离，并将最小化该距离作为训练目标，以减小语义偏差问题。最终**提升生成的文本摘要质量**。

### 语义监督作用机制 

由于文本摘要任务需要对源文本的信息进行抽取归纳，最后总结为摘要。此过程可能会导致生成的摘要与原文本的相关度不高，为此 Ma 等人提出了一种基于语义关联性的神经网络模型，通过**计算输入与输出层的隐状态之间的余弦相似值**，并将其加入损失函数中，他们期望模型能够通过训练学习使得输入与输出层的隐状态向量表达的**语义尽可能相似**，从而来达到提升源文本与生成摘要的语义关联性。

借鉴此方法，我们**对 BERT 中的 Token Embedding 的输出**作为胶囊网络的输入，胶囊网络计算后得到 BERT 输入聚类后的语义向量 。
我们对最后一层的 Transformer 的输出做同样的操作，可以得到 BERT 输出聚类后的语义向量 ，同时我们使用欧式距离来衡量二者之间的相似度，我们通过最小化二者距离来实现对 BERT 输入与输出的语义监督。 

损失函数以下两个损失的和
模型主体的损失，分类交叉熵
自定义的语义监督模块的损失函数

 Adam 作为优化器
学习率为  $\alpha=1\times 10^{-5}$两
阶段参数为$\beta_1=0.9, \beta_2=0.999$
$\epsilon=1\times 10^{-8}$

## 实验设计

### 数据集

#### LCSTS

划分数据集：同原文作者一样，选取 PART I 的数据作为训练集，PART II 中得分不小于 3 的数据作为验证集，PART III 中得分不小于 3 的数据作为测试集。
最大长度固定（先统计，过短特殊字符补齐，过长截取）：由于每条数据包含的文本和摘要的长度都不同，而我们的输入需要固定最大输入长度，因此我们对 LCSTS 的各部分数据集的长度进行了统计。为了尽可能满足所有输入数据的长度，同时又需要**剔除一些文本过长的数据**，因此我们设置最大源文本长度为 text_max_len=140 ，而设置最大摘要长度为 summary_max_len=30。

#### CNN/Daily Mail

处理同 LCSTS 
而我们使用的 BERT 模型接收的输入最大长度为 512，考虑到输入长度过大导致计算速度慢，因此
源文本最大长度 text_max_len = 400
最大摘要长度 summary_max_len=100

### 评估标准

NLP 研究的子任务有很多，不同的任务有不同的评估标准。如文本分类，情感分析等任务通常会使用 P(准确率)和 R(召回率)来评估一个文本分类模型的性能优劣。下面将主要介绍用于评估自动文本摘要任务的 ROUGE 评估标准。

### 基准系统

#### LCSTS 数据集 

##### RNN and RNN-context

这两个模型都是基于 seq2seq 结构的，它们也是 LCSTS 数据集的创建者实现的基准模型。其中，前者编码器解码器都使用了 GRU 结构，而后者在前者的基础上增加了 attention 机制。 

##### CopyNet

该模型是基于复制机制的 seq2seq 模型。复制机制允许模型从原文中复制一些词作为解码的摘要输出，该机制可以有效改善文本生成任务中不断生成重复词的问题。 

##### DRGD

它是一个具备深度递归生成解码器的 seq2seq 模型。该模型在解码器中使用了变分自编码器，并且它利用一个潜在递归随机模型学习参考摘要中隐
含的潜在结构信息。 

##### WEAN

它是一个基于编码器-解码器框架的新模型，它的全称是 Word  Embedding Attention Network。该模型通过查询分布式词表示来生成词汇，从而希
望捕捉对应词汇的含义。 

##### ##### Seq2Seq+superAE
它是一个携带有辅助监督功能的 seq2seq 模型。辅助监督主要完成源内容的语义表示对摘要语义表示的监督。该模型使用自编码器实现辅助监督。此外，为了更动态地确定监督力度，作者在模型中引入了对抗学习。 

## Transformer 下融合关键信息的神经机器翻译模型

翻译系统(基于词汇，语法等规则) -> 基于统计学的机器翻译 -> 神经机器翻译(NMT)

神经机器翻译的任务主要是利用神经网络相关技术及大量的数据来训练得到一个通用的翻译模型。模型训练好以后，我们只需给定模型输入源语言句子，模型通过运行计算即可得到对应的翻译结果。

前文提到的 seq2seq 结构在神经机器翻译中具备一定的优势，seq2seq 模型极大地促进了神经机器翻译模型的研究进展。因此，近几年关于神经机器翻译模型的研究大多都基于该结构。2017 年，Vaswani 等人提出了 Transformer 模型。该模型相较于 seq2seq 在 NMT 中拥有更好的实验表现。尽管当下的神经机器翻译模型的翻译结果都还不错，但是对于关键信息的翻译依然存在**错译、漏译**等情况。翻译过程中最重要的就是对于关键信息的翻译，因为只要关键信息翻译准确完整，即使一些连接词、辅助词等信息翻译有误，仍然可以保证最终的翻译质量。因此，本文提出了一种基于 Transformer 融合关键信息的神经机器翻译模型。并在 WMT17  中-英数据集及英-德数据集上进行了实验验证，最终的实验结果表明我们所提出的方法有效可行，能在一定程度上提升模型的翻译质量。 

### 关键信息提取

本文提出的基于 Transformer 融合关键信息的神经机器翻译模型，主要有两个研究难点：其一是选择何种方式获取关键信息；其二是如何将获取到的关键信息融入 Transformer 结构。获取关键信息的方式有很多，但是最简单有效的便是利用关键词提取算法获取关键词。而本文也是采用提取关键词的方式获取关键信息。下面将简要介绍几种主流的关键词提取算法。




---

自动文本摘要可以极大地提升人们从海量数据中获取关键信息的效率，除此自动文本摘要还具备为短新闻文本生成标题的用途。当下基于神经网络设计的文本摘要模型都用到了多层编码器，而源文内容经过多层编码器后虽然可以挖掘到深层语义信息，但容易产生语义偏差。机器翻译任务即让机器代替人类完成源语言到目标语言的翻译工作。当下机器翻译研究使用神经机器翻译模型居多。当下多数神经机器翻译模型都是基于 Transformer 模型进行改进，但是这些模型都没有利用到源语言文本中的关键信息，而对关键信息的翻译是否准确直接决定了最终的翻译质量。

本文工作

1. 提出了一种结合语义监督的文本摘要方法。参考 UNILM 模型的思路，改造 BERT 的掩码矩阵，以适应文本摘要任务。
   解决输入经过多层编码后出现的语义偏差问题：提出了一种基于胶囊网络的语义监督方法。通过胶囊网络对第一层及最后一层的编码结果进行语义特征聚类，之后再对聚类后的语义特征进行距离监督，从而起到语义监督作用。测试数据集： LCSTS（哈工大） 及 CNN/Daily Mail 数据集

2. 提出基于 Transformer 模型的融合关键信息的神经机器翻译模型，以改善机器翻译模型对于关键词翻译存在的漏译、错译等问题。筛选关键词提取算法，选择 TextRank 算法。关键信息的融合我们使用 Multi-head attention 的方式，并通过阈值控制关键信息对深层编码信息的干扰。验证方法的有效性： WMT 数据集的中-英和英-德两个数据集。 

3. 线上文本摘要系统。基于 B/S 架构设计，前后端分离部署。该系统包括用户登录注册及文本摘要等主要功能。系统完成了功能性测试及压力测试。

## 基础知识

### 词向量

one-hot 向量，normal 方法

特点

1. 稀疏的

缺陷

1. 向量维度爆炸：向量的维度与词汇表大小正相关。
2. 无法捕捉词之间的相似度。

### 神经网络语言模型 NNLM

NNLM 于 2003 年由 Bengio 等人提出。当时神经网络技术研究并不被看好，导致此方法在当时未能激起强烈反响。但在 2013 年随着 word2vec 的出现，该方法理论重新大放异彩。神经网络语言模型 NNLM 希望通过
神经网络结构去学习每个词的分布式表示，从而消除 one-hot 向量的维数灾难，并且学习到的向量表示使得词向量在向量空间中具备关联性(即词向量之间的距离意味着词之间的相似程度)。NNLM 的训练目标是给定一个词的上文信息，然后预测该词。NNLM 的网络结构如图 2-1 所示，其中$w_i\in\R^n$为文本中每个词的 one-hot 向量，矩阵$C \in \R^{n \times m}$为模型学习到的 Embedding 矩阵( $n$ 代表词汇表大小， $m$ 代表向量压缩后的维度)， $w_i$乘上该矩阵即可得到压缩后的词向量$C(w_i)$ 。 

### Word2vec

Word2vec 与 NNLM 的思路大同小异，都是希望通过神经网络学习到词嵌入矩阵$C$ ，**这样就能将稀疏的 one-hot 向量压缩成为稠密且具备语义信息的词向量**。NNLM 语言模型的目标是通过给定一个词的上文信息，然后让模型预测该词，即模型能够根据上文信息来预测下文信息。NNLM 只是在训练过程中间接学习到了词嵌入矩阵。而 word2vec 的目标是让模型通过训练直接学习得到词嵌入矩阵。这样无论下游任务是什么，都可以通过 word2vec 将 one-hot 向量转化为词向量。word2vec 的训练有两种模式：CBOW 模型和 Skip-gram 模型。

NNLM 是根据上文预测下文信息。

CBOW 模型相当于“完形填空”，即根据该词上下文信息预测挖掉的词。 用周围的词去预测中心词，利用中心词的预测情况，通过使用梯度下降的方法，不断的去调整周围的词的向量表示。当训练完成后，每个词都被作为中心词，完成对周围的词的向量表示调整，从而得到所有词的向量表示信息。CBOW 的预测次数与整个文本包含词的个数一致，即预测的时间复杂度接近 O(V)，V 为整个文本包含的词的个数。

Skip-gram 给定一个单词让模型去预测该词的上下文信息。用中心词去预测周围的词，并利用周围的词的预测情况反向更新中心词的向量表示，整个文本遍历结束后也就得到所有的词的向量表示。但每个词作为中心时，其上下文窗口内的词都需要被预测。因此其预测的时间复杂度接近 O(KV)，其中 K 为上下文窗口大小，V 为整个文本包含的词的个数。

虽然，Skip-gram 的预测次数比 CBOW 模型更多，意味着训练时间更长。但是在 Skip-gram 模型中每个词的向量表示都要受其上下文窗口内的词的预测结果的影响，即每个词的向量表示都需要经过多次调整。因此当
训练数据较少，或该词为生僻词时，Skip-gram 方法的多次调整使得每个词的向量表示更加准确，而 CBOW 却无法做到。

总之，两种方法各有优劣，**CBOW 模型训练时间更短，但后者训练的词向量更准确**。 

### 深度语境化词表示

word2vec 却无法解决如何区分多义词在不同语义环境下的语义表示问题。

为什么？这是由于 word2vec 是固定训练模型，即我们需要提前训练好 word2vec，再引入到我们的模型中进行使用。word2vec 训练好之后，每个词的向量表示也就固定了，也就意味着每个词的语义也基本固定了。但是，在不同的上下文中多义词会表达出不同的含义，因此 word2vec 无法解决不同语境下的多义词问题。

2018 年，Peters 等人提出了深度语境化词表示模型(ELMo)。在该模型中首次引入两阶段训练目标，其中包括预训练阶段和微调阶段。预训练阶段主要负责学习词向量表示，而微调阶段允许词向量表示在连接的下游任务中，上游模型可以根据下游任务对参数进行微调。与 word2vec 固定的词向量表示不同，ELMo 的词向量表示会根据下游任务的具体语境进行动态调整，从而解决多义词在不同语境下的语义表达问题，同时也降低了对下游任务训练的复杂度。ELMo 模型预训练阶段的目标与 CBOW 模式相似，都是根据词的上下文信息来预测词出现的概率，但是 **ELMo 训练得到的词向量表示允许被下游任务动态调整**。为了捕捉到丰富的上文信息及下文信息，ELMo 采用的是双向 LSTM 结构，前向 LSTM 时序结构负责捕捉上文信息，而逆向 LSTM 时序结构负责捕捉下文信息，然后再将二者信息进行合并提供给模型。除此，为了捕捉到更深层的句法特征及语义特征，ELMo 使用了多层双向 LSTM 结构来完成深度特征的提取工作。

