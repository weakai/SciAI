# XLNet

18 年底谷歌爸爸推出了 bert，该模型一经问世就占据了 nlp 界的统治地位，如今 CMU 和 google brain 联手推出了 bert 的改进版 xlnet。
在这之前也有很多公司对 bert 进行了优化，包括百度、清华的知识图谱融合，微软在预训练阶段的多任务学习等等，但是这些优化并没有把 bert 致命缺点进行改进。
xlnet 作为 bert 的升级模型，主要在以下三个方面进行了优化

- 采用 AR 模型替代 AE 模型，解决 mask 带来的负面影响
- 双流注意力机制
- 引入 transformer-xl

## AR 与 AE 语言模型

目前主流的 nlp 预训练模型包括两类 AR 和 AE。
AR 模型的缺点，没错该模型是单向的，我们更希望的是根据上下文来预测目标，而不单是上文或者下文，之前 open AI 提出的 GPT 就是采用的 AR 模式，
包括 GPT2.0 也是该模式，那么为什么 open ai 头要这么铁坚持采用单向模型呢，看完下文你就知道了。
AE 模型采用的就是以上下文的方式，最典型的成功案例就是 bert。
bert 的最大问题也是处在这个 MASK 的点，因为在微调阶段，没有 MASK 这就导致预训练和微调数据的不统一，从而引入了一些人为误差，我觉得这应该就是为什么 GPT 坚持采用AR模型的原因。

## 排列语言模型

该模型不再对传统的 AR 模型的序列的值按顺序进行建模，而是最大化所有可能的序列的排列组合顺序的期望对数似然

## 基于目标感知表征的双流自注意力

虽然排列语言模型能满足目前的目标，但是对于普通的 transformer 结构来说是存在一定的问题的

## 集成 Transformer-XL

片段循环机制，解决超长序列的依赖问题
相对位置编码，搭配片段循环机制
预训练，去除了 Next Sentence Prediction，因为该任务对结果的提升并没有太大的影响

## 参考

- [最通俗易懂的 XLNET 详解](https://blog.csdn.net/u012526436/article/details/93196139)