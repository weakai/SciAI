---
title: 长尾分布
date: 2022-2-18
---

在传统的分类和识别任务中，训练数据的分布往往都受到了人工的均衡，即不同类别的样本数量无明显差异。

一个均衡的数据集固然大大简化了对算法鲁棒性的要求，也一定程度上保障了所得模型的可靠性，但随着关注类别的逐渐增加，维持各个类别之间均衡就将带来指数增长的采集成本。

在自然情况下，数据往往都会呈现如下相同的长尾分布。

这种趋势同样出现在从自然科学到社会科学的各个领域各个问题中，参考 Zipf's Law 或者我们常说的 28 定律。

直接利用长尾数据来训练的分类和识别系统，往往会对头部数据过拟合，从而在预测时忽略尾部的类别。

如何有效的利用不均衡的长尾数据，来训练出均衡的分类器就是我们所关心的问题，从工业需求上来说，该研究也将大大地提升数据采集的速度并显著降低采集成本。

同时长尾效应主要体现在有监督学习里，无监督/自监督学习等因为不依赖标注，所以长尾效应体现的不明显，目前也缺少这方面的研究（但并不代表无监督/自监督学习不受长尾效应的影响，因为图片本身也有分布，常见的图案和罕见的图案也会形成这样的长尾效应，从而使模型对常见的图案更敏感）。

## 两种基本方法

长尾分布的最简单的两类基本方法是重采样（re-sampling）和重加权（re-weighting）。这类方法本质都是利用已知的数据集分布，在学习过程中对数据分布进行暴力的 hacking，即反向加权，强化尾部类别的学习，抵消长尾效应。

## 参考

[Long-Tailed Classification (1) 长尾(不均衡)分布下的分类问题简介](https://zhuanlan.zhihu.com/p/153483585)