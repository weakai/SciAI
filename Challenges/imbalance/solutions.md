# 长尾问题解决方案

#### 重采样（re-sampling）

更具体可分为对少样本的过采样，或是对多样本的欠采样。
但因过采样容易 overfit 到 minor class，无法学到更鲁棒易泛化的特征，往往在非常不平衡数据上表现会更差；
而欠采样则会造成 major class 严重的信息损失，导致欠拟合发生。

#### 数据合成（synthetic samples）

即生成和少样本相似的“新”数据。经典方法 SMOTE，思路简单来讲是对任意选取的少类样本，用 K 近邻选取其相似样本，通过对样本线性插值得到新样本。
这里会想到和 mixup 很相似，于是也有 imbalance 的 mixup 版本出现。

#### 重加权（re-weighting）

对不同类别（甚至不同样本）分配不同权重。
注意这里的权重可以是自适应的。
此类方法的变种有很多，有最简单的按照类别数目的倒数来做加权，按照“有效”样本数加权，根据样本数优化分类间距的 loss 加权，等等。

#### 迁移学习（transfer learning）

这类方法的基本思路是对多类样本和少类样本分别建模，将学到的多类样本的信息/表示/知识迁移给少类别使用。

#### 度量学习（metric learning）

本质上是希望能够学到更好的 embedding，对少类附近的 boundary/margin 更好的建模。

#### 元学习/域自适应（meta learning/domain adaptation）

分别对头部和尾部的数据进行不同处理，可以去自适应的学习如何重加权，或是 formulate 成域自适应问题。

#### 解耦特征和分类器（decoupling representation & classifier）

最近的研究发现将特征学习和分类器学习解耦，把不平衡学习分为两个阶段，在特征学习阶段正常采样，在分类器学习阶段平衡采样，可以带来更好的长尾学习结果。

**这也是目前的最优长尾分类算法。**
