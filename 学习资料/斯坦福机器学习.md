---
title: 机器学习课程
url: https://c.d2l.ai/stanford-cs329p
tags: [斯坦福, 李沐]
---

## 1.1 机器学习提升各行业收入

#### ML workflow

problem formulation
collect & process data <- monitor
train & tune models
deploy models -> monitor: data recollection

#### 君子善假于物

机器人不能叠衣服，但能翻译。

#### 滚动部署

刚开始用简单模型，测试数据的好坏。运气好的话可以直接部署，收集更多数据。

#### 挑战

行业过分专注于有商业价值的领域
数据：质量，隐私
模型训练：模型更复杂，贵，需要更多的数据
模型部署：重计算不适合实时推断
监控：数据分布的偏移，公平问题

#### 其他

Good slides help you learn quickly

#### 职业与角色

领域专家：商业嗅觉，抓住重要的数据。
数据科学家：全栈数据挖掘，模型训练和部署。数据科学家，也会发些论文，google 经常霸榜顶级期刊。70% 时间在清洗数据。
机器学习专家：如何从学术模型定制化，达到 SOTA 水平
软件开发工程师（最主要）：开发维护整个模型资源网络

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img20211214152858.png)

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img20211214153028.png)

## 1.2 数据获取

找数据，数据不够？生成来凑。

找可用的数据：

- 公用的做好的数据集，来检验我的算法。
    - 新想法是超参数：一般要小一点，类型比较丰富，噪音要多一点。
    - 新想法是深网络
    - 新想法是产品：缺少数据集，只能收集数据：安装接收器，然后采集数据。
- 常见数据集：
    - MNIST：手写的
    - ImageNet：图片搜索引擎来的
    - AudioSet：Youtube 
    - Kinetics：Youtube 
    - KITTI：自动驾驶
    - Amazon Review
    - SQuAD: 维基问答
    - LibriSpeech：1000h 有声读物

能用现有数据集就不要自己建造：

- Paperwithcoded Datasets: academic datasets with learderboard
- Kaggle Datasets: 有很多数据科学家上传自己的
- Google Datasets search：搜索引擎
- Various toolkits datasets：tensorflow, huggingface 自带的数据集
- Various conference/company ML competitions
- Open Data on AWS: 100+ large-scale raw data(P 级别，比如气象数据)
- Data lakes in your own organization.（比如公司的产品线数据）

数据集的比较

- academic datasets
- competition datasets
- raw data: greate flexibility but effort to process

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img20211214163957.png)

### 自己造数据
用 GANs
数据增强：图片 photoshop；翻译另一种语言，再翻译回来。

## 数据标注

### 自学习 self-training

一种半监督学习（SSL）。对一部分数据贴标签(labeled data)，然后训练，得到比较确切的标签，比如 98:1:1。不断迭代，提升标签的准确度。此时，可以使用比较贵的模型，因为不需要部署上线。

### 众包标注 crowdsourcing

ImageNet 数据集与众包服务 Amazon Mecahnical Turk

### 弱监督学习

Weak Supervision: 半自动生成 label

数据编程技术 Data programming heuristic programs，你总结一下规律，然后用编程写规则，生成若标签。也可以用现成的 API 接口。

## 数据预处理：探索性数据分析（EDA）

Pandas 可以几百 G 大小的文件，更大换其他包，或者采样 

Pandas 能够直接读压缩文件。效果更好。

```python
data = pd.readcsv('test.zip') 
```

```python
null_sum = data.isnull().sum()
data.columns[null_sum < len(data) * 0.3]  # columns will keep

data.drop(columns=data.columns[null_sum > len(data) * 0.3], inplace=True)
```

`inplace` 效率高，但只能跑一次

```python
# 看看每列的数据类型
data.dtypes

# 把 ￥1000 转化为数字
currency = ['Sold Price', 'Listed Price', 'Tax assessed value', 'Annual tax amount']
for c in currency:
    # pandas 自带的replace
    # 带正则
    data[c] = data[c].replace(
        r'[$,-]', '', regex=True).replace(
        r'^\s*$', np.nan, regex=True).astype(float)
```

Also convert areas from string format such as `1000 sqft` and `1 Acres` to float as well. 这些都转化为熟悉

`astype(float) str.contains replace`

```python
areas = ['Total interior livable area', 'Lot size']
for c in areas:
    acres = data[c].str.contains('Acres') == True
    col = data[c].replace(r'\b sqft\b|\b Acres\b|\b,\b','', regex=True).astype(float)
    col[acres] *= 43560
    data[c] = col
```

### 分析数据

describe() 看噪音：数据合不合理。

square feet

除去离群值。

### 画图

给价格做一个 log 因为，不同房子的价格差距太大了，要平滑一下。

看看房子有多少种类。

SingleFamily

Condo 单身公寓

Multifamily

Townhouse

拿出数量前几的类别。画图。

kde 密度分布

boxplot  看不同分布的数据
25% quantile
median
fliersize=0 不显示离群值

zip codes 房子的邮政编码

协方差 correlation matrix：两两列的关系。

税务局的估价 Annual tax amount（用来纳税用的）可能比列出来的价格（Listed price）更准。

Elementry School score

## 数据清洗
Data Cleaning
Good ML models are robust to errors
模型能拟合（收敛），但是会降低精度。
数据噪音可以部分修正，以获得不断变好的训练。

### 数据类型错误
Types of Data Errors
- Outliers
- Rule violations ID 冲突，非负
- Pattern violations 语法语义错误

### Outlier Detection

箱线图，适用于数值类型的数据的分布查看：
- Outliers
- 1.5 box length 最大值定义：=箱子最高点+1.5倍箱子的高度
- 75th percentile
- 25th percentile

### Rule-based Detection

Functional dependencies：
x 可以推出 y 列

Denial constraints:
手机号码必须非空；

Syntactic patterns 语法错误：
english -> English
修改合适的数据类型

Semantic patterns 语义错误：
比如说国家类，里面有台湾就不对了，应该说是中国，或者中国台湾。

## 数据变换 Data Transformation

你要权衡：storage，quality，loading speed

### 实数列的正常化

Normalization for Real Value Columns

- Min-max normalization
- Z-score normalization 用的最多，简单，用到均值与方差。
- Decimal scaling。除以 10 的 j 次方
- Log scaling。比如房价的 Log 变换。log 加减等于原数的乘除
 ，房子可能更在乎价格比例的相对值。

 ### 图片的变换

大体积的图片的存储是个问题。
所以图片压缩：下采样：size 1000 -> size 320。
机器学习对低分辨率效果也挺好，因为它是像素像素看的。

jpeg 的图片质量(80%-90%) 可能会导致 1% acc drop in ImageNet. 这个下采样的参数可能会导致不能复现。

也可以用降维来缩小图片尺寸

### Video Transformations

提取关键的视频片段 video clips(<10 sec)

视频算法有很多，如何解码是关键。如何提取关键帧（图片）。
一般用 GPU 采样。

### Text Transformations

主要是建造词典

词干法Stemming and lemmatization: a word a common base form

比如 Topic modeling
am, are, is -> be
car, cars, car's, cars' -> car

Tokenization（用的多）：分词
- by word: text.split(' ')
- By char: text.split('')
- By subwords: “a new gpu!” -> “a”, “new”, “gp”, “##u”, “!”

中文也是需要分词的。

## 特征工程 FE

传统的人工智能。对数据对象抽取特征。要产生 SVM 喜欢的向量。

现在用神经网路做特征提取，用一个线性分类器， 端到端的一个过程。费数据和计算资源。

### Tabular Data Features

numeric / categorical / string type

#### Int/float
directly use or bin to n unique int values

数值变化较小就忽略，如房价100w，101w 其实没有太大区别

#### Categorical data
one-hot encoding。列太长咋么办？不重要的行我们全部设为 unkown。
cat -> `[0, 1, 0, 0, 0]`
dog -> `[0, 0, 0, 1, 0]`

#### Date-time

a feature list such as
`[year, month, day, day_of_year, week_of_year, day_of_week]`
如果你用时间戳去编码，就拿不到某些信息。因为人的活动是有周期的，比如星期。

#### Feature combination
特征组合: Cartesian product of two feature groups 双子列独热编码
用外乘1x2 x 1x2 = 4x2
`[cat, dog] x [male, female]` ->
`[(cat, male), (cat, female), (dog, male), (dog, female)]`

#### Text Feature

- token 表示

  - bag of word

  - 词向量嵌入，例如 word2vec

- 预训练的语言模型，如 transformer

#### Image/Video Feature

- 传统手动提取图片特征的方法，如 SIFT
- 预训练的深度神经网络

## 3.1 ML Model Overview

Types of ML Algorithms
- Supervised learning，最主流
	- Semi-supervised
	- Self-supervised，自己产生标号
	
- Unsupervised learning
  - clustering
  - density estimation
  - GAN
- Reinforcement learning
  - 有环境和最大化奖励，更像人，难。

### 监督学习的组成对象
Model: 最重要
LOSS: 真实值与预测值的差距
Objective: 优化的目标，比如要最小化 loss
Optimization: 这是一个过程

### 监督学习模型的类型

Decision trees
Linear methods
Kernel machines
Neural Networks

## 3.2 Decision Trees

可以用于回归和分类

优点
- Explainable，为数不多的几个可以解释的
	- 比如银行贷款
	- 工业界用的最多，简单，无需调参，结果也不错
	- 通常是第一选择，用于开路
- Handle both numerical and categorical features
  without preprocessing

缺点

- 不稳定

  - 使用随机森林。许多树独立产生结果，然后通过均值或者投票进行集成，大大提升稳定性
    为什么是随机的？bagging 采样是随机的，而且是替换着采样。如同随机行采样和随机列采样。

  - 梯度提升决策树：一棵树有顺序地不断迭代提升
    小树对残差进行训练，梯度下降。最终树等于原始树加生成的许多小树的加和，目的是接近真实值

- 过拟

- 并行度差

## 3.3 线性方法

神经网络的原始形态：Linear Regression

Objective Function 目标函数

Objective: minimize the mean square error (MSE)

### 如何做预测呢？

输出y是长为m的向量，m为类别数。得到每一类的置信度[o1,o2,o3]是一个独热编码，oi为置信度。
这个可以做均方误差，预测用 argmax 函数。

#### 为什么要用 softmax 函数

线性回归 vs softmax 回归（i.e. 分类）

我们只需要预测正确的那一个类的置信度，其它的则不重要。如果都算的化，拟合计算复杂度很大。这时候我们引进 softmax，把置信度转化为概率，解决 m 分类问题。

$$\hat{y}=\frac{exp(o_i)}{\sum_{k=1}^mexp(o_k)}$$

```python
O_exp = torch.exp(0)
partition = O_exp.sum(1, keepdim=True)
Y = O_exp / partition
```

交叉熵衡量两个概率的差别。用 $H(y,\hat{y})$ 来计算交叉熵损失。

也就是 $-log\hat{y}_y$ 最大

## 3.4 SGD

小批量随机梯度下降：深度学习几乎唯一的求解算法（除了决策树）

## 3.5 Neural Networks

Handcrafted Features -> Learned Features

NN 模型的架构 

- Multilayer perceptions
- Convolutional neural networks
- Recurrent neural networks
- Attention mechanism

### MLP 多层感知机

单层是线性的，咋样成为非线型呢？要用激活函数,加入非线性性质，比如 sigmoid, $ReLU(x)=max(x,0)$

有几个超参数：多少个隐藏层，每个隐藏层的输出维度是多少。

隐藏层也是全连接层

为什么要乘以 0.01 

```python
def relu(X):
	return torch.max(X, 0)
W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs))
H = relu(X @ W1 + b1) 
Y = H @ W2 + b2
```

### Dense layer -> Convolution layer

如何从全连接层到卷积层？

为何。因为全连接要输入全部的数据个数。尤其是图片。所以需要卷积来压缩像素点，大大降低可学习的参数量，参数量与窗口大小有关。

根据图片的原则，所以有了卷积层：

1. Translation invariance 平移不变性：参数共享，池化层（叫汇聚层更好），平移在k像素之内，具有鲁棒性，减少卷积层对位置变化的敏感性，所以卷积层后要加一个池化层。只要不加激活函数，都是线性的卷积层。
2. Locality 本地性，像素与旁边像素相关：感受野，kernel

了解了卷积的原理，就明白了它不适合原子的性质与预测。

为什么要 flatten？如果卷积层输出4维，我们的全连接层需要2维的东西。flatten就把4维展开成2维的东西。

### 循环神经网络 Recurrent network

加入序列的时序信息到下一个词的预测。

门控忘掉过去，忘掉现在不重要的东西。

隐藏状态：传递的信息。

Simple RNN, LSTM, Gated RNN

Bi-RNN, Bidirectional RNN

Deep RNN, RNN 像 Dense 一样堆叠，RNN 可以是 Bi-RNN

### 咋么选模型呢？

Tabular -> Tree decision, Linear/MLP

Text/speech -> RNN, Transformers

Images/audio/video -> Transformers, CNNs

## 4.1 模型评估 Model Validation

P （分布）-> data -> 算法（模型）

算法的好坏和采样的数据和超参数有关。

### Model Metrics

多指标衡量，选取最合适的模型。商业的话还要考虑计算速率。

case Study: Display Ads

二分类问题：用户点不点击

**常见做分类的指标**

```python
# acc
sum(y == y_hat) / y.size
# 但是如果负类太多的话，acc就不准确了
# precision
## 对具体类 i ,看看它预测对了多少个
## 对样本不平衡的数据有用
sum((y_hat == 1) & (y == 1)) / sum(y_hat == 1)
# Recall
## 召回
sum((y_hat == 1) & (y == 1)) / sum(y == 1)
# F1
## 精度和召回的权衡
(2*p*r)/(p+r)
```

广告中更常用：AUC & ROC

$\theta$ 是正类的概率的阈值，可以自己选，当 $\hat{y}>\theta$ 被判断为正类。

当 $\theta$ 足够大时，精确度会很高，但是召回率很低。所以，我们需要调 $\theta$。

就可以得到 ROC curve(横轴为假阳性率，纵轴为真阳性率)

AUC 是 ROC 的曲线下面积，反应了模型对两个类的区分能力

AUC 等于 -1 方向完全区分。

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img20220112154500.png)

商业指标：revenue = #pageviews x ASN x CTR x ACP

Latency: 延迟，广告加载迟了，用户不会看
ASN: average #ads shown in a page
CTR: actual user click through rate
ACP: average price advertiser pays per click

有时候模型 AUC 提升了（关系相对值，如果 CTR 都降低 0.1 AUC 不变），但是商业指标下降了。所以，只看 AUC 也是不够的。

## 4.2 过拟和欠拟

training error: on the training data

generalization error: on new data

![](https://cdn.jsdelivr.net/gh/zhucheng99/picture/202207091504424.png)

![](http://cdn.jsdelivr.net/gh/zhucheng99/picture/202207091447330.png)

**model complexity**

- the ability to fit variety of functions
- It's hard to compare between very different algorithms
- 相似的模型
  - 参数个数
  - 参数大小

![](https://cdn.jsdelivr.net/gh/zhucheng99/picture/202207091505801.png)

**data complexity**

- 大小
- 特殊的数据结构
- 多样性
- 不同种类的数据难以比较

![](https://cdn.jsdelivr.net/gh/zhucheng99/picture/202207091508240.png)

## 4.3 模型验证

training dataset, validation dataset, test dataset

![](https://cdn.jsdelivr.net/gh/zhucheng99/picture/202207091517863.png)

k-fold cross validation

## 5.1 偏差和方差

三项公式

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-13 21-02-18屏幕截图.png)

如何消除

 ![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-13 21-09-17屏幕截图.png)

## 5.2 Bagging

Bagging - Bootstrap AGGregatING

减少方差误差

Bagging 对 Unstable Learners 更有效
多次采样（物理中多测几次）不改变均值，但会减少方差
不稳定的模型：指多个 learner 之间的方差较大

线性回归一般比较稳定，不需要 bagging
分类（搭配决策树）做 bagging，一般有效果。

- Learn base learners in parallel, combine to reduce model variance
- Each base learner is trained on a bootstrap sample
	- Given a dataset of m examples, create a sample by randomly sampling m examples with replacement
	• Around 1 − 1/e ≈ 63 % unique examples will be sampled use the out-of-bag examples for validation

- Bagging reduces model variance, especially for unstable learners

## 5.3 Boosting

多个弱模型放在一起，降低偏差（均值靠近）。
每个弱模型不独立，按序列

### Gradient Boosting

关键：在残差上 boosting。（第一次是在原始数据上拟合）
不断加一个函数进来，每次新学习一个H(t)来拟合一个负梯度，负梯度是要乘以学习率的。
通过学习率防止一次性拟合残差，避免过拟合。

### Gradient Boosting Decision Trees GBDT

Decision Trees 太强了，怎么弱化它？
Regularize by a small max_depth and randomly sampling features（层取 3 左右）
学习率控制好，不容易过拟合。

GBDT有XGBoost, lightGBM（light 是快的意思）

## 5.4 Stacking

刷榜利器
Combine multiple base learners to reduce variance
与 bagging 的区别是 bagging 在 bootstrap samples 的数据上训练同一个模型。而 Stacking 是在相同数据上训练不同模型。目的都是减少偏差。

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-13 21-39-29屏幕截图.png)

```python
# autogluon 有现成的
from autogluon.tabular import TabularPredictor
predictor = TabularPredictor(label=label).fit(train)
```

上面的代码会列出不同模型对总结果的贡献，还会把不好的模型从结果中剔除。

### Multi-layer Stacking

多层 Stacking 就可以同时降低偏差和方差了。
注意，L1 到 L2 要传一个原始数据，不然损失太大了，因此对同一份数据拟合，多层 Stacking 很容易过拟合。

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-13 21-44-26屏幕截图.png)

如何减少过拟合。
1. 将数据分两份，每层用一半数据作训练。
2. Repeated k-fold bagging：每个模型训练 k 个模型再取平均（bagging 法）

```python
predictor = TabularPredictor(label=label).fit(
train, num_stack_levels=1, num_bag_folds=5)
```

效果如下：
Error: 0.229 → 0.227
Training time: 39 sec → 207 sec (5x)

bagging 和 stacking 能并行
boosting 不能并行

![](https://cdn.jsdelivr.net/gh/zhucheng99/picture/202207122103391.png)

## 9.1 调参

敏感的参数，不敏感的参数
一定要做笔记，不然你会忘了 10 天前做的东西。训练日志和超参数都记录下来。方便大家复现。

adam 调参比较简单
sgd 比较好，但是费力

[weight & bias](https://wandb.ai)

重复别人的实验是很难的，有时候自己的实验也复现不了。
主要有三点：

1. 环境（硬件和库）
	1. 新老 GPU 是有区别的，cpu 就没这种区别。
	2. 写 Python 的人不太注意新版本和就版本的兼容性，要注意 python 的 version。如果你没有作版本控制的话，最好把所有的东西都放一个地方。
2. 代码
3. 随机数（seed）
	1. Dropout 会带来随机性
	2. 库里的某些函数也会有随机性
	3. SGD 会带来随机性

所以一个模型稳不稳还得看能不能重现。

### 自动超参数调节

计算机算力提升，但是人力成本提升。所以要自动化。

通常算法调参 1000 次，往往能干掉 90% 的数据科学家。（大约需要一两周）。机器的优势相当大。人是打不过机器

automl：目标太宏大了，想在机器学习的每一步都实现自动化，但它也作了许多贡献。
- 超参数优化（Hyperparameter optimization(HPO)）
- NAS：Neural architecture search 自动构建网络模型

有人问数据科学家在干什么？我的 autoML 正在优化我的模型。十年前，我的代码正在编译。每个时代，都有技术的痛点。

## 9.2 HPO

```text
momentum [0.89, 0.95] # 在 0.89 到 0.95 中取
```

搜索空间不要太大，价格贵
也不要太小，找不到需要的值

### HPO algorithms: Black-box or Multi-fidelity

两大类算法
黑盒：完全训练每个模型
Multi-fidelity：关心超参数的排序。fidelity 是近似的意思。
- 用小数据，先试试
- 小模型，先试试
- 提前结束不好的超参数组合

具体算法

```yaml
Black-box:
  - Grid search: 
  - Random search: 当你没想法时，这个做法很好
  - Bayesian Optimization: Active 的研究方向，比较复杂，现在研究还在进行，可以等成熟了再去用
  - Successive Halving: SSH 算法
  - Simulated Annealing: 模拟退火
  - Generic Algorithms: 遗传算法
Multi-fidelity:
  - Bandit Based
    - Successive Halving
    - Hyper-Band
  - Modeling Learning Curve
```

```python
# Grid search
for config in search_space:
train_and_eval(config)
return best_result

# Random search
for _ in range(n):
config = random_select(search_space)
train_and_eval(config)
return best_result
```

#### Bayesian Optimization BO

“代理模型” Surrogate model：选下一个参数前，先训练一个选参数的模型。置信区间，采样点 observation(x)，一次一次采样

Acquisition function 获取函数
基于当前挖掘，离采样点近，置信度高
基于当前挖掘，置信度低

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-14 12-51-44屏幕截图.png)


长远情况比随机要好，但是优化难一点，代价大一点
BO 是顺序的，不能并行
BO，刚开始就是随机搜索

#### Successive Halving

Save the budget for most promising config
节约资源给有前景的参数

Randomly pick n configurations to train m epochs
随机选取 `n` 个超参数组训练 `m` 个epoch

每次挑最好的 `n/2` 个超参数多训练 `m*2` 个 epoch

#### Hyperband

对 Successive Halving 的改进，n 和 m 会自动根据 epoch 来调。前面算的少可以多算一点。
![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-14 13-58-39屏幕截图.png)

In Successive Halving
• n : exploration
• m : exploitation

Hyperband runs multiple Successive Halving, each time decreases $n$ and increases $m$
• More exploration first, then do more exploit

## 9.3 NAS algorithms

是自动化人工智能的一大方向
Explainability of NAS result 很重要
手机端（边缘设备）可能更需要高效的神经网络参数搜索

### Neural Architecture Search (NAS)

A neural network has different types of hyperparameters
 - Topological structure: resnet-ish, mobilenet-ish, `#layers`
 - Individual layers: kernel_size, `#channels` in convolutional layer, `#hidden_outputs` in dense/recurrent layers

### NAS with Reinforcement Learning

强化学习和 BO 很像，代价更大。10 年前 2000 个 GPU 训练一天

Zoph & Le 2017

- A RL-based controller (REINFORCE) for proposing architecture

### The One-shot Approach

Combines the learning of architecture and model params
把模型和参数看成一个大模型，既训练架构也训练参数。
这个模型太大了，一般电脑放不下，所以用了 multi-fidelity 的方法

#### Differentiable Architecture Search

可微架构搜索。通过 softmax 来帮忙选，softmax 类似于 注意力机制，弱化不重要的值。比较实用 practical

### Scaling CNNs

简单，适用面少。

A CNN can be scaled by 3 ways:
- Deeper: more layers
- Wider: more output channels
- Larger inputs: increase input image resolutions

EfficientNet proposes a compound scaling
![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img2022-01-14 14-22-15屏幕截图.png)

上面三个参数，最好一起变。

## 10.1 深度神经网络架构

Deep Network Tuning

**批归一化**
不会改变计算结果，只是平滑损失函数。
一般但不能用在深度网络，因为它只能处理一层。
用在 CNN 比较多。
能不用还是不用好，因为它可以用其他方法代替，而且它在计算上比较复杂。

**层归一化**
跟 BN 很像，对之前的所有样本计算均值和方差。
每个新样本算一次就好了。
Transformer 就用了大量的层归一化 。

## 11.1 迁移学习 transfer learning

出圈在深度学习
用模型抽取特征也是迁移学习。

**预训练**
加快微调的训练速度
不会降低精度
只是改变初始权重的设定

**微调**
效果好

## 12.2 NLP中的微调

### 词嵌入

### 基于Transformer的预训练模型

## 词汇

Flow chart 流程图

`#beds` 用井号表示事物的个数

CTR: click thriugh rate 点击率

Curse of dimensionality 维度诅咒

BO：Bayesian Optimization

wall-clock time

Performance estimation 性能评估 