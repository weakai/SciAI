---
title: Bert XAI
date: 2022-7-19
tags: [Bert, XAI]
---

通过 HEAD-MASK 操作对各个 head 进行对比实验，发现了下面几个有趣的点

- attention-head 很冗余/鲁棒，去掉 20% 的 head 模型不受影响
- 各层 transformer 之间不是串行关系，去掉一整层 attention-head 对下层影响不大
- 各个 head 有固定的功能
  - 某些 head 负责分词
  - 某些 head 提取语序关系
  - 某些 head 负责提取 query-title 之间 term 匹配关系

本文主要探讨了在 query-title 分类场景下, bert 模型的可解释性。主要从 attention-head 角度入手，发现 attention 一方面非常的冗余，去掉一部分 head 其实不会对模型造成多大的影响。另外一方面有一些 head 却非常的能影响模型，即使去掉一个都能让模型表现变差不少。同时发现不同的 head 实际上有特定的功能，比如底层的 head 负责对输入进行特征提取，如分词、提取输入的语序关系、提取 query 和 title(也就是上下句)中相同的 term 信息等。这部分底层的 head 提取到的特征会通过残差连接送到高层的 head 中，高层 head 会对这部分特征信息进行进一步融合，最终作为分类特征输入到分类器中。

本文重点讨论了哪些 head 是对模型有正面作用，也就是去掉这些 head 后模型表现变差了。但是如果知道了哪些 head 为什么对模型有负面作用，也就是为什么去掉某些 head 模型效果会更好，实际上对于我们有更多的指导作用。这部分信息能够帮助我们在模型加速，提升模型表现上少走弯路。

#### 诊断分类器

通过可视化的方法有时很难看出一个向量包含什么样的信息。因此我们可以换一个思路。即：如果一个向量包含某个信息，那么它就有能力去做需要这个信息的任务。
因此，另一个常用的方法是：对于模型内部的一个向量，我们可以训练一个简单的模型（通常是线性分类器）去预测一些特征（通常是一些语言学特征）。用这种方法，我们就可以知道，在模型内部的每一层究竟学到了什么。
2019 年，Google 的研究员用这种方法对 Bert 进行了分析。他们将 Bert 每一层输出的 embedding 提取出来，在 8 个任务上用诊断分类器进行试验。
实验表明，Bert 在它的 transormer 层中，进行了传统 NLP 的 pipeline（pos tagging, 指代消解等）的任务。
同时还表明，这些 pipeline 中的细分任务可以动态调整自己以获得更好的表现。下图展示了完成这些任务所需要信息在哪一层的期望值，以及这些层的“重心”在哪里。

![](https://inews.gtimg.com/newsapp_bt/0/12514528813/1000)

从结果可以看出，对于一些底层的任务（例如pos tagging，实体识别等），会在较低层就获得了需要的信息，而对于一些复杂任务（例如指代消解）则在较高层才能获得需要的信息。这和我们的预期相符合。
笔者认为，诊断分类器这种方法可以较为简单地对模型的训练过程进行剖析，并且更适合于深层模型。
**同时，这对于模型的优化能起到启发作用。2020 年 Jo & Myaeng 的 ACL 文章指出，在使用 Bert 时，若选择中间层的一些 embedding 和最后一层的 embedding 一起进行任务，比仅仅用最后一层能够取得更好的效果。**

#### attention

由于 Attention 本身的特点，词与词之间会有一个权重，这个权重也许能够表达出模型的推理过程。
2019 年，斯坦福大学和 facebook 的研究人员对 Bert 中的 attention 的 head 进行分析，发现不同的 head 会提取到不同的信息，且直接用 head 进行细分任务也可以取得较为不错的效果。
然而，对于 attention 是否能够表示模型的推理过程，还存在很多的争议。
同样在 2019 年，Jain & Wallance 的文章《Attention is not explaination》，指出 attention 和 gradient-based 分析方法得出的结论不同，且不同的 attention weight 可以导致相同的预测。
因此主张 attention 和输出结果没有直接的联系。
而 Wiegreffe & Pinter 的文章《Attention is not not explaination》反驳了 J&W 的观点，认为其实验设计不充分，不能证明其结论。
2020 年的 ACL 文章中，关于 attention 的讨论依旧是热点。Abnar & Zuidema 提出一种可以使 attention 更加具有解释性的方法。
由于在第一层 Transformer 之后，不同 token 的信息被混合了起来，因此他们提出一种 rollout 的方法可以将混合的信息抽离出来。
Sen 等人提出，将模型的 attention 和人为标注的 attention 进行比对，并提出计算二者相似程度的方法。
而 Pruthi 等人认为，基于 attention 的解释不具有可靠性，因为可以通过人为操纵的方法使人人为模型是无偏的，而事实却并非如此。
笔者认为，attention 自带可解释的特性，相较于其他模型有着天然的优势。
而如何运用 attention 进行解释，还需要更多的研究。并且，这种方法局限在模型本身，并不适用于其他的模型。
此外，有关 attention 的可解释性如何应用在模型优化上，还需要更多的思考。

#### 对抗数据集

将数据进行一些微小的改变，根据这些扰动是否会对模型结果造成干扰，可以分析出模型的推理过程。
例如，2019 年，Niven & Kao 在 Bert 进行推理任务的数据集中，加入了 not，导致 Bert 的推理能力直线下降到盲猜水平。
由此可以看出，Bert 在进行推理时，是将 not 作为一个明显的“线索”。
若将这个线索改变，模型的能力就会大幅下降。
然而，对抗数据集的构造比较有难度，在某些任务中，需要大量语言学知识。
因此，对抗数据集的构造成本很高。
2020 年 ACL 最佳论文《Beyond Accuracy: Behavioral Testing of NLP Models with CheckList》提出了一套 NLP 模型诊断的方法。
他们提出一个常用语言学特征的 capabilities 和 test types 的矩阵，可以帮助较为完善的构造测试数据集。
他们根据软件测试的方法，提出了三个 capabilities, 即，最小单元（MFT）、不变性（INV）和方向性。
并且提出了一个工具能够快速生成大量的测试数据。

![](https://inews.gtimg.com/newsapp_bt/0/12514528819/1000)

笔者认为，这种 checklist 的方法，具有较低的成本，并且能够较为完善的构造对抗数据集。
虽然其无法解释出模型的推理过程，但是它可以解释出模型是“怎样看数据”的。
即，哪些类型的数据扰动会对模型产生较大的影响，模型更关注哪些类型的扰动信息等等。
这在实际业务场景中，可以作为模型鲁棒性保证的方法。
目前我们也已经尝试用这种方法进行模型评测。

#### 局部代理模型

局部代理模型是一种“事后分析”的方法。
它可以适用于所有的模型。
它的基本思想是，用一些可解释的模型，去局部拟合不可解释的黑盒模型，使得模型结果具有一定的可解释性。

2016 年，Ribeiro 等人提出了 LIME 方法。
对于每一条数据，对其进行微小的扰动，生成一个数据子集，并且得到模型对于这个子集中的数据的预测结果。
然后，用可解释的模型（例如线性回归），对这个子集中的结果进行拟合，以此就可以局部解释这条数据的结果。

类似的方法还有 SHAP。然而，这种方法得到的解释并不具有鲁棒性。

因为我们认为，微小的调整并不会对结果造成很大的影响，而这对于局部代理模型而言并不总是成立。
并且，如果训练数据和测试数据的分布不同，局部代理模型的结果就不可信。

Rahnama 等人的研究《A study of data and label shift in the LIME framework》就提出了这一点。
他们通过实验发现，通过 LIME 的扰动过程得到的数据和训练数据的分布有较大的差异。
此外，LIME 的解释很大程度上依赖于超参数的选择。例如，应该选择多少个扰动数据点？
如何设置这些点的权重？
正则化的强度需要多少？

笔者认为，局部代理模型虽然能够对所有模型进行解释，但这个解释并不能反映模型的推理过程，而是“人们认为的模型推理过程”。
在业务场景中，对于决策支持的模型（例如智能投顾）也许有效，但是这种方法的 faithfulness 仍然值得讨论和研究。

## Reference

- [BERT 可解释性-从"头"说起](https://zhuanlan.zhihu.com/p/148729018)
- [让我如何相信你-NLP 模型可解释性的 6 种常用方法](https://new.qq.com/omn/20200923/20200923A0HQWO00.html)