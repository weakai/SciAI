---
title: 黑盒优化
date: 2022-7-20
---

## 简介

黑盒优化在工业设计中有着广泛的应用，机器学习中的超参数优化是黑盒优化的一种。近年来，随着深度学习的不断发展，深度学习建模中涉及到的超参数数量远超传统机器学习方法（如 XGBoost、SVM 等)。机器学习中的超参数优化，特别是深度学习，越来越凸显重要作用，引起了学术界和工业界的重视：

- 2021 年，QQ 浏览器在国际综合 AI 学术会议之一顶会 CIKM 和中国计算机学会（CCF）多媒体技术专委会支持下举办 2021AI 算法大赛。自动超参数优化赛题为超参数优化问题或黑盒优化问题：给定超参数的取值空间，每一轮可以获取一组超参数对应的 Reward，要求超参数优化算法在限定的迭代轮次内找到 Reward 尽可能大的一组超参数，最终按照找到的最大 Reward 来计算排名。
- 2020 年， 在 2020 NeurIPS 举办了黑盒优化的比赛，比赛任务是对机器学习中的超参数进行优化。

在有限的参数搜索限制条件，寻找能够使机器学习模型达到一个较优的解(或者Loss)，是一个很具挑战的黑盒优化问题：

## 贝叶斯优化

近年来贝叶斯优化在求解黑盒函数问题中应用越来越广泛，已经成为超参数优化的主流方法。贝叶斯优化是一种全局优化的方法，目标函数只需要满足一致连续或者利普希茨连续等局部平滑性假设；通过引入采集函数，进行有效探索和利用，能在较少的评估次数下取得复杂目标函数的近似解。

## 背景

自动超参数优化是黑盒优化的一个具体应用。黑盒优化具有如下的特点：

- 优化目标：没有显式的函数表达式，获取不到其梯度信息；其值域分布通常是多峰、非凸非凹的复杂昂贵函数；
- 函数空间：由连续、离散整数值构成的，且实际应用中维度较高；
- 优化目标求解：求解非常昂贵耗时，在工业设计中通常一次评估花费数十万至百万元以上；
- 在大规模的深度学习训练中通常花费若干天甚至数周时间进行一次参数搜索。

黑盒优化的求解方法：

- 元启发式算法(meta-heuristic algorithm) ：遗传算法 (genetic algorithm, GA)、差分进化(differential evolution, DE) 算法、粒子群算法 (particle swarm optimization, PSO)、进化策略(evolutionary strategies, ES)
- 无导数优化方法 ：随机搜索、直接搜索法；基于模型的如多项式方法，径向基函数插值方法和基于代理模型方法，如贝叶斯优化。

近年来，贝叶斯优化在求解黑盒函数问题中应用越来越广泛，已经成为超参数优化的主流方法。贝叶斯优化的优势：

- 是一种全局优化的方法，目标函数只需要满足一致连续或者利普希茨(Lipschitz)连续等局部平滑性假设；
- 能在较少的评估次数下，取得复杂目标函数的近似解；
- 引入采集函数，进行有效的探索和利用(Exploration and Exploitation)

我们将在本节末尾讨论贝叶斯优化的局限性。

## 贝叶斯优化建模

### 问题定义

贝叶斯算法求解超参数优化问题，通常分为两步：

- 第 1 步：学习一个代理模型(Surrogate Model)；
- 第 2 步：通过采集函数(Acquisition Function)，来决定输出下一个采集点。

### 代理模型

当黑盒函数的自变量是连续值时，**高斯过程回归模型(GPs)**是一个非常高效的代理模型。
对于黑盒函数，我们通常假设其符合某一个 GPs 先验分布。高斯过程回归模型(GPs)有如下的两方面来决定：

- 均值函数 $m(x)$ 
- 协方差函数(矩阵)，或者核函数 $k_{\theta}(x,x\prime)$，其中 $\theta$ 为核函数超参数。

通常假设 GP kernel 是平稳的，只依赖于两个点 $x$ 和 $x\prime$ 之间的模，进一步我们假设在带噪音分布条件数据点是平稳的且同质的。如果我们观测的数据点 ${x,y}$ 不满足这个条件，我们将不能很好的近似黑盒函数。

### 采集函数

接下来我们列出贝叶斯优化中常见的 3 种采集函数

- 提升的概率(PI)
- 提升的期望(EI)
- 上置信边界(UCB):

![](https://pic2.zhimg.com/80/v2-8878cbc7540cdbc00db2da60552eb3a9_720w.jpg)

### 贝叶斯优化进一步探讨

贝叶斯优化中存在的难点：

- 代理模型：我们通常选取 GPs；当我们求解的问题维度变高时，GPs 求解的复杂度变高；如何求解高维的贝叶斯优化问题，是一个值得探讨和研究的问题；
- 采集函数：通常选取上述函数中的一个作为代理模型的采集函数（中途不会更改采集函数）。这个采集函数相对于我们的代理模型和我们要优化的问题是否是最优的呢？我们在 HEBO 这一节详细讨论这个问题；
- 和具体优化问题相关：在应用贝叶斯优化求解实际问题，通常要根据实际问题，结合相关的领域知识选择合适的代理模型和采集函数，如我们后续讨论的没有”一招鲜吃遍天“的方法。

## 实验设计

什么是实验设计？DOE（Design of Experiment）实验设计，实验设计通过对实验进行合理安排，以较小的实验规模和较短的实验周期和较低的实验成本，获得理想的实验结果以及得出科学的结论。本文介绍了实验设计的基本概念、点集偏差、拟随机序列和拉丁超立方生成方法，推荐在实际应用中使用 Sobol 和拉丁超立方序列。

现代实验设计发展历程

![](https://pic2.zhimg.com/80/v2-69a7400944f690d5661ebab21c510be1_720w.jpg)

实验设计在黑盒函数优化、超参数优化中有较广泛的应用。如上图所示，现代实验设计发展起来的方法有如下几种

- 静态方法(Static)
  - 和实验系统无关(Systerm free)，图中圆圈1
    - 蒙特卡洛、拟蒙特卡洛方法
    - 拉丁超立方方法、最优拉丁超立方方法
  - 和实验系统相关(Systerm aided)，图中圆圈2
    - 最大熵、最小均方误差 
  - 自适应方法(Adaptive)，图中圆圈3
    - MSD-CV 方法
    - LOLA-Voroni 方法

在超参数优化中一个重要的问题是在初始阶段如何有效的生成初始点集合。这个初始点集合一般来说有如下的要求：

- 在定义域分布均匀；
- 相邻的维度相关性小，特别是在高维情况；
- 计算速度快。

### 点集偏差

在超参数(黑盒)优化初始阶段，需要从实验设计空间中采样若干个点作为初始点集，记为 $S$。如何评估我们采样点集 $S$ 的质量呢？我们在对求解的黑盒函数一无所知的时候，最占优的策略是从实验设计空间均匀的采样，这点和机器学习的最大熵原理类似。

评估采样点集 $S$ 的好坏，可以通过点集偏差来评估。通俗讲，偏差是评估采样点集生成均匀程度的度量，偏差越小，生成的点集质量越好。

#### 一维点集偏差

#### 任意有界区域中的点集偏差

### 拟蒙特卡洛采样

随机序列的生成可以大致分为如下两类：

- 伪随机序列：其均匀性是随机均匀性， 如目前广泛使用的线性同余生成器；
- 拟随机序列：其均匀性是等分布均匀性， 如 van Der Corput 序列、Halton 序列、Faure 序列及 Sobol 序列。

用随机数进行拟随机数代替蒙特卡洛模拟，称为拟蒙特卡洛方法（采样）。

#### 低偏差序列

拟随机数序列是低偏差序列。低偏差序列通过将一系列的整数表示成某个基radix(base)的位数形式：

#### 低偏差序列适用场景

Halton 序列在在 $N>14$ 的情况下容易产生高度从聚和相关的样本，使得求解的黑灰函数容易陷入局部最优解。在实际使用中建议 Halton 序列在 $N<8$ 维下使用。我们在 QQ 浏览器超参数比赛中，因为测试的维度 $N<3$，表现好于拉丁超立方，在提交算法也使用 Halton 序列，实际线上评估数据可能是超高维的，算法可能会陷入局部最优解。

上面我们讨论的几种常见的拟随机序列其适用：

![](https://pic4.zhimg.com/80/v2-c368615b6c63fe1cee33b7d2daad327f_720w.jpg)

### 拉丁超立方采样

![](https://pic1.zhimg.com/80/v2-505dd5339f6ef95f97aaf07e1a0f7bb8_720w.jpg)

上图展示了拉丁超立方在 N=2 维空间内生成 p=20 个点的情形：

- 图9-a是最坏的情形；
- 图9-b是随机情形，呈现随机性；
- 图9-c是拉丁超立方采样，呈现均匀性。

#### 关于拉丁超立方的几个问题

> 为什么拉丁超立方会为大家喜爱

一个好的的采样算法，需要满足如下的条件：

- 生成的采样点，能够适应不同的统计假设；
- 生成的采样点，能够覆盖由小到大的设计空间。

拉丁超立方恰好能满足上面两点，使得它能成为一种有效的实验设计方法。

另外一个原因是拉丁超立方具有较好的可扩展性，非退化的(Non-collapsing)：
**将 N 维空间去掉若干维，剩余维度构成的样本，仍然是一个拉丁超立方样本。**

> 拉丁超立方的用处有多广泛？

正如我们前面提到，拉丁超立方方法是和模型无关的，也不会假设拉丁超立方生成点和模型输出有较大的相关性。基于拉丁超立方的优化问题，实际上是个组合优化问题：p 个样本点、N 维空间，其对应的空间复杂度为 $O(p!^d)$。 **单独使用拉丁超立方方法求解黑盒问题，实际上并不高效，需要结合其他的优化方法才能进行有效的搜索，这是本系列文章后续讨论的内容。**

> 拉丁超立方有什么缺点呢？

在工业设计中，我们可能需要将原实验设计空间(N 维)，迁移到目标实验设计空间总去(M):
当 $M<N$ 时，由拉丁超立方的非退化性，原实验设计空间建模可以复用在目标实验设计空间去；但不能保证在原空间求得的最优解可以复用在目标实验设计空间去；
当 $M>=N$ 时，此时原实验设计空间建模不能复用在目标实验设计空间，需要重新建模。

> 当我们进行实验设计时，拉丁超立方总是可以使用的吗？

答案是依据我们求解的问题而定。正如我们在讨论HEBO的章节中所说的，没有”一招鲜吃遍天“的方法。我们可以尝试多种方法，或者将他们组合来用，更好的服务于我们需要求解的问题。

## TuRBO：基于信赖域的贝叶斯优化

摘要：TuRBO 是一种全局优化方法， 通过构建一系列的局部 GPs 代理模型，从全局角度能够避免过度探索(Exploitation)；同时在局部能够充分利用信赖方法的二阶收敛性，进行高效的求解(Exploitation)。TuRBO 方法近年来在机器学习超参数优化中表现效果优异，本文介绍了其原理和代码详解。

### TuRBO 简介

在贝叶斯优化中，我们通常尝试构建一个全局模型；全局模型过度强调了探索(Exploration)，在高维场景中，不能很好的进行利用(Exploitation)。 高维黑盒问题优化，通常面临如下的问题：

- 随着求解问题维数增大，难度呈指数增长；局部最优解将变得非常的稠密，寻找全局最优解将变得困难；
- 黑盒函数通常是各种各样的，很难找到一个全局的代理模型来近似；
- 搜索空间相比于采样点个数(Budget)，随着维数的增大将变得巨大。

TuRBO 方法结合了信赖域和贝叶斯优化，近年来在机器学习超参数优化中表现效果优异。其主要的思想：

- 基于当前的评估点集，构建 GPs 代理模型；
- 选取当前最小点，以 GPs 代理模型的 lentgh_scale 作为权重半径，构建信赖域 $\Omega$；
- 在信赖域 $\Omega$ 内，随机抽样若干个点；用 GPs 模型预测出最大后验候选点集；
- 根据当前最大候选点集的函数值，增大或者缩小信赖域半径；
- 结束条件：信赖域半径小于给定值或者大于给定次数。

TuRBO 是一种全局优化方法， 通过构建一系列的局部 GPs 代理模型，从全局角度能够避免过度探索(Exploitation)；同时在局部能够充分利用信赖方法的二阶收敛性，进行高效的求解(Exploitation)。

### 总结

在 2020 NeurIPS 举办了黑盒优化的比赛中，在 128 个评估点集下，TuRBO 方法在作为单个求解算法，是非常的优秀的:

![](https://pic2.zhimg.com/80/v2-558a0b4942a66e8a934a30c3a7af1599_720w.jpg)

如果将评估点限定为 50 个，在 Q Q浏览器超参数优化比赛中，我们将看到 TuRBO 方法因为收敛较晚，效果也下降了。

## 五、MCTS+TuRBO：基于蒙特卡洛树搜索的贝叶斯优化

本文介绍了一种基于蒙特卡洛树搜索 MCST+TuRBO 的黑盒函数优化方法。主要的思想是在全局函数空间，通过二叉树递归的将函数空间划分为高函数值和低函数值两部分；能够有效的对高潜力的函数空间进行搜索，特别是在高维情况；在上述方法划分出的局部空间内，使用基于信赖域的二阶求解方法进行高效的求解。

- 基于蒙特卡洛树搜索空间划分。通过二叉树，递归的将函数空间划分为高函数值和低函数值两部分；在每次函数空间划分时，通过聚类算法聚成两类（高潜力点、低潜力点），使用两类类别信息构建出非线性的决策面（SVM）,能够有效的对高潜力的函数空间进行搜索，特别是在高维情况；
- 在局部空间使用 TuRBO 进行求解。在上述方法划分出的局部空间内，使用基于信赖域的二阶求解方法进行高效的求解。

MCST+TuRBO在全局空间的探索上，表现出色；当问题有较多的局部最优解时，容易陷入局部最优解，跳不出来。后面我们介绍的《HEBO：异质方差进化贝叶斯优化》、 《pySOT和POAP: 代理模型优化工具箱及其异步并行框架》将在一定程度上弥补MCST+TuRBO的不足。

## HEBO：异质方差进化贝叶斯优化

HEBO 通过分析经典贝叶斯优化中数据输入、数据输出、代理模型和采集函数存在的问题和局限性，并对每一个问题做了相应的优化：对数据输入、数据输出进行变换校准；将数据变换校准和 GPs 核函数一起联合优化；引入多目标采集函数，对候选点进行更鲁棒的探索。

近年来，贝叶斯优化在求解黑盒函数问题中应用越来越广泛。使用贝叶斯优化来求解黑盒函数问题通常会做一些较为严格的假设。

- 建模假设，使用代理模型来近似黑盒函数问题。常见的代理模型是高斯过程回归模型（Gaussian processes，GPs)。GPs模型本身是同方差的（Heteroscedastic），但其实际建模的问题是带异质方差噪音，同时同方差的 GPs 模型不能处理非平稳的评估点集，这种现象在机器学习超参数优化中尤为常见。
- 采集函数和优化器假设。在采集函数阶段，通常是最大化采集函数并输出下一个候选点。这一过程将引入额外的限制和假设。如假设黑盒函数中只包含连续变量，采取一阶或二阶（如LBFGS，ADAM）来求解，而忽略了超参数优化中包含的离散变量（如深度网络中的隐层大小）。另外在整一个贝叶斯优化框架中采集函数只选择一种，忽略了多个采集函数组合的情况。

## HEBO

<https://link.zhihu.com/?target=https%3A//github.com/huawei-noah/HEBO/blob/master/HEBO/hebo/optimizers/hebo.py>

本文介绍的 HEBO 方法，在 2020 BBO 大赛和 QQ 浏览器 2021AI 算法大赛中表现都非常优异。HEBO 算法框架仍然是贝叶斯优化（Bayesian Optimisation）。作者通过分析经典贝叶斯优化的数据输入、数据输出、代理模型和采集函数存在的问题和局限性，对每一步的问题都做了相应的优化：

- 对数据输入、数据输出进行变换校准；
- 将数据变换校准和 GPs 核函数联合在一起优化；
- 引入多目标采集函数，进行更鲁棒的探索（HEBO方法最大亮点)。

## pySOT 和 POAP: 代理模型优化工具箱及其异步并行框架

POAP 是一个以构建和组合多个异步优化策略，专门为解决黑盒函数全局优化问题而设计的事件驱动框架。pySOT 是基于 POAP 框架，实现了同步和异步代理模型策略优化开源包。

## References

- [黑盒优化：超参数优化算法最新进展总结](https://zhuanlan.zhihu.com/p/488588215)