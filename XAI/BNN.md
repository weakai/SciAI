# 贝叶斯神经网络

## 贝叶斯思想: 我对世界的看法随世界变化而随时变化

用一句话概括贝叶斯方法创始人 Thomas Bayes 的观点就是：任何时候，我对世界总有一个主观的先验判断，但是这个判断会随着世界的真实变化而随机修正，我对世界永远保持开放的态度。

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img532548-20191016101117699-1196006334.png)

1763 年，民间科学家 Thomas Bayes 发表了一篇名为《An essay towards solving a problem in the doctrine of chances》的论文

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/img532548-20191016101336670-865850886.png)

这篇论文发表后，在当时并未产生多少影响，但是在20世纪后，这篇论文逐渐被人们所重视。
人们逐渐发现，贝叶斯方法既符合人们日常生活的思考方式，也符合人们认识自然的规律，经过不断的发展，最终占据统计学领域的半壁江山，与经典统计学分庭抗礼。

当时数理统计的主流思想是“频率学派”。
所谓频率学派，举个例子来说：“有一个袋子，里面装着若干个白球和黑球（例如3黑2白），请问从袋子中取得白球的概率 $$a$$ 是多少？

频率派把需要推断的参数 θ 看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值。同时，样本X是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X的分布。

这种观点看起来确实没有什么问题，也很合理。但是我们仔细深度思考一下就会发现一个重大问题。

频率学派之所以能够得到这种确定性，是因为研究的对象是“简单可数事件”，例如装有固定数量球的袋子、只有正反两面的硬币、只有6面的标准筛子。但是当研究的问题变得复杂之后，频率理论就无法处理了。

例如一个朋友创业，你现在需要估计他创业成功的几率有多大？

这个时候你就无法逐一枚举出导致他成功或者失败的所以子原因了（做法有方法？思路清晰？有毅力？能团结周围的人？和其他竞争对手相比，好多少？....），这是一个连续且不可数的事件空间。

让我们暂时回到Thomas Bayes所处的学术时代18世纪，来一起体会下贝叶斯的思想。

贝叶斯主义来自于统计学中的一个定律—贝叶斯定律： 对一件事发生概率的判断，会先考虑这件事的先验概率，然后根据新的数据和信息，得出后验概率。
而贝叶斯主义，就是对新信息保持开放的态度，不断地修正自己的判断。


贝叶斯神经网络，简单来说可以理解为通过为神经网络的权重引入不确定性进行正则化（regularization），也相当于集成（ensemble）某权重分布上的无穷多组神经网络进行预测。

贝叶斯神经网络不同于一般的神经网络，其权重参数是随机变量，而非确定的值。如下图所示：

![](https://pic2.zhimg.com/80/v2-af5a6d9aacbd34618d8441a3c04f5fed_720w.jpg)

也就是说，和传统的神经网络用交叉熵，mse 等损失函数去拟合标签值相反，贝叶斯神经网络拟合后验分布。

这样做的好处，就是降低过拟合。

## 原理

### 0. 神经网络的概率模型

众所周知，一个神经网络模型可以视为一个条件分布模型。
在分类问题中这个分布对应各类的概率，在回归问题中一般认为是（标准差固定的）高斯（Gaussian）分布并取均值作为预测结果。
相应地，神经网络的学习可以视作是一个最大似然估计（Maximum Likelihood Estimation, MLE）。
回归问题中我们代入高斯分布就可以得到平均平方误差（Mean Squared Error, MSE），分类问题则代入逻辑函数（logistic）可以推出交叉熵（cross-entropy）。
求神经网络的极小值点一般使用梯度下降，基于反向传播（back-propagation， BP）实现。

MLE 中不对 $w$ 的先验概率作假设，也就是认为 $w$ 取什么值的机会都均等。
如果为 $w$ 引入先验，那就变成了最大后验估计（Maximum Posteriori, MAP）

### 1. 贝叶斯起来了

贝叶斯估计（bayesian estimation）同样引入先验假设，与 MAP 的区别是贝叶斯估计求出 $w$ 的后验分布，而不限于 argmax 值，这样我们就可以为神经网络的预测引入不确定性。
但是求后验分布也是件麻烦的事情。
所以，为了在神经网络中引入贝叶斯估计，需要找到方法近似这些东西，并且最好能转化成为求解优化（optimization）问题的形式，这样比较符合我们炼丹师的追求。

### 3. 变分估计: 基于变分推断的 BNN 训练

如果直接采样后验概率 $P(W|D)$ 来评估 $P(W|X,D)$ 的话，存在后验分布多维的问题，而变分推断的思想是使用简单分布去近似后验分布。

这个过程可以通过最小化两个分布的 KL 散度（Kullback-Leibler divergence）实现。

### 4. 遇事不决，蒙特卡罗

蒙特卡罗方法（Monte Carlo method）是刻在炼丹师 DNA 里的方法。

众所周知，同样利用贝叶斯估计推导出来的变分自编码器（Variational Auto-Encoder, VAE）引入了一个妙不可言的重参数化（reparameterize）操作。

### 5. 贝叶斯小批梯度下降

实践中的现代炼丹都是采用的小批梯度下降（mini-batch gradient descent），所以需要相应地缩放复杂性代价。

### 6. 局部重参数化

神经网络权重中引入的不确定性可以看作是全局的（global）不确定性。
在神经网络中引入全局不确定性意味着在推理计算（inference）过程中要对全局所有参数进行采样操作，这个代价其实要比想象中高昂。

如果所有参数都是独立高斯分布，那么进行矩阵乘法后的结果也都会是独立高斯分布。
因此，我们就没有必要每次都采样参数 $W$ 了，可以直接计算出结果 $W$ 的均值和方差进行采样，然后反向传播到 $W$ 上。
这样每次计算进行的采样都是相应数据点的局部（local）采样，这个技巧称为局部重参数化（local reparameterization）。

## BNN 模型

### 神经网络和 BNN

神经网络： 使用全连接网络来拟合数据，相当于使用多个全连接网络。 但是神经网络容易过拟合，泛化性差；并且对预测的结果无法给出置信度。

BNN: 把概率建模和神经网络结合起来，并能够给出预测结果的置信度。

先验用来描述关键参数，并作为神经网络的输入。
神经网络的输出用来描述特定的概率分布的似然。
通过采样或者变分推断来计算后验分布。
同时，和神经网络不同，权重 W 不再是一个确定的值，而是一个概率分布。

### 应用

BNN 不同于 DNN，可以对预测分布进行学习，不仅可以给出预测值，而且可以给出预测的不确定性。
这对于很多问题来说非常关键，比如

1. 机器学习中著名的 Exploration & Exploitation （EE）的问题
2. 在强化学习问题中，agent 是需要利用现有知识来做决策还是尝试一些未知的东西；
3. 实验设计问题中，用贝叶斯优化来调超参数，选择下一个点是根据当前模型的最优值还是利用探索一些不确定性较高的空间。
4. 比如：异常样本检测，对抗样本检测等任务，由于 BNN 具有不确定性量化能力，所以具有非常强的鲁棒性。

## 实践

### Awesome



### Repositories

- [1.3k | Bayesian-Neural-Networks | 3 years ago](https://github.com/JavierAntoran/Bayesian-Neural-Networks)

## Links

- [必读之墙裂推荐 | 概率图模型（PGM）：贝叶斯网（Bayesian network）初探](https://www.cnblogs.com/LittleHann/p/11683607.html)
- [必读 | 贝叶斯神经网络BNN(推导+代码实现)](https://zhuanlan.zhihu.com/p/263053978)
- [必读 | Bayesian Neural Networks：贝叶斯神经网络](https://zhuanlan.zhihu.com/p/81170602)
- [成功的人，都是贝叶斯主义者](https://zhuanlan.zhihu.com/p/462368360)
