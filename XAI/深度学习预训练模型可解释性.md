---
title: 深度学习预训练模型可解释性
date: 2022-7-19
tags: []
---

## 引言

### 主流的分析方法

#### Analyse Pre-trained Model

我们都知道预训练（pre-train）方法因为数据量巨大需要消耗大量的计算资源，那这么多计算资源最后都变成了啥？
或者说在整个 pre-train 的过程中，模型到底学到了什么能力？以下的 pre-train model 以 BERT 为例。

- Probing tasks：一般做法是设计一些简单的分类任务（一种任务包含一种特定的语言学信息），将 BERT 每一层的输出作为 embedding，在得到 embedding 后通过简单的分类器（例如一层的 FFN，为了不过多干扰模型本身的能力），来验证模型的不同层在不同 probing task 上的能力。
- Visualization：我们都知道 BERT 是由多层 Transformer 构成的，每一层 Transformer 都有一个 multi-head self-attention 模块。在这个模块中，每个 word 和其他所有 word 都会有交互，从而得到一个注意力（attention）分数，通过可视化这个分数，可以知道在不同层，甚至不同 head 中，不同的 word 关注的部分。还有的做法是将不同层的特定词 embedding 做聚类，同样也可以了解模型不同层的能力。

#### Analyse Fine-tuned Model

目前 NLP 许多任务的 SOTA 都是通过微调（fine-tune）预训练（pre-train）的模型达到的。
那么问题来了，在 Fine-tune 的过程中，模型又学到了什么能力？
在这一部分同样有 Probing tasks 和 Visualization 两种做法，而且做法同上述方法类似，只是将 pre-train 的 model 作为对比对象。
Adversarial attacks：对于 fine-tuned model，对抗性攻击是一种新做法。
对抗性攻击通过使用特定干扰信息创建的例子来验证模型的鲁棒性。
具体例子请见之后的论文。

## Pre-trained model Analysis

### Probing tasks Method

Open Sesame: Getting Inside BERT’s Linguistic Knowledge@ACL 2019

本文在尝试解释 BERT 上主要做了两件事：
- 通过设计 probing tasks 来证明 BERT 在 pre-train 之后得到的 word embeddings 包含语法信息
  - Figure 1 在第 4 层突然准确率变得很高，作者认为是因为浅层特征是一个高度 localized 的特征，包含的句子上下文信息不足以确定 main auxiliary。
    - 随着 self-attention 的作用，层数加深，信息变得越来越抽象，到 4-6 层左右正好能足够表示 main auxiliary，之后层数再加深，信息变得更抽象，导致 acc 有所下降。
  - 在复合名词的分类任务（Figure 2 是复合名词的结果）中，large 的效果就要差很多了。
    - base 上的效果很好，作者认为是由于主语名词特征的显著性。至于为什么 large 效果很差，作者也没有给出很好的解释。
- 提出了基于 attention 的对主谓一致和共指消解问题的评价分数，根据 self-attention 来确定 BERT 对语言相关元素的关注程度。
  - 作者提出了一个叫“confusion score”的概念，定量地表示一句话给模型带来的混淆程度。
  - 作者观察到的 BERT 的自我注意值对语法扭曲的敏感性表明，BERT 的句法知识实际上被编码在其注意力矩阵中。
  - 共指消解的混淆程度与层深度的负相关。
  - 主谓一致性的混淆分数呈现出相似的趋势。作者推测 BERT 随着层数的加深，包含的信息也更抽象。

### Visualization Method

- What does BERT look at? An Analysis of BERT’s Attention@ACL 2019
- 本文主要通过可视化 BERT 的 attention 层，发现有很多 attention 集中在 `[CLS]` 和 `[SEP]` 符号上
- 文章还通过实验得出不同的 multi-head 中不同的 head 关注的是不同的语法现象
- 此外文章也实验了融合多个 head，得出模型关于语法整体的建模程度
- 作者通过可视化 attention 的权重，得到了几种不同的 attention 的 pattern
  - 有的 attention 是 broadly 形式（关注所有的词）
  - 有的 attention 是关注的是其下一个 token
  - 有的 attention 则集中关注 `[SEP]` 符号上
  - 还有的 attention 则关注标点符号。

- 作者在分析 attention 的过程中发现一个很有趣的现象
  - 在 6-10 层，有超过一半的 attention 关注的是 `[SEP]`，一开始作者的想法是 `[SEP]` 用于聚合段与段之间的信息，方便后面的 head 去获取信息。
  - 如果假设成立的话，那 `[SEP]` 关注的信息应该是整句话，但是 Figure 2 的下图表明，90% 以上的 `[SEP]` 关注的是他们自身，所以作者又推测，指向 `[SEP]` 很可能是一种 “no-op”（无作用）的操作
  - 为了进一步验证以上的假设，作者又采用了一种基于梯度的可以衡量特征重要性的方法（Sundararajan et al., 2017） 简单地说，这个值的大小意味着将注意力转移到 token 上会多大程度改变 BERT 的输出。Figure 3 中可以看到，在 5 层之后，`[SEP]` 的梯度就基本下降到 0，说明这个时候给 `[SEP]` 或多或少的 attention 都改变不了模型的输出，进一步验证了作者“no-op”的结论。

## Fine-tuned Model Analysis

### Adversarial attacks

What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?

目前最新模型已经在多个 MCRC 数据集上实现了 SOTA。
然而，作者认为这样的表现可能并不能反映模型真正的语言理解和推理能力。
在这项工作中，作者采用了两种方法来研究 BERT 从 MCRC 数据集中学到了什么：

- 不可读的数据攻击：添加一些不可读数据来混淆 BERT，误导其作出错误的判断
  - 这意味着 BERT 无法检测出正确的词序和语法，严重依赖于关键词匹配
- 不可回答的训练数据：增加不可回答的训练数据或者洗牌 examples 来训练 BERT
  -  BERT 仍然可以从无序输入中学习，这表明它对词序或句法信息不敏感



## Reference

- [深度学习预训练模型可解释性概览](https://zhuanlan.zhihu.com/p/109289951)