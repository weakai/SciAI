---
title: 过拟合
---

解决过拟合问题有两个方向：降低参数空间的维度或者降低每个维度上的有效规模（effective size）。

降低参数数量的方法包括greedy constructive learning、剪枝和权重共享等。降低每个参数维度的有效规模的方法主要是正则化，如权重衰变（weight decay）和早停法（early stopping）等。

## 早停法 early stopping

早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。

主要步骤如下
1. 将原始的训练数据集划分成训练集和验证集
2. 只在训练集上进行训练，并每个一个周期计算模型在验证集上的误差，例如，每 15 次 epoch（mini batch训练中的一个周期）
3. 当模型在验证集上的误差比上一次训练结果差的时候停止训练
4. 使用上一次迭代结果中的参数作为模型的最终参数

如何使用早停法
我们需要一个停止的标准来实施早停法。因此，我们希望它可以产生最低的泛化错误，同时也可以有最好的性价比，即给定泛化错误下的最小训练时间。

- [[过拟合]早停法 (Early Stopping)](https://codeantenna.com/a/IE1DvYd5Lx)
- [Keras Callbacks | Early Stopping](https://www.cxyzjd.com/article/zwqjoy/86677030)

EarlyStopping 的参数:
- monitor: 监控的数据接口，有’acc’,’val_acc’,’loss’,’val_loss’等等。正常情况下如果有验证集，就用’val_acc’或者’val_loss’。但是因为笔者用的是 5 折交叉验证，没有单设验证集，所以只能用’acc’了。
- min_delta：增大或减小的阈值，只有大于这个部分才算作improvement。这个值的大小取决于 monitor，也反映了你的容忍程度。例如笔者的 monitor 是’acc’，同时其变化范围在70%-90% 之间，所以对于小于 0.01% 的变化不关心。加上观察到训练过程中存在抖动的情况（即先下降后上升），所以适当增大容忍程度，最终设为0.003%。
- patience：能够容忍多少个 epoch 内都没有 improvement。这个设置其实是在抖动和真正的准确率下降之间做 tradeoff。如果patience 设的大，那么最终得到的准确率要略低于模型可以达到的最高准确率。如果 patience 设的小，那么模型很可能在前期抖动，还在全图搜索的阶段就停止了，准确率一般很差。patience的大小和 learning rate 直接相关。在 learning rate 设定的情况下，前期先训练几次观察抖动的 epoch number，比其稍大些设置 patience。在 learning rate 变化的情况下，建议要略小于最大的抖动 epoch number。笔者在引入 EarlyStopping 之前就已经得到可以接受的结果了，EarlyStopping 算是锦上添花，所以patience 设的比较高，设为抖动 epoch number 的最大值。
- mode: 就’auto’, ‘min’, ‘,max’三个可能。如果知道是要上升还是下降，建议设置一下。笔者的monitor是’acc’，所以mode=’max’。

min_delta 和 patience 都和“避免模型停止在抖动过程中”有关系，所以调节的时候需要互相协调。通常情况下，min_delta 降低，那么 patience 可以适当减少；min_delta 增加，那么patience 需要适当延长；反之亦然。

