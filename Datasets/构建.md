---
title: 数据集
---

对于小数据量（100~10000），可以把数据70(trainning set)：30(test set)，或者60(trainning set)：20(dev set)：20(test set)。

对于大数据量（百万级别），验证集和测试集占数据总量的比例会趋于变得更小。因为验证集的目的就是检验哪种算法更有效，所以我们可能不需要拿出20%的数据作为验证集。（1000000-10000-10000）

---

数据集来源不同，会导致各数据集之间的不同分布。这种情况应该避免（即确保验证集和测试集的数据来自同一分布）。

如果不需要对神经网络系统做出无偏评估，则只有验证集 dev set，没有测试集 test set 也 OK

---

只要**正则**适度，通常来说构建一个更大的网络便可以在不影响 variance 的同时来减少 bias。而采用更多数据通常可以在不过多影响bias的同时减小variance。这样我们就得到了一个非常规范化(well regularize)的网络。通过正则化 regularization，训练一个更大的网络而几乎没有任何负面影响。训练一个大型神经网络的主要代价也只是计算时间，当然前提是网络是规范化 regularized 的。正则化 regularization是一种非常实用的减少方差variance的方法。 虽然bias可能会略有增加，但如果网络足够大，bias的增幅通常不会太高。

---

#### 数据漂移

[浅谈数据漂移](https://paxinla.github.io/posts/2020/04/qian-tan-shu-ju-piao-yi.html)

数据漂移(Data Drift)，指的是将进入数据仓库的数据，偏离标准、正常或预期的情况。它三种表现形式:

- Structural Drift ，指的是数据源的数据结构发生了改变，比如字段增减、字段数据类型变更等。
- Semantic Drift ，源于(历史)数据的改变，比如过了一段时间后，才发现已入仓库的历史数据有错误等。
- Infrastructure Drift ，源于系统依赖的软件或平台发生变化后，与变化前不兼容。

数据漂移的后果主要有3个

- Garbage in, garbage out.
- Trust takes a lifetime to build and only a moment to lose.
- The biggest expense is opportunity cost.

传统的 ETL 过程是脆弱且不透明的，这意味着它对 structural drift 和 semantic drift 是敏感的。也就是说发生了“数据漂移”就是发生了异常状况。

国内面试一般提及“数据漂移”时，通常指的是 semantic drift ，特别是指 ETL 过程中的在预期的计算周期里不是刚好包含了所有预期的数据的情况。比如 ODS 里某个T+1表的同一个业务日期数据中包含前一天或者后一天凌晨附近的数据或者丢失当天的变更数据。这时就丢失了当天的变更数据，或者说“上一天”(D)的数据漂移到了“下一天”(D+1)。

传统的 ETL 过程大多是周期性调度的。典型的 ETL 调度将一系列相互之间有数据依赖关系的 ETL 任务，组织为一个 DAG ，整个 DAG 依据事先设定的调度周期或触发条件启动本轮的执行。
