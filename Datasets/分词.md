---
title: 分词
subtitle: subword
tags: [subword]
---

Subword 算法，提升 NLP 模型性能

2018 Bert 伴生

与传统空格分隔 tokenization 技术的对比

- OOV 问题，传统词表示方法无法很好的处理未知或罕见的词汇
- 传统词 tokenization 方法不利于模型学习词缀之间的关系
  - 模型学到的“old”, “older”, and “oldest”之间的关系无法泛化到“smart”, “smarter”, and “smartest”。
- Character embedding 作为 OOV 的解决方法粒度太细
- Subword 粒度在词与字符之间，能够较好的平衡 OOV 问题

Byte Pair Encoding

BPE(字节对)编码或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节

后期使用时需要一个替换表来重建原始数据

OpenAI [GPT-2 ](https://link.zhihu.com/?target=https%3A//towardsdatascience.com/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655)与Facebook [RoBERTa ](https://link.zhihu.com/?target=https%3A//github.com/pytorch/fairseq/tree/master/examples/roberta)均采用此方法构建subword vector

- 优点：可以有效地平衡词汇表大小和步数(编码句子所需的token数量)
- 缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果

## WordPiece

2012 年

BPE 的变种

WordPiece 基于概率生成新的 subword 而不是下一最高频字节对

## ULM

2018

能够输出带概率的多个子词分段

引入假设：所有 subword 的出现都是独立的，并且 subword 序列由 subword 出现概率的乘积产生。
WordPiece 和 ULM 都利用语言模型建立 subword 词表

subword 可以平衡词汇量和对未知词的覆盖。 极端的情况下，我们只能使用 26 个token（即字符）来表示所有英语单词。一般情况，建议使用 16k 或 32k 子词足以取得良好的效果，Facebook [RoBERTa](https://link.zhihu.com/?target=https%3A//github.com/pytorch/fairseq/tree/master/examples/roberta)甚至建立的多达50k的词表。

对于包括中文在内的许多亚洲语言，单词不能用空格分隔。 因此，初始词汇量需要比英语大很多。