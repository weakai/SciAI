---
title: 归纳偏置
subtitle: Inductive Bias
---

#### 什么是归纳偏置

**归纳偏置**在机器学习中是一种很微妙的概念：在机器学习中，很多学习算法经常会对学习的问题做一些假设，这些假设就称为归纳偏置(Inductive Bias)。归纳(Induction)是自然科学中常用的两大方法之一(归纳与演绎, induction and deduction)，指的是从一些例子中寻找共性、泛化，形成一个比较通用的规则的过程；偏置(Bias)是指我们对模型的偏好。因此，归纳偏置可以理解为，从现实生活中观察到的现象中归纳出一定的规则(heuristics)，然后对模型做一定的约束，从而可以起到“模型选择”的作用，即从假设空间中选择出更符合现实规则的模型。



- 归纳偏置，让算法优先某种解决方案，这种偏好是独立于观测的数据的。
- 常见的归纳偏置，包括：贝叶斯算法中的先验分布、使用某些正则项来惩罚模型、设计某种特殊的网络结构等。
- 归纳偏置，一般是对样本的产生过程，或者最终解的空间的一些假设。例如我们设计各种模型结构/形式，就是对解的空间上的假设。
- 好的归纳偏置，会提升算法搜索解的效率（同时不会怎么降低性能），而不好的归纳偏置则会让算法陷入次优解，因为它对算法带来了太强的限制。
- 在深度学习方面也是一样。以神经网络为例，各式各样的网络结构/组件/机制往往就来源于归纳偏置。

如果你想要知道某种数据增强的方法是否有效，我们可以把这种假设，设计成方法，加到模型的身上，那如果模型表现更好的话，就说明这个假设是符合实际情况的，从而是有效的假设。

好的论文，多半都是包含了作者丰富的经验，然后设计出有效的归纳偏置。机器本身是没法学习的，只能按照人定义的方式去学习，学习的目的，也是为了让机器能接近人的能力，因此从这种视角看，有丰富的人生经验的人，对事物有深刻思考和体验的人，能有可能提出对机器、模型有效的先验假设，从而让机器、模型学习得更好。说到这里，就想到最近看了一期之前的许知远的节目，在节目的最后，看到许知远说过的一句话，印象很深刻：

> “每个人都是带着成见来看待世界的，如果你没有带着成见，那你对世界根本没有看待方式。”

这句话挺有意思的，但其实也是一句有意思的废话。对世界，我们没办法保持客观，我们都是先入为主、根据自己的人生经验做出的判断。而那些所谓的客观的人，也只是他们经历了更多，有了更多的经验，所以看起来会更客观一些，但世界这么大，历史这么久，我们谁也无法了解全部，因此我们就没法做到真正的客观。训练模型也是一样，我们都是多多少少带着自己的“偏见”或者“偏好”去训练模型的，而那些 benchmark，GLUE 啊 CLUE 啊，你说它们真的客观吗？也不一定，我们见过太多例子，在 GLUE 榜单上表现很好的模型，受到轻微的攻击，就表现地像屎一样。这只能说明，某些偏好训练出的模型，正好也符合了数据集本身的偏好，所以在数据集上才能得到很好的成绩。

这给我最大的启示是什么？无论在工业界还是学术界，我们都需要去努力挖掘 target 场景中的各种规律、偏好甚至是偏见，然后尽可能把这些先验的知识给注入到我们机器学习模型的开发过程中来。一味的调参、排列组合式地修改模型结构，可能大部分都是无用功吧。**可惜，调参往往是最无脑的，而挖掘数据往往是最枯燥的。**

本文大致了解了 inductive bias（归纳偏置）的定义、背景和具体使用。理解这个词，对于我们理解机器学习中的各种方法是很有帮助的，如果在写作中正确使用这个词，也可以让咱们的论文显得更加专业吧！

#### 神经网络结构背后的归纳偏置?

![](https://ask.qcloudimg.com/http-save/yehe-3692577/00ac22bb33b5334456b2ee0d9258a90b.png?imageView2/2/w/1620)

- 可以看到，全连接网络的 inductive bias 是最轻微的，它就是假设所有的单元都可能会有联系；
- 卷积则是假设数据的特征具有局部性和平移不变性
- 循环神经网络则是假设数据具有序列相关性和时序不变性
- 而图神经网络则是假设节点的特征的聚合方式是一致的。
- 总之，网络的结构本身就包含了设计者的假设和偏好，这就是归纳偏置。