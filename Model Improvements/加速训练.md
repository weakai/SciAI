# 加速训练

#### 在 DataLoader 中使用多个 worker 和页锁定内存

当使用 `torch.utils.data.DataLoader` 时，设置 `num_workers > 0`，而不是默认值 0，同时设置 `pin_memory=True`，而不是默认值 False。

参考文档：<https://pytorch.org/docs/stable/data.html>

来自 NVIDIA 的高级 CUDA 深度学习算法软件工程师 Szymon Micacz 就曾使用四个 worker 和页锁定内存（pinned memory）在**单个 epoch 中实现了 2 倍的加速**。
人们选择 worker 数量的经验法则是将其设置为可用 GPU 数量的四倍，大于或小于这个数都会降低训练速度。请注意，增加 num_workers 将增加 CPU 内存消耗。

#### 把 batch size 调到最大

把 batch 调到最大是一个颇有争议的观点。一般来说，如果在 GPU 内存允许的范围内将 batch 调到最大，你的训练速度会更快。
但是，你也必须调整其他超参数，比如学习率。一个比较好用的经验是，batch 大小加倍时，学习率也要加倍。
OpenAI 的论文《An Empirical Model of Large-Batch Training》很好地论证了不同的 batch 大小需要多少步才能收敛。
在《How to get 4x speedup and better generalization using the right batch size》一文中，作者 Daniel Huynh 使用不同的 batch 大小进行了一些实验（也使用上面讨论的 1Cycle 策略）。
最终，他将 batch 大小由 64 增加到 512，实现了 4 倍的加速。

#### 将批大小选择为 2 的幂次

1. 内存对齐和浮点效率
2. 矩阵乘法和 Tensor Core
3. 多 GPU

具体原因参见 [一番实验后，有关 Batch Size 的玄学被打破了](https://mp.weixin.qq.com/s/eMxxpPwRYSeYy9suUSX_gQ)

#### 使用自动混合精度（AMP）

#### 考虑使用另一种优化器

AdamW 是由 fast.ai 推广的一种具有权重衰减（而不是 L2 正则化）的 Adam，在 PyTorch 中以 torch.optim.AdamW 实现。
AdamW 似乎在误差和训练时间上都一直优于 Adam。
Adam 和 AdamW 都能与上面提到的 1Cycle 策略很好地搭配。
目前，还有一些非本地优化器也引起了很大的关注，最突出的是 LARS 和 LAMB。NVIDA 的 APEX 实现了一些常见优化器的融合版本，比如 Adam。
与 PyTorch 中的 Adam 实现相比，这种实现避免了与 GPU 内存之间的多次传递，速度提高了 5%。

#### cudNN 基准

如果你的模型架构保持不变、输入大小保持不变，设置 torch.backends.cudnn.benchmark = True。

#### 小心 CPU 和 GPU 之间频繁的数据传输

当频繁地使用 tensor.cpu() 将张量从 GPU 转到 CPU（或使用 tensor.cuda() 将张量从 CPU 转到 GPU）时，代价是非常昂贵的。
item() 和 .numpy() 也是一样可以使用. detach() 代替。
如果你创建了一个新的张量，可以使用关键字参数 device=torch.device( cuda:0 ) 将其分配给 GPU。
如果你需要传输数据，可以使用. to(non_blocking=True)，只要在传输之后没有同步点。

#### 使用. as_tensor() 而不是. tensor()

torch.tensor() 总是会复制数据。如果你要转换一个 numpy 数组，使用 torch.as_tensor() 或 torch.from_numpy() 来避免复制数据。

#### 设置梯度为 None 而不是 0

梯度设置为. zero_grad(set_to_none=True) 而不是 .zero_grad()。这样做可以让内存分配器处理梯度，而不是将它们设置为 0。
正如文档中所说，将梯度设置为 None 会产生适度的加速，但不要期待奇迹出现。注意，这样做也有缺点，详细信息请查看文档。

#### 使用梯度裁剪

关于避免 RNN 中的梯度爆炸的问题，已经有一些实验和理论证实，梯度裁剪（gradient = min(gradient, threshold)）可以加速收敛。
在 PyTorch 中可以使用 torch.nn.utils.clip_grad_norm_来实现。
当然也可使用 `lightning.trainer` 自动实现

#### 使用输入和 batch 归一化

## 参考

- [【他山之石】PyTorch | 优化神经网络训练的17种方法](https://mp.weixin.qq.com/s/3VCQFt0wn_7R9ehamjiw_w)
