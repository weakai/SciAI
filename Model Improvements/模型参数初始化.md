---
title: Initialization
subtitle: 模型参数的初始化
---



#### [从几何视角来理解模型参数的初始化策略](https://kexue.fm/archives/7180)

对于复杂模型来说，参数的初始化显得尤为重要。糟糕的初始化，很多时候已经不单是模型效果变差的问题了，还更有可能是模型根本训练不动或者不收敛。在深度学习中常见的自适应初始化策略是Xavier初始化，它是从正态分布中随机采样而构成的初始权重。

标准的初始化策略的推导是基于概率统计的，大概的思路是假设输入数据的均值为0、方差为1，然后期望输出数据也保持均值为0、方差为1，然后推导出初始变换应该满足的均值和方差条件。这个过程理论上没啥问题，但在笔者看来依然不够直观，而且推导过程的假设有点多。本文则希望能从几何视角来理解模型的初始化方法，给出一个更直观的推导过程。

推论1： 高维空间中的任意两个随机向量几乎都是垂直的。
推论2： 从N(0,1/n)中随机选取n2个数，组成一个n×n的矩阵，这个矩阵近似为正交矩阵，且n越大，近似程度越好。

当然，两两正交还不是正交矩阵，因为正交矩阵还要求每个向量的模长为1，而我们有Ex∼N(0,1)[x2]=1，所以这意味着从N(0,1)中采样出的n维向量模长近似为n−−√，所以为了接近正交，还需要将每个元素除以n−−√，这等价于采样方差由1变成了1/n。

此外，采样分布还不一定要是正态分布，比如均匀分布 $U[−\sqrt{\frac{3}{n}},3/\sqrt{\frac{3}{n}}]$ 也行。事实上我们有

推论3： 从任意的均值为0、方差为1/n的分布p(x)中独立重复采样出来的n×n矩阵，都接近正交矩阵。

所以任意两个向量都是接近正交归一的，因此采样出来的矩阵也接近正交矩阵。

如果读者对线性代数还有印象的话，那么应该还记得正交矩阵的重要意义在于它在变换过程中保持了向量的模长不变。

深度学习模型本身上就是一个个全连接层的嵌套，所以为了使模型最后的输出不至于在初始化阶段就过于“膨胀”或者“退化”，一个想法就是让模型在初始化时能保持模长不变。

这个想法形成的一个自然的初始化策略就是“以全零初始化b，以随机正交矩阵初始化W”。而推论2就已经告诉我们，从N(0,1/n)采样而来的n×n矩阵就已经接近正交矩阵了，所以我们可以从N(0,1/n)采样来初始化W。这便是Xavier初始化策略了，有些框架也叫Glorot初始化，因为作者叫Xavier Glorot～此外，采样分布也不一定是N(0,1/n)，前面推论3说了你可以从任意均值为0、方差为1/n的分布中采样。

上面说的是输入和输出维度都是n的情况，如果输入是n维，输出是m维呢？这时候W∈Rm×n，保持Wx模长不变的条件依然是W⊤W=I。然而，当m<n时，这是不可能的；当m≥n时，这是有可能成立的，并且根据前面相似的推导，我们可以得到

推论4： 当m≥n时，从任意的均值为0、方差为1/m的分布p(x)中独立重复采样出来的m×n矩阵，近似满足W⊤W=I。

所以，如果m>n，那么只需要把采样分布的方差改为1/m就好，至于m<n时，虽然没有直接的推导，但仍然可以沿用这个做法，毕竟合理的策略应该是普适的。注意，这个改动跟Xavier初始化的原始设计有点不一样，它实际上叫“LeCun初始化”，而Xavier初始化的做法是方差改为2/(m+n)，这平均了前向传播和反向传播的直觉做法，而我们这里主要考虑的是前向传播。

可能还会有读者疑问：你这里只是考虑了没有激活函数的场景，就算y的模长跟x一样，但y经过激活函数后就不一样了。确实是存在这样的情况，而且这时候只能针对具体问题具体分析。比如tanh(x)在x比较小的时候有tanh(x)≈x，所以可以认为Xavier初始化直接适用于tanh激活；再比如relu时可以认为relu(y)会有大约一半的元素被置零，所以模长大约变为原来的1/2√，而要保持模长不变，可以让W乘上2√，也就是说初始化方差从1/m变成2/m，这就是何凯明大神提出来的针对relu的初始化策略。

当然，事实上很难针对每一个激活函数都做好方差的调整，所以一个更通用的做法就是直接在激活函数后面加上一个类似Layer Normalization的操作，直接显式地恢复模长。这时候就轮到各种Normalization技巧登场了～

## Reference

[Initializing neural networks](https://www.deeplearning.ai/ai-notes/initialization/)

[How to initialize deep neural networks? Xavier and Kaiming initialization](https://pouannes.github.io/blog/initialization/)
