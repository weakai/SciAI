# 损失函数的设计

## 常见分类损失函数

平方损失最常用，其缺点是对于异常点会施以较大的惩罚，因而不够 robust。
绝对损失具有抵抗异常点干扰的特性，但是在 y-f(x) 处不连续可导，难以优化。
Huber 损失是对二者的综合。

### 平均偏差误差（mean bias error）

与其它损失函数相比，这个函数在机器学习领域没有那么常见。
它与 MAE 相似，唯一的区别是这个函数没有用绝对值。
用这个函数需要注意的一点是，正负误差可以互相抵消。
尽管在实际应用中没那么准确，但它可以确定模型存在正偏差还是负偏差。

### L1 Loss 与 L2 Loss 的区别

当训练数据被异常点所污染，那么 MAE 损失就更好用（比如，在训练数据中存在大量错误的反例和正例标记，但是在测试集中没有这个问题）。

然而 MAE 存在一个严重的问题（特别是对于神经网络）：更新的梯度始终相同，也就是说，即使对于很小的损失值，梯度也很大。
这样不利于模型的学习。为了解决这个缺陷，我们可以使用变化的学习率，在损失接近最小值时降低学习率。

总而言之，处理异常点时，L1损失函数更稳定，但它的导数不连续，因此求解效率较低。
L2 损失函数对异常点更敏感，但通过令其导数为 0，可以得到更稳定的封闭解。

二者兼有的问题是：在某些情况下，上述两种损失函数都不能满足需求。
例如，若数据中 90% 的样本对应的目标值为 150，剩下 10% 在 0 到 30 之间。
那么使用 MAE 作为损失函数的模型可能会忽视 10% 的异常点，而对所有样本的预测值都为 150。

这是因为模型会按中位数来预测。
而使用 MSE 的模型则会给出很多介于 0 到 30 的预测值，因为模型会向异常点偏移。
上述两种结果在许多商业场景中都是不可取的。

这些情况下应该怎么办呢？
最简单的办法是对目标变量进行变换。
而另一种办法则是换一个损失函数，这就引出了下面要讲的第三种损失函数，即 Huber 损失函数。

### Huber 损失，平滑的平均绝对误差

Huber 损失对数据中的异常点没有平方误差损失那么敏感。
它在 0 也可微分。
本质上，Huber 损失是绝对误差，只是在误差很小时，就变为平方误差。
误差降到多小时变为二次误差由超参数 δ（delta）来控制。
当 Huber 损失在`[0-δ,0+δ]`之间时，等价为 MSE，而在`[-∞,δ]`和`[δ,+∞]`时为 MAE。

这里 超参数 $\delta$ 的选择非常重要，因为这决定了你对与异常点的定义。
当残差大于 delta，应当采用 L1（对较大的异常值不那么敏感）来最小化，而残差小于超参数，则用 L2 来最小化。

因此，Huber 损失结合了 MSE 和 MAE 的优点。但是，Huber 损失的问题是我们可能需要不断调整超参数 delta。

### Log-Cosh 损失

Log-cosh 是另一种应用于回归问题中的，且比 L2 更平滑的的损失函数。
它的计算方式是预测误差的双曲余弦的对数。

![](https://t12.baidu.com/it/u=3139409714,1388214145&fm=173&app=25&f=JPEG?w=436&h=90&s=0D2CED12CDE4DA110E74CCC6000090B1)

优点：对于较小的 x，log(cosh(x)) 近似等于 $(x^2)/2$，对于较大的 $x$，近似等于 $abs(x)-log(2)$。
这意味着 `logcosh` 基本类似于均方误差，但不易受到异常点的影响。
它具有 Huber 损失所有的优点，但不同于 Huber 损失的是，Log-cosh 二阶处处可微。

> 为什么需要二阶导数？
> 许多机器学习模型如 XGBoost，就是采用牛顿法来寻找最优点。
> 而牛顿法就需要求解二阶导数（Hessian）。
> 因此对于诸如 XGBoost 这类机器学习框架，损失函数的二阶可微是很有必要的。

但 Log-cosh 损失也并非完美，其仍存在某些问题。
比如误差很大的话，一阶梯度和 Hessian 会变成定值，这就导致 XGBoost 出现缺少分裂点的情况。

### 分位数损失

在大多数现实世界预测问题中，我们通常希望了解预测中的不确定性。清楚预测的范围而非仅是估计点，对许多商业问题的决策很有帮助。

当我们更关注区间预测而不仅是点预测时，分位数损失函数就很有用。使用最小二乘回归进行区间预测，基于的假设是残差（y-y_hat）是独立变量，且方差保持不变。

一旦违背了这条假设，那么线性回归模型就不成立。但是我们也不能因此就认为使用非线性函数或基于树的模型更好，而放弃将线性回归模型作为基线方法。这时，分位数损失和分位数回归就派上用场了，因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。

下面让我们看一个实际的例子，以便更好地理解基于分位数损失的回归是如何对异方差数据起作用的。

> 理解分位数损失函数
> 如何选取合适的分位值取决于我们对正误差和反误差的重视程度。
> 损失函数通过分位值（γ）对高估和低估给予不同的惩罚。
> 例如，当分位数损失函数γ=0.25时，对高估的惩罚更大，使得预测值略低于中值。

![](https://t11.baidu.com/it/u=2218073872,3469543857&fm=173&app=25&f=JPEG?w=640&h=416&s=DDA43972190A444D0855C1CB000050B0)

## 常见分类损失函数

### 交叉熵损失/负对数似然：

logistics loss 通常用于逻辑回归分类模型。
多分类问题时通常可以采用 softmax loss。
多标签预测可以使用 sigmoid cross entropy loss。

注意在 softmax loss 中，由于需要经过归一化操作，因此每个类别的预测不是相互独立的。
为了加深理解我们可以看下 softmax loss 的梯度更新公式。

#### 交叉熵函数与最大似然函数的联系和区别？

区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；
似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。

联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说最小化交叉熵函数的本质就是对数似然函数的最大化。

### Hinge Loss/多分类 SVM 损失

简言之，在一定的安全间隔内（通常是 1），正确类别的分数应高于所有错误类别的分数之和。
因此 hinge loss 常用于最大间隔分类（maximum-margin classification），最常用的是支持向量机。
尽管不可微，但它是一个凸函数，因此可以轻而易举地使用机器学习领域中常用的凸优化器。

### Dice Loss

即骰子损失，出自 V-Net，是一种用于评估两个样本之间相似性度量的函数，取值范围为0~1，值越大表示两个值的相似度越高，其基本定义（二分类）如下：

![](https://pic3.zhimg.com/v2-4e9e4226d0777e200c6b63610c9db7be_b.jpg)

其中，|X∩Y|表示X和Y之间的交集，|X|和|Y|分别表示集合X和Y中像素点的个数，分子乘于2保证域值范围在0~1之间，因为分母相加时会计算多一次重叠区间，如下图：

![](https://pic1.zhimg.com/v2-b629203738fe1c9ce5235663dc6c71d4_b.jpg)

从右边公式也可以看出，其实 Dice 系数是等价于 F1 分数的，优化 Dice 等价于优化 F1 值。
此外，为了防止分母项为 0，一般我们会在分子和分母处同时加入一个很小的数作为平滑系数，也称为拉普拉斯平滑项。


有益于正负样本不均衡的情况，侧重于对前景的挖掘；
训练过程中，在有较多小目标的情况下容易出现振荡；
极端情况下会出现梯度饱和的情况。

所以一般来说，我们都会结合交叉熵损失或者其他分类损失一同进行优化。

### Focal Loss

难易不平衡：
假设：易分样本（置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本。

焦点损失，出自何凯明的《Focal Loss for Dense Object Detection》，出发点是解决目标检测领域中 one-stage 算法如 YOLO 系列算法准确率不高的问题。
作者认为样本的类别不均衡（比如前景和背景）是导致这个问题的主要原因。
比如在很多输入图片中，我们利用网格去划分小窗口，大多数的窗口是不包含目标的。
如此一来，如果我们直接运用原始的交叉熵损失，那么负样本所占比例会非常大，主导梯度的优化方向，即网络会偏向于将前景预测为背景。
即使我们可以使用 **OHEM（在线困难样本挖掘）算法** 来处理不均衡的问题，虽然其增加了误分类样本的权重，但也容易忽略掉易分类样本。
而 Focal loss 则是聚焦于训练一个困难样本的稀疏集，通过直接在标准的交叉熵损失基础上做改进，引进了两个惩罚因子，来减少易分类样本的权重，
使得模型在训练过程中更专注于困难样本。其基本定义如下：

![](https://pic1.zhimg.com/v2-78ec2c165b42da5dd8fd351189473c4c_b.png)

其中：

参数 α 和 (1-α) 分别用于控制正/负样本的比例，其取值范围为 `[0, 1]`。α 的取值一般可通过交叉验证来选择合适的值。
参数 γ 称为聚焦参数，其取值范围为[0, +∞)，目的是通过减少易分类样本的权重，从而使模型在训练时更专注于困难样本。
当 γ = 0 时，Focal Loss 就退化为交叉熵损失，γ 越大，对易分类样本的惩罚力度就越大。

实验中，作者取（α=0.25，γ=2）的效果最好，具体还需要根据任务的情况调整。
由此可见，应用 Focal-loss 也会引入多了两个超参数需要调整，而一般来说很需要经验才能调好。

### Tversky loss

Tversky loss，发表于 CVPR 2018上的一篇《Tversky loss function for image segmentation using 3D fully convolutional deep networks》文章，
是根据 Tversky 等人于 1997 年发表的《Features of Similarity》文章所提出的 Tversky 指数所改造的。
Tversky 系数主要用于描述两个特征（集合）之间的相似度，其定义如下：

![](https://pic1.zhimg.com/v2-c4476d95ccc5cf8af42f1981a8e6bbb0_b.png)

由上可知，它是结合了 Dice 系数（F1-score）以及 Jaccard 系数（IoU）的一种广义形式，如：

当 α = β = 0.5时，此时 Tversky loss 便退化为 Dice 系数（分子分母同乘于2）
当 α = β = 1 时，此时 Tversky loss 便退化为 Jaccard 系数（交并比）
因此，我们只需控制 α 和 β 便可以控制假阴性和假阳性之间的平衡。
比如在医学领域我们要检测肿瘤时，更多时候我们是希望 Recall 值（查全率，也称为灵敏度或召回率）更高，
因为我们不希望说将肿瘤检测为非肿瘤，即假阴性。
因此，我们可以通过增大 β 的取值，来提高网络对肿瘤检测的灵敏度。
其中，α + β 的取值我们一般会令其 1。

## 多任务常见损失函数

### 交叉熵损失函数

ps. pytorch 1.10 开始，CrossEntropyLoss 的构造参数中，增加了 label_smoothing，默认值为 0，这样就非常方便的支持了 label smoothing 标签平滑功能。
在开启了这个功能后，CrossEntropyLoss 的计算流程和 BCELoss 有点类似了。

### NLLLoss 

和 CrossEntropyLoss 类似，只是去掉了 logsoftmax 的计算，直接从输入 Tensor 中挑选第 y 个作为 loss

### BCELoss

计算的流程中没有了 softmax，可以用在多标签分类（一张图片命中多个类别），或二分类中，或使用了 labelsmooth 标签平滑时。


### KL 散度



## Tricks

### 标签平滑

什么叫标签平滑
正常模型的正样本标签为 1，负样本标签为 0，这是一种 hard 的学习，这样的模型可以叫做过于自信的模型。
标签平滑就是将正样本的标签为 0.9，负样本的标签为 0.1，这是一种 soft 的学习，也就是让告诉模型，不要这么自信。
当然标签平滑的程度可以根据情况修改。

标签平滑的例子
例如如，5 分类问题 C 为 5，smooth 设置为 0.1，则某一个样本的 one hot 的 label,
从 `[0, 1, 0, 0, 0]` 变为 `[0.025, 0.9, 0.025, 0.025, 0.025]`，再参与 loss 计算

标签平滑的禁忌
在知识蒸馏中的教师网络中，采用标签平滑的话会影响效果，导致教师网络无法有效传递知识。

- [Label Smoothing 标签平滑详解+ Pytorch 保姆级实际操作](https://blog.csdn.net/weixin_41811314/article/details/115863126)

### GHM(gradient harmonizing mechanism)

首先 Focal Loss 存在很多问题 - 关注难分样本：如果样本中存在离群点，Focal Loss 过分关注离群点，那么模型就跑偏了。 - labmda 是超参数，全靠经验。

GHM 不是根据样本的难易程度来进行衰减，而是根据（一定梯度内）的样本数量进行衰减，也就是谁的样本数量多，就衰减谁。

### 负采样 negative sample

解决神经网络输出层计算量太大问题，修改损失函数同时采样少部分负样本

### sampled softmax

将样本分为几份，每份样本内部进行 softmax 损失，这样可以一定程度减少复杂度，但是缺点是 serving 阶段还是要 *全量的softmax*

## Links

- [策略算法工程师之路-损失函数设计 | 强烈推荐](https://zhuanlan.zhihu.com/p/94596648)
- [机器学习大牛最常用的5个回归损失函数，你知道几个？ | 强烈推荐](https://baijiahao.baidu.com/s?id=1603857666277651546&wfr=spider&for=pc)
- [损失函数总结-应用和 trick](https://zhuanlan.zhihu.com/p/104273926)