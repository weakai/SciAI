# R-drop

原论文提供了对 R-Drop 的一个理论分析，大致意思是 R-Drop 会促进参数的同化，从而起到正则化作用。它将“Dropout两次”的思想用到了有监督任务中，每个实验结果几乎都取得了明显的提升。此外，笔者在自己的实验还发现，它在半监督任务上也能有不俗的表现。最后，分享了笔者对 R-Drop 的三个角度的思考～

正向计算是双倍的

#### 一致性

R-Dropout 可以看成是 Dropout 的改进，那 Dropout 有什么问题呢？其实 Dropout 是典型的训练和预测不一致的方法。
具体来说，Dropout 在训练阶段往（某些层的）输入加上了乘性噪声。
带 Dropout 的模型的正确步骤是“模型融合”。
它通过增加一个正则项，来强化模型对 Dropout 的鲁棒性，使得不同的 Dropout 下模型的输出基本一致，
因此能降低这种不一致性，促进“模型平均”与“权重平均”的相似性，从而使得简单关闭 Dropout 的效果等价于多 Dropout 模型融合的结果，提升模型最终性能。

#### 连续性

VAT也是通过一个正则项，使得模型对扰动更加鲁棒，增强模型本身的连续性（小的变化不至于对结果产生大的影响）。它们不同的地方在于加扰动的方式，VAT只把扰动加入到输入中，并且通过对抗的思想提升扰动的针对性；R-Drop的扰动则可以施加到模型的每一层中，并且扰动是随机的。+

VAT 可是主打半监督训练的，那是不是意味着 R-Drop 也可以做半监督训练？这部分原论文并没有实验，是笔者自己做的实验，答案是确实可以，跟 VAT 类似，R-Drop新增的KL散度项是不需要标签的，因此可以无监督训练，混合起来就是半监督了，效果也还不错。R-Drop的半监督效果完全不逊色于VAT，而且它实现比VAT简单，速度也比VAT快！看来VAT有望退休了～直觉上来看，虽然R-Drop的扰动是随机的，但是R-Drop的扰动更多，所以它造成的扰动也会放大，也可能比得上VAT经过对抗优化的扰动，所以R-Drop能够不逊色于VAT。

## 参考

- [又是 Dropout 两次！这次它做到了有监督任务的 SOTA](https://kexue.fm/archives/8496/comment-page-2)
- [Github | R-Drop](https://github.com/dropreg/R-Drop)