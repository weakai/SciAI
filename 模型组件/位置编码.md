---
title: 位置编码
---

不同于 RNN、CNN 等模型，对于 Transformer 模型来说，位置编码的加入是必不可少的，因为纯粹的 Attention 模块是无法捕捉输入顺序的，即无法区分不同位置的 Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了绝对位置编码的一般做法；2、想办法微调一下 Attention 结构，使得它有能力分辨不同位置的 Token，这构成了相对位置编码的一般做法。

## links

- [让研究人员绞尽脑汁的 Transformer 位置编码](https://kexue.fm/archives/8130)