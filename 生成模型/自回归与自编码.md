#### 自回归模型 AR

一个句子的生成过程如下：首先根据概率分布生成第一个词，然后根据第一个词生成第二个词，然后根据前两个词生成第三个词，……，直到生成整个句子。 AR 模型是一种线性预测，利用前期若干时刻的随机变量的线性组合来描述以后某时刻随机变量的线性回归模型。即已知 N 个数据，可由模型推出第N点前面或后面的数据（设推出 P 点），所以其本质类似于插值，其目的都是为了增加有效数据，只是 AR 模型是由 N 点递推，而插值是由两点（或少数几点）去推导多点，所以 AR 模型要比插值方法效果更好。

在 ELMO／BERT 出来之前，大家通常讲的语言模型其实是根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的 LM 被称为自回归语言模型。

GPT 就是典型的自回归语言模型。

缺点是只能利用上文或者下文的信息，不能同时利用上文和下文的信息，当然，貌似 ELMO 这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。

就是说自回归语言模型有个缺点，要么从左到右，要么从右到左，尽管可以类似 ELMO 两个都做，然后再拼接的方式。但是跟 Bert 比，效果明显不足够好（这里面有 RNN 弱于 Transformer 的因素，也有双向语言模型怎么做的因素）。

NLP 中的 seq2seq 和 Transformer 都是 AR 模型。

#### 自编码语言模型

一种**无监督学习输入的特征**的方法：我们用一个神经网络把输入(输入通常还会增加一些噪声)变成一个低维的特征，这就是编码部分，然后再用一个 Decoder 尝试把特征恢复成原始的信号。我们可以把 BERT 看成一种 AutoEncoder，它通过 Mask 改变了部分 Token，然后试图通过其上下文的其它 Token 来恢复这些被 Mask 的 Token。

自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。相比而言，Bert 通过在输入 X 中随机 Mask 掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被 Mask 掉的单词，如果你对 Denoising Autoencoder 比较熟悉的话，会看出，这确实是典型的 DAE 的思路。那些被 Mask 掉的单词就是在输入侧加入的所谓噪音。类似 Bert 这种预训练模式，被称为 DAE LM。

这种 DAE LM 的优缺点正好和自回归 LM 反过来，它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。

缺点是啥呢？主要在输入侧引入`[Mask]`标记，导致预训练阶段和 Fine-tuning 阶段不一致的问题，因为 Fine-tuning 阶段是看不到`[Mask]`标记的。DAE 吗，就要引入噪音，`[Mask]` 标记就是引入噪音的手段，这个正常。

#### 非自回归模型 NAR

非自回归模型的提出缓解了自回归模型的高时延问题。

在非自回归模型中，每个单词之间没有依赖关系，整个输出序列的每个单词被并行地同步预测。虽然其推断速度得到了很大改善，但是生成质量却往往弱于自回归模型。

为了平衡推断速度和生成质量，半非自回归的模型被提出和研究。半非自回归的经典做法是把非自回归生成的结果进行多次迭代，但不同半非自回归模型的算法差异比较大。由于和自回归相比，非自回归和半非自回归的依赖关系学习和生成难度较大，所以它们往往在文本-文本翻译，或者语音-文本翻译，文本-语音翻译等输入输出较为对齐的任务上可以提供不错的生成效果，但是很少在问答、对话、摘要等任务上进行研究，而这些领域被自回归生成验证可以拥有不错的生成质量且在预训练下得到提升。

针对上述问题，微软亚洲研究院的研究员们提出了新的自然语言生成预训练 BANG，并指出自回归和非自回归生成可以被统一地理解为，有多大比例的上文信息可以被使用。BANG 的贡献主要有：

1. BANG 在大规模预训练中，通过考虑遮盖任意长度的前文来沟通自回归和非自回归生成；

2. 提出跨流可见的多流注意力机制来实现高效的预训练，所有单词在考虑到任意长度前文被遮盖的前提下都可被并行预测；

3. 对于不同的需求状况，BANG 支持自回归微调，非自回归微调和半非自回归微调。BANG 第一次把不同的生成方案在同一个预训练模型里进行支持；

4. 研究员们在 16GB 的英语语料上进行了预训练，在摘要、对话、问题生成上，BANG 对自回归效果和半非自回归效果带来了显著的提升，并达到了与非预训练的  Transformer 自回归模型相似的评测结果。对于自回归生成的微调，BANG 也可以和当前主流的自回归预训练模型达到相似的结果。