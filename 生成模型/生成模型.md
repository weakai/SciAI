# 生成模型

## 0. 生成模型介绍

### 0.1. 为什么要研究生成模型？

那么研究生成模型的意义何在呢？
尤其是对于非直接密度估计而只能从模型中生成样本的一类情况，特别是对于图像，这类模型只能提供更多的图像，而我们并不缺少海量的图像。

主要原因如下：

1. 这是对我们能够**表示和操控高维概率分布**的能力的有效检验
2. 我们可以将生成模型结合到**强化学习**(reinforcement learning)中，例如对于 model-based RL 可用生成模型来模拟可能发生的未来情况，以便 RL 算法进行规划(planning)。
   - [Deep Visual Foresight for Planning Robot Motion](https://arxiv.org/abs/1610.00696)
3. 生成模型可以用有损失（部分样本无标记）的数据进行训练，进行**半监督学习**(semi-supervised learning)，降低了我们获得数据样本的难度。
4. 生成模型可以处理**多峰值**(multi-modal)的输出。对于很多任务，一个输入可能对应多个可能的输出，一些传统的机器学习模型只能学到一种输出而无法学习多种可能的输出。
5. 还有一些任务需要产生看起来真实的样本。如由低分辨率图片产生高分辨率图片，图像转换等等。

### 0.2. 判定模型和生成模型的区别？

- [机器学习“判定模型”和“生成模型”有什么区别？](#Links)

目前比较常见的生成模型有如下三种：

1. VAE
2. GAN
3. Flow-based Generative Model(FBGM)

比较三种方法，从影响力的角度来说，GAN > VAE > FBGM.
GAN 如今已是遍地开花，成为各大顶会的宠儿，字母表都不够用了。
在应用上广泛用于各种图片生成 ， 图片风格迁移等。
之前用 supervised learning 能够解决的问题，采用 Conditional GAN 的思路，往往效果还会更好。
VAE也经常能在各种论坛、博客中看到， FBGM 就显得相对暗淡很多，可能是因为FBGM用到了太多的数学知识，而显得晦涩难懂。
但如果从方法本身来说，显然 FBGM 研究的角度，更加接近于问题的本质，而且提出了可行的解决办法，至于说为何得不到研究者的青睐，就仁者见仁，智者见智了。

另外，从生成效果来看，依然是 GAN > VAE(FBGM 应用较少，不做比较）.
GAN 的思路更加泛化，Generator 和 Discriminator 只是一种范指。
在实际应用中， Generator 可以采用 CNN, RNN 或者是 Seq2Seq model.
Discriminator 可以是简单的二分类，也可以是复杂的 data pair 的判断。
另外， Cycle GAN 的思路，为 Unsurpervised learning 问题的求解提供了新的思路。
VAE 能够做到 Typical GAN 的事情，但效果上会稍有欠缺。

这三种方法都是 Generative Model 领域值得细细研究的算法。
总体来说，GAN 应用更为广泛，VAE 也有用武之地，FBGM 更接近于问题本质。

### 0.3. 从 GAN VAE 到 CVAE-GAN

[深度神经网络生成模型：从 GAN VAE 到 CVAE-GAN](https://zhuanlan.zhihu.com/p/27966420)

GAN VAE 加上了类别作为输入，成为了 CVAE 和 CGAN。

- CVAE 生成的图像很中规中矩，但是模糊。
- CGAN 生成的图像清晰，但是喜欢乱来。

## 1. 变分自编码器

[变分自编码器 (VAE) Overview](https://zhuanlan.zhihu.com/p/420419446)

- 从 MLE 到变分推断
- VAE 与 AE 的关系
- Beta-VAE: Beta-VAE 是 VAE 的简单推广，但十分有效。
- Posterior Collapse in VAE
  - 涉及两个网络的模型训练起来都不是很简单, GAN 如此，VAE 亦如此。
  - VAE 容易出现一种被称为 posterior collapse 的问题。

[变分自编码器 VAE：原来是这么一回事 | 附开源代码 | by 苏剑林 | 强烈推介](https://zhuanlan.zhihu.com/p/34998569)

## 2. 生成对抗网络

### 2.1. GAN 的优缺点

1. 能更好建模数据分布（图像更锐利、清晰）
2. 理论上，GANs 能训练任何一种生成器网络。其他的框架需要生成器网络有一些特定的函数形式，比如输出层是高斯的。
3. 无需利用马尔科夫链反复采样，无需在学习过程中进行推断，没有复杂的变分下界，避开近似计算棘手的概率的难题。

4. 难训练，不稳定。生成器和判别器之间需要很好的同步，但是在实际训练中很容易 D 收敛，G 发散。D/G 的训练需要精心的设计。
5. 模式缺失（Mode Collapse）问题。GANs 的学习过程可能出现模式缺失，生成器开始退化，总是生成同样的样本点，无法继续学习。

[互怼的艺术：从零直达 WGAN-GP](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247484880&idx=1&sn=4b2e976cc715c9fe2d022ff6923879a8&chksm=96e9da50a19e5346307b54f5ce172e355ccaba890aa157ce50fda68eeaccba6ea05425f6ad76&scene=21#wechat_redirect)

### 2.2. GAN 的应用

[生成模型 Generative Model 在业界有哪些应用？](https://www.zhihu.com/question/465918667/answer/1950181642)

### 1. CGAN

![CGAN and GAN](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtybbs/029/274/683/0080086000029274683.20211209160613.20262054214017663225875796503058:50530427113342:2800:0C70ED5AC6BACF078E637F124A590ADC6F7848B4CB3601EB8FA5911D73B5055E.png)

### 2. DCGAN

卷积神经网络和对抗神经网络结合起来的一篇经典论文，核心要素是：在不改变 GAN 原理的情况下提出一些有助于增强稳定性的 tricks。
注意，这一点很重要。因为 GAN 训练时并没有想象的稳定，生成器最后经常产生无意义的输出或奔溃，但是 DCGAN 按照 tricks 能生成较好的图像。

DCGAN 论文使用的 tricks 包括：

1. 所有 pooling 都用 strided convolutions 代替，pooling 的下采样是损失信息的，strided convolutions 可以让模型自己学习损失的信息 
2. 生成器 G 和判别器 D 都要用 BN 层（解决过拟合） 
3. 把全连接层去掉，用全卷积层代替
4. 生成器除了输出层，激活函数统一使用 ReLU，输出层用 Tanh
5. 判别器所有层的激活函数统一都是 LeakyReLU

### 3. ACGAN

既能生成图像又能进行分类

Conditional Image Synthesis with Auxiliary Classifier GANs，该判别器不仅要判断是真（real）或假（fake），还要判断其属于哪一类。

![](https://cdn.jsdelivr.net/gh/xxzhai123/img/imgacgan.png)

### 4. infoGAN

Interpretable Representation Learning by Information Maximizing Generative Adversarial Networks

这个号称是 OpenAI 在 2016 年的五大突破之一

## 基于流的生成模型

- [VAE、GAN 和流模型的区别和联系：对生成模型家族的分析](#Links)

### NICE

NICE 的[论文](https://arxiv.org/abs/1410.8516)最早发布于2014年，作者是来自蒙特利尔大学的 Laurent Dinh、David Krueger、Yoshua Bengio。

- [细水长 flow 之 NICE：流模型的基本概念与实现 | By 苏剑林](#Links)

### GLOW

《NICE: Non-linear Independent Components Estimation》是 glow 的奠基石

Glow 是一种可逆生成模型，也称为基于流的生成模型，是 NICE 和 [RealNVP](https://arxiv.org/pdf/1605.08803.pdf) 技术的扩展。相比 GAN 和 VAE，基于流的生成模型迄今为止在研究界受到的关注寥寥无几。
基于流的生成模型具有以下优点：

1. 准确的潜在变量推理和对数似然估计。在 VAE 中，只能推理出对应于数据点的潜在变量的近似值。 GAN 根本没有编码器来推理潜在变量。而在可逆生成变量中，可以在没有近似的情况下实现精准推理。不仅实现了精准推理，还得以优化数据的准确对数似然度（而不是下界）。
2. 高效的推理与合成。自回归模型，如 PixelCNN，也是可逆的，然而从这样的模型合成难以实现并行化，并且通常在并行硬件上效率低下。基于流的生成模型如 Glow 和 RealNVP 都能有效实现推理与合成的并行化。
3. 下游任务有用的潜在空间。自回归模型的隐藏层有未知的边际分布，使其更难执行有效的数据操作。在 GAN 中，数据点通常不是在潜在空间中直接被表征的，因为它们没有编码器，并且可能无法表征完整的数据分布。而在可逆生成模型和 VAE 中不会如此，它们允许多种应用，例如数据点之间的插值，和已有数据点的有目的修改。
4. 内存存储的巨大潜力。在可逆神经网络中计算梯度需要恒定而不是和深度呈线性关系的内存，如 RevNet 论文中所述。

- [下一个GAN？OpenAI提出可逆生成模型Glow | 机器之心](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650745032&idx=1&sn=a889433dd4c4d9f62bfab347909d9d28&chksm=871aecb6b06d65a02625abdf4b21a2116251e311a49508db587b76ae8f76d7a9e03d4a6ab80a&scene=21#wechat_redirect)
- [GLOW | OpenAI | GitHub](https://github.com/openai/glow)
- [[细读经典+代码解析]Waveglow: 基于流模型的vocoder](https://zhuanlan.zhihu.com/p/355219393)

## Links

- [Generative Model Summary](https://zhuanlan.zhihu.com/p/124256839)
- [VAE、GAN 和流模型的区别和联系：对生成模型家族的分析](https://zhuanlan.zhihu.com/p/116775904)
- [细水长 flow 之 NICE：流模型的基本概念与实现 | By 苏剑林](https://spaces.ac.cn/archives/5776)
- [生成模型综述——深度学习第二十章（一）](https://zhuanlan.zhihu.com/p/50278440)
- [机器学习“判定模型”和“生成模型”有什么区别？](https://www.zhihu.com/question/20446337/answer/256466823)

