---
title: 双流自注意力
date:
tags: []
---

在我之前的文章什么是 XLNet，它为什么比 BERT 效果好？
中，我主要讨论了 XLNet (AR语言模型)和 BERT (AE语言模型)之间的区别以及重排列语言建模。
我相信对 XLNet 有一个直观的理解远比实现细节重要，所以我只解释了重排列语言建模，而没有提到另一个重要的部分，双流自注意力架构。
但正如陈家明在评论中提到的，双流自注意力是 XLNet 论文的另一个亮点，所以我写这篇文章是为了尽可能清楚地解释双流自注意力。

内容结构如下。

- 重排列语言建模的快速回顾
- 重排列带来了什么问题？
- BERT 有这样的问题吗?
- XLNet 如何解决这个问题?
- 注意力 mask：XLNet 如何实现重排列？

## 重排列建模的快速回顾

XLNet 提出使用重排列语言建模，使 AR 语言模型学习双向上下文。
这样可以避免 AE 语言模型中 MASK 方法带来的弊端。

## 重排列带来了什么问题？

重排列语言建模的目标函数，即以 t-1 个 tokens 为上下文，预测第 t 个 token。
有两个 Transformer 不能满足的标准要求：
1. 为了预测 token x_t，模型应该只看到 x_t 的位置，而不是 x_t 的 content(我将在下一节解释什么是 content)
2. 为了预测 token x_t，模型应该将 x_t 之前的所有 token 编码为 content

特别是第一个要求，transformer 将位置编码合并到 token 嵌入中。
因此，它不能将位置信息与 token 嵌入分离开来。

## BERT 有这样的问题吗？

BERT 是一个 AE 语言模型，它不像 AR 语言模型那样需要单独的位置信息。
与 XLNet 需要位置信息来预测第 t 个 token 不同，BERT 使用 `[MASK]` 来表示要预测哪个 token(我们可以认为 `[MASK]` 只是一个占位符)。
例如，如果 BERT 使用 x2, x1, x4 来预测 x3，那么 x2, x1, x4 的嵌入包含了位置信息和其他与 `[MASK]` 相关的信息。
因此，该模型很有可能预测 `[MASK]` 是x3。

在这里我将对*信息*做更详细的说明。
BERT embedded (BERT 所学的信息)包含两种信息：位置信息和内容信息(为简单起见，我将其分为两部分)。

在这里我将对信息做更详细的说明。BERT embedded (BERT 所学的信息)包含两种信息：位置信息和内容信息(为简单起见，我将其分为两部分)。

![](https://ask.qcloudimg.com/http-save/yehe-1622140/f2rgs3v40s.png?imageView2/2/w/1620)

位置信息很容易理解，因为它告诉模型当前 token 的位置。
内容信息(语义和语法)包含当前标记的“意义”。
一个直观的例子是 $kind - man + woman = queen$。

## XLNet 怎么解决这个问题？

![](https://ask.qcloudimg.com/http-save/yehe-1622140/lnsrxamfjw.png?imageView2/2/w/1620)

XLNet 提出了双流自注意力来解决这个问题。
顾名思义，它包含两种自注意力。
一个是 content stream attention，它是 Transformer 中的标准自注意力。
另一个是 query stream attention。
XLNet 引入它来替换 BERT 中的 `[MASK]` token。
例如，如果 BERT 想用上下文单词 x1 和 x2 的知识来预测 x3，它可以使用 `[MASK]` 来表示 x3 token。
`[MASK]` 只是一个占位符。x1 和 x2 的嵌入包含位置信息，帮助模型“知道” `[MASK]` 的是x3。
XLNet 的情况有所不同。一个 token x3 将服务两种角色。
当它被用作内容来预测其他标记时，我们可以使用内容表示(通过内容流注意力来学习)来表示 x3。
但是如果我们想要预测 x3，我们应该只知道它的位置而不是它的内容。
这就是为什么 XLNet 使用查询表示(通过查询流注意力来学习)来保留 x3 之前的上下文信息，只保存 x3 的位置信息。
为了直观地理解双流自注意力，我们可以认为 XLNet 用查询表示代替了 BERT 中的 `[MASK]`。他们只是选择不同的方法做同一件事。

## 注意力 mask：XLNet 如何实现重排列？

![](https://ask.qcloudimg.com/http-save/yehe-1622140/r8db9vmsk3.jpeg?imageView2/2/w/1620)

这个句子的原始顺序是[x1, x2, x3, x4]。我们随机得到一个分解的顺序为[x3, x2, x4, x1]。

左上角是内容表示的计算。如果我们想要预测x1的内容表示，我们应该拥有所有4个token内容信息。KV = [h1, h2, h3, h4]和Q = h1。

左下角是查询表示的计算。如果我们想要预测x1的查询表示，我们不能看到x1本身的内容表示。KV = [h2, h3, h4]，Q = g1。

右下角是整个计算过程。我把它从头到尾解释了一遍。首先，h和g被初始化为e(xi)和w。在内容掩码和查询掩码之后，双流注意力将输出第一层输出h^(1)和g^(1)，然后计算第二层。

注意右边的内容掩码和查询掩码。它们都是矩阵。在内容mask中，第一行有4个红点。这意味着第一个token (x1)可以看到(注意到)所有其他tokens，包括它自己(x3->x2->x4->x1)。第二行有两个红点。这意味着第二个token (x2)可以看到(注意到)两个token(x3->x2)。等等。

内容掩码和查询掩码之间惟一的区别是，查询掩码中的对角元素为0，这意味着token不能看到它们自己。

让我们总结一下。输入的句子只有一个顺序。但是我们可以使用不同的注意力mask来实现不同的分解顺序。




## Reference

- [什么是 XLNet 中的双流自注意力](https://cloud.tencent.com/developer/article/1593111)