---
title: 双流自注意力
date:
tags: []
---

在我之前的文章什么是 XLNet，它为什么比 BERT 效果好？
中，我主要讨论了 XLNet (AR语言模型)和 BERT (AE语言模型)之间的区别以及重排列语言建模。
我相信对 XLNet 有一个直观的理解远比实现细节重要，所以我只解释了重排列语言建模，而没有提到另一个重要的部分，双流自注意力架构。
但正如陈家明在评论中提到的，双流自注意力是 XLNet 论文的另一个亮点，所以我写这篇文章是为了尽可能清楚地解释双流自注意力。

内容结构如下。

- 重排列语言建模的快速回顾
- 重排列带来了什么问题？
- BERT 有这样的问题吗?
- XLNet 如何解决这个问题?
- 注意力 mask：XLNet 如何实现重排列？

## 重排列建模的快速回顾

XLNet 提出使用重排列语言建模，使 AR 语言模型学习双向上下文。
这样可以避免 AE 语言模型中 MASK 方法带来的弊端。

## 重排列带来了什么问题？

重排列语言建模的目标函数，即以 t-1 个 tokens 为上下文，预测第 t 个 token。
有两个 Transformer 不能满足的标准要求：
1. 为了预测 token x_t，模型应该只看到 x_t 的位置，而不是 x_t 的 content(我将在下一节解释什么是 content)
2. 为了预测 token x_t，模型应该将 x_t 之前的所有 token 编码为 content

特别是第一个要求，transformer 将位置编码合并到 token 嵌入中。
因此，它不能将位置信息与 token 嵌入分离开来。

## BERT 有这样的问题吗？

BERT 是一个 AE 语言模型，它不像 AR 语言模型那样需要单独的位置信息。
与 XLNet 需要位置信息来预测第 t 个 token 不同，BERT 使用 `[MASK]` 来表示要预测哪个 token(我们可以认为 `[MASK]` 只是一个占位符)。
例如，如果 BERT 使用 x2, x1, x4 来预测 x3，那么 x2, x1, x4 的嵌入包含了位置信息和其他与 `[MASK]` 相关的信息。
因此，该模型很有可能预测 `[MASK]` 是x3。

在这里我将对*信息*做更详细的说明。
BERT embedded (BERT 所学的信息)包含两种信息：位置信息和内容信息(为简单起见，我将其分为两部分)。



## Reference

- [什么是 XLNet 中的双流自注意力](https://cloud.tencent.com/developer/article/1593111)