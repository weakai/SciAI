---
title: WL test
date: 2022-7-18
---

今天学习斯坦福大学同学 2019 年的工作《HOW POWERFUL ARE GRAPH NEURAL NETWORKS?》，这也是 Jure Leskovec 的另一大作。

我们知道 GNN 目前主流的做法都是通过迭代地对邻居进行「聚合」（aggreating）和「转换」（transforming）来更新节点的表示向量。
而在这篇文章中，本文作者提出了一个可以用于分析 GNN 能力的理论框架，通过对目前比较流行的 GNN 变体（如 GCN、GraphSAGE 等）进行分析，其结果表明目前的 GNN 变体甚至无法区分某些简单的图结构。
本文作者设计了一个简单的架构 GIN（Graph Isomorphism Network），并证明该架构在目前所有 GNN 算法中最具表达能力，并且具有与 Weisfeiler-Lehman 图同构测试一样强大的功能。
读完这段介绍大家可能会有多疑问，包括但不限于：

- 「为什么 GCN、GraphSAGEE 无法区分简单的图结构」？
- 「分析 GNN 捕获图结构的能力的理论框架是什么」？
- 「Weisfeiler-Lehman 图同构测试是什么」？
- 「为什么提出的 GIN 要与 Weisfeiler-Lehman 图同构测试进行比较」？

接下来，我们带着问题来阅读本篇文章。

## Introduction

GNN 的许多变体都采用了不同的邻域聚合图级别的池化方案，虽然这些变体在节点分类、连接预测和图分类等任务中取得了 SOTA，但是这些 GNN 的设计主要是基于经验而谈，并没有很好的理论基础来分析 GNN 的性质和局限性。

于是，作者提出了一个理论框架用于分析 GNN 及相关变体的表达能力和区分不同图结构的表现力。该框架的灵感来源于 Weifeiler-Lehman 图同构测试（以下简称 WL-test），WL-test 非常强大，其可用于区分各种图结构。与 GNN 类似，WL-test 可以通过聚合邻居节点的特征向量来迭代给定的特征向量，但目前的 GNN 的表达能力都不如 WL-test。WL-test 之所以那么强主要原因在于**「单射聚合更新」**（injective aggregation update），所以如果要想获得与 WL-test 一样强的表现力，首先考虑对 GNN 的聚合方案对单射函数进行建模。

本文有以下贡献：

1. **「证明出 WL-test 是 GNN 的表达能力上限」**；
2. **「提出了一个可以用于分析 GNN 能力的理论框架」**；
3. **「设计了聚合函数和读出函数，使得 GNN 可与 WL-test 一样强大」**；
4. **「构建了一个与 WL-test 一样强大的架构——GIN」**；

## Weisfeiler-Lehamn

为了让大家无痛学习，我们先写介绍一下 WL-test。

### Graph Isomorphism

为什么要计算图同构：我们在分析社会网络、蛋白质、基因网络等通常会考虑彼此间的相似度问题，比如说，具有相似结构的分子可能具备相似的功能特性，因此度量图的相似度是图学习的一个核心问题。

而图同构问题通常被认为是 NP 问题，目前最有效的算法是 Weisfeiler-Lehman 算法，可以在准多项式时间内进行求解。

### 1-dimensional

WL 算法可以是 K-维的，K-维 WL 算法在计算图同构问题时会考虑顶点的 k 元组，如果只考虑顶点的自身特征（如标签、颜色等），那么就是 1-维 WL 算法。

### Upper limit of GNN performance

结论也在文章的开头说了，WL-test 是 GNN 的性能上界。我们这里来证明一下。

直观来说，一个好的 GNN 的算法仅仅会在两个节点具有相同子树结构的情况下才会将其映射到同一位置。由于子树结构是通过节点邻域递归定义的，所以我们可以将分析简化为这样一个问题：GNN 是否会映射两个邻域（multiset）到相同的 Representation？一个好的 GNN 永远不会将两个不同领域映射得到相同的 Representation。即，**「聚合模式必须是单射」**。因此，我们可以将 GNN 的聚合方案抽象为一类神经网络可以表示的多重集函数，并分析其是否是单射。

**「引理 1」**：对任意两个非同构图  和 ，如果存在一个图神经网络  将图  和  映射到不同的 Embedding 向量中，那么通过 WL-test 也可以确定  和  是非同构图。

其证明方法用的是反证法：假设在迭代 k 此后，图神经网络有可以判断，但 WL 判断。那么在第 0 到 k-1 的迭代过程中， 和  都需要具有相同 Multiset（如果是不同的 Multiset 会得到唯一的新标签）。这也间接说明  和  的在之前的迭代过程中除了自身节点相同外，邻居节点也相同，而 GNN 的聚合过程中，AGGREGATE 和 COMBINE 函数都是不变的，即对于相同的输入会得到相同的输出，所以会一直保持一种迭代状态：，GNN 的 READOUT 函数对顺序不敏感，所以最终会得到  和  两图同构，与条件出现了矛盾。

## GIN

由引理 1 可知，WL-test 是 GNN 的性能上界，那么是否存在一个与 WL-test 性能相当的 GNN 呢？答案是肯定的。

**「定理 1」**：设 GNN 有  ，多于可以通过 WL-test 判断的两个图  和 ，在 GNN 层数多的情况下，满足以下情况时，GNN 也可以判断两个图：

a）GNN 的节点聚合和更新函数通过以下公式进行迭代：

其中，f 作用在 multisets 上， 为**「单射函数」**。

b）GNN 作用在 multiset 的 READOUT 函数也是**「单射」**的。

证明就不证了，感兴趣的可以去看论文附录。

通过定理 1 我们看出 GNN 和 WL-test 的主要区别在于**「单射函数」**中。顺利成章的，作者设计一个满足单射函数的图同构网络（Graph Isomorphism Network，以下简称 GIN）。

## Comparation

这一节我们分析下为什么诸多 GNN 变体没有 GIN 能力强，其实也就是分析下 GNN 变体不满足单射的原因。

### Layer

首先，大部分 GNN 变体的层数都比较少，大部分都只有一层。单层感知机的行为就像是线性映射，此时 GNN  层对退化为简单的对领域特征求和。虽然我们可以证明在有偏执项和足够多的输出维度的情况下，单层感知机是可以完成多重集的单射的，但这种映射无法捕获图结构的相似性。并且对于单层感知机来说，有时会出现数据拟合严重不足的情况，其拟合能力无法与 MLP 相提并论，并且在测试精度上往往比具有 MLP 的 GNN 表现更差。

### Structures

其次，对于 sum 操作来说，mean 和 max 池化操作可能会混淆某些结构，以下图为例：

max 和 min 并不满足单射性，故其性能会比 sum 差一点

### Information

- sum 可以捕捉到全部到所有的标签及其数量，mean 只能学习到标签的相对分布信息（标签比例），max 则偏向于学习具有代表性的信息（标签集合）；
- sum 可以学到网络结构信息，而其他不可以。

## Experiments

- 拟合:
  - 不同条件 GNN 的拟合能力，可以看到 GIN 的最终结果还是比较接近于 ML 的

- 泛化:

  - GIN-0 比 GIN-* 的性能稍微好一点，可能是因为模型简单的缘故；

  - GIN 和 SUM-GNN 可以捕捉到图结构，所以效果不错；

## Conclusion

本文首先证明了 WL-test 是目前所有 GNN 的性能上界，并通过分析目前 GNN 出现的问题提出了构建有效的 GNN  的方法，即使用单射函数，同时也利用 MLP 拟合单射函数设计出一个新的架构模型 GIN，并通过实验证明 GIN 的性能逼近 WL-test。

## Others

![](https://ask.qcloudimg.com/http-save/yehe-3534454/zcnsiyezoa.png?imageView2/2/w/1620)

在一维 WL-test 中，下面这两张图是非同构的，但是在二维 WL-test 中下面这两图是同构的。

有学者证明出 WL-test 的维度最多为 3 维，感兴趣的同学可以自己去探索。


## References

- [【GNN】WL-test：GNN 的性能上界](https://cloud.tencent.com/developer/article/1624944)