# 词向量各向异性

- [强烈推荐 | Bert中的词向量各向异性具体什么意思啊？](https://www.zhihu.com/question/460991118)
- [Bert做不好无监督文本匹配的原因找到了！！！](https://zhuanlan.zhihu.com/p/327992556)

#### 为什么 Bert 做无监督文本匹配效果不好？

简单说就是 Bert 输出向量语义信息是足够的，但是 consine 这种简单东西不能很好的度量出来而已。

词向量各向异性导致 sen-emb 相似度很大。

#### 各项异性 anisotropic

各向异性就有个问题，那就是最后学到的向量都挤在一起，彼此之间计算余弦相似度都很高，并不是一个很好的表示。一个好的向量表示应该同时满足Alignment 和 uniformity，前者表示相似的向量距离应该相近，后者就表示向量在空间上应该尽量均匀，最好是各向同性的。

- 现象: 高频离原点近+高频分布紧密
  - 词频影响了词向量空间分布: 这种相似性是不能让人信服的
  - 词频影响词向量空间稀疏性

#### 如何消除各向异性？

- 映射为各向同性
  - BERT-flow 的工作就是将原来的分布校准为高斯分布。标准的高斯分布就是各向同性的。
  - 类似的还有 whitening 操作。大概流程就是根据 SVD 分解的结果，旋转缩放后得到一个标准正态分布。
- 消除主成分
- 正则化
