# 对比学习

对比学习可以用来学习数据的特征表示
对比学习方法的核心：是将一个样本通过数据增广的方式获得两个增广，来自相同样本的增广互正例，来自不同样本的增广互为负例。
对比学习是一种判别方法：将相似的样本彼此靠近，而相异的样本彼此远离。
对比学习与句子表征：对比学习直接进行的是句子级别的语义拉近和拉远，而不是 token 级别的，对于句子级别的表达是很有必要的。

## 对比学习简介

### 什么是对比学习

Metrics learning + self-supervised learning

![](https://pic3.zhimg.com/80/v2-379099fa00fca7e8750bf6abe849d3aa_720w.jpg)

对比学习的目标是学习一个编码器，此编码器对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同。

### 什么是模型坍塌？

回答之前先介绍下什么是好的对比学习系统。
好的对比学习系统右 2 个要素： Alignment 和 Uniformity。
1. Alignment 代表我们希望对比学习把相似的正例在投影空间里面有相近的编码。
2. Uniformity 代表什么含义呢?当所有实例映射到投影空间之后，我们希望它在投影空间内的分布是尽可能均匀的。

追求分布的 Uniformity 的目的。
我们希望实例映射到投影空间后，在对应的 Embedding 包含的信息里，可以更多保留自己个性化的与众不同的信息。
比如有两张图片，都是关于狗的，但是一张是在草地上跑的黑狗，一张是在水里游泳的白狗。
如果在投影成 Embedding 后，模型能各自保留各自的个性化信息，那么两张图片在投影空间里面是有一定距离的，以此表征两者的不同。
而这就代表了分布的均匀性，指的是在投影球面上比较均匀，而不会说因为都是关于狗的图片，而聚到球面的同一个点中去，那就会忽略掉很多个性化的信息。
这就是说为什么 Uniformity 分布均匀代表了编码和投影函数 f 保留了更多的个性化信息。

什么是模型坍塌？
不论你输入的是什么图片，经过映射函数之后，在投影空间里面，所有图像的编码都会坍塌到同一个点。
坍塌到同一个点又是什么含义呢？
就是说不论我的输入是什么，最终经过函数映射，被映射成同一个 embedding，所有图像对应的 Embedding 都是一样的，这意味着你的映射函数没有编码任何有用的信息。

如何防止模型坍塌？
通过引入负例来防止模型坍塌。
InfoNCE 的分子部分体现了 Alignment 这个要素，因为它期望正例在投影空间里面越近越好，也就是相似性越大越好。
它防止坍塌是靠分母里的负例，负例体现了 “Uniformity” 属性，也就是说，如果图片和负例越不相似，则相似性得分越低，代表投影空间里距离越远，则损失函数就越小。
InfoNCE 通过强迫图片和众多负例之间，在投影球面相互推开，以此实现分布的均匀性，也就兼顾了 Uniformity 这一要素。所以可以理解为 SimCLR 是通过随机负例来防止模型坍塌的。


### 对比学习在解决什么问题？ 

- 如何学习 representation
- 解决数据稀疏的问题
- 如何更好的利用没有 label 的数据
- 未打标的数据远远多于打标的数据，不用简直太浪费了，但是要打标又是一个耗时耗力耗钱的事儿
- **有监督学习的缺点**：
  - 泛化能力 spurious correlations（伪相关）
  - adversarial attacks（对抗攻击）

为什么现有的方法解决不了这个问题? 
有监督学习天然所带来的问题：泛化能力、过拟合、对抗攻击等等.
有监督学习本身就无法使用无标签的数据.

对比学习的优势
- 可以比较好的单独优化 representation，和下游任务无关，能够最大程度上的保留 meta-information，
- 如果一旦做有监督的学习，那抽取出来的信息就是和当前目标相关的，不排除可能学到一些噪音特征在做 data augumentation，
- 模型见到了更多的样本，记忆的东西更全，效果好也是预期之中的去噪

ConSERT、SimCSE 等方法，在应用了对比学习之后，都比之前的方法有了很大提升。
这是因为 BERT 的表示本身存在塌缩的问题。
之前 BERT-flow 的论文就有提到过，通过对 BERT 词表示的观察，会发现整体呈锥形分布：高频词聚集在锥头部，低频词分散在锥尾。
又由于高频词本身是高频的，因此会 dominate 句子表示，让整体的相似度都很高，如下图 a 所示：

![](https://pic3.zhimg.com/80/v2-9706a5c25730d420dd8e6a51b5b0a006_720w.jpg)

如果我们去掉句子中的高频词，效果就会好一些，如下图蓝色的线：

![](https://pic1.zhimg.com/80/v2-e99709675a5f1acf3a78a2f3cda6a228_720w.jpg)

再来看对比学习，它是通过拉近相同样本的距离、拉远不同样本的距离，来刻画样本本身的表示，正好可以解决BERT表示的塌缩问题。
经过对比学习增量训练后，我们也可以观察出整体的句子表示确实更均匀了些（上图b、橙色线）。

### 对比学习为何 work?

主要是因为它优化了两个目标：

1. 正例之间表示保持较近距离
2. 随机样例的表示应分散在超球面上。

并提出这两个目标分别可以用指标 **alignment** 和 **uniformity** 来衡量。

## 对比思想

动机：人类不仅能从积极的信号中学习，还能从纠正不良行为中获益。
对比学习其实是无监督学习的一种范式。根据经典的SIMCLR，我在这里就直接提供了对比学习中模型的常见形式。

![](https://pic4.zhimg.com/80/v2-a17404a49d4f980ac69653464dbcc3fb_720w.jpg)

### 前置任务

前置任务是自监督学习中非常重要的一种策略。它可以用伪标签从数据中学习表示。

伪标签是从数据本身中定义而来的。

这些任务可以应用到各种数据之中，例如图片、视频、语言、信号等。

在对比学习的前置任务之中，原始图片被当作一种 anchor，其增强的图片被当作正样本（positive sample），然后其余的图片被当作负样本。

大多数的前置任务可以被分为四类：
1. 颜色变换
2. 几何变换
3. 基于上下文的任务
4. 基于交叉模式的任务

具体使用哪种任务取决于具体的问题。

### 架构

对比学习依赖于负样本的数量，来生成好的表示。

它有时候可以看作是一个字典查询任务，字典有时候是整个训练集，有时候是训练集的子集。

有时候对比学习可以根据如何采样负样本来进行分类。

我们将对比学习的架构分为以下四类（见图十一）。

![](https://pic2.zhimg.com/80/v2-f54582825895c2944989a919fa319065_720w.jpg)

图十一：
（a）端到端训练，双塔模型，一个 encoder 用来生成正样本的表示，一个 encoder 用来生成负样本的表示；
（b）使用一个 memory bank 来存储和抽取负样本；
（c）使用一个 momentum encoder 当作一个动态的字典查询来处理负样本；
（d）额外使用一个聚类机制

#### 端到端学习

端到端学习是一种复杂的基于梯度的学习系统，其中所有的模块都是可微的。
这种架构偏好于大的 batch size 来存储更多的负样本。
除了原图片和其增强的图片，其余的 batch 中的图片被当作负样本。
这种架构包含两个 encoder：一个 query 一个 key（见图十一a）。

这两个 encoder 可以是一样的，也可以是不一样的。

使用一个**对比损失**，模型会让正样本的表示相近，让负样本和正样本的表示相远。

最近，一种端到端的模型 SimCLR 获得了很大成功。
他们使用了非常大的 batch size（4096）训练了 100 个 epochs。
**SimCLR 证明了一种简单机制的模型也可以获得非常好的效**果。

另外一种端到端的模型 _CPC_ 从高维的时序数据中学习表示，其使用对比损失来预测将来。

端到端学习中的负样本与 batch size 有关。
而 batch size 的大小受限于 GPU/TPU 内存，所以这里有一个计算资源的限制，而且如何优化大 batch 训练也是一个问题。

#### 使用 Memory Bank

端到端依赖于大的 batch（译者：存疑，某些任务可能不需要），所以一种可行的解决方案是使用 _memory bank_。

Memory bank：
的作用是在训练的时候维护大量的负样本表示。
所以，创建一个字典来存储和更新这些样本的嵌入。
Memory bank M 在数据集 D 中对每一个样本存储一个表示 。
该机制可以更新负样本表示，而无需增大训练的 batch size。

_PIRL_ 是一种使用了 Memory bank 来学习图像表示的方法。
但是，在训练的时候维护一个大的 memory bank 是一个很复杂的任务。
这种策略的一个缺点是更新表示的计算复杂度很高。

#### 使用 Momentum Encoder

为了解决 memory bank 的缺点，_momentum encoder_ 被提了出来。
这种机制创建了一种特殊的字典，它把字典当作一个队列的 keys，当前的 batch 进入队列，最老的 batch 退出队列。
Momentum encoder 共享了 encoder Q 的参数。它不会在每次反向传播后更新，而是依据 query encoder 的参数来更新： 

![](https://pic2.zhimg.com/80/v2-56490e26511e78c76f7089969ca910c1_720w.jpg)

#### 特征表示聚类 Clustering Feature Representation

上面介绍的三种架构都是用某种相似度衡量来对比样本，使得相似样本相近，不相似样本变远，从而学习到好的表示。

本节介绍的机制使用两个共享参数的端到端架构，这种架构使用聚类算法来聚类相似样本表示。

SwAV 使用了聚类方法。
其背后的 idea 在于，在一个嵌入空间中，猫的样本们应该和狗的样本们相近（都是动物），而与房子的样本们相远。

在基于样本的学习中，每个样本被当作一个数据集中的离散类。
离散类在连续的嵌入空间中（相似的样本表示相近）可能会有问题。
例如在一个 batch 里，正样本是猫，负样本们中也有猫，模型会让正样本的猫和负样本中的猫变远，不利于表示学习。

### 编码器

在自监督学习中，Encoder 非常重要，因为它们把数据样本遍历到隐空间中。

![](https://pic2.zhimg.com/80/v2-dbf97f6e3d3c9411814c2fb1ad4c2b75_720w.jpg)

没有一个强大的 encoder 的话，模型可能难以学到有效的表示，从而执行分类任务。
对比学习中的大多数模型都采用了 ResNet 或其变种。

### 训练

为了训练一个 encoder，需要一个前置任务(pretext task)来利用对比损失来进行反向传播。
对比学习最核心的观点是将相似样本靠近，不相似样本靠远。
所以需要一个相似度衡量指标来衡量两个表示的相近程度。
在对比学习中，最常用的指标是 cosine similarity。

$$cos_sim(A,B)=\frac{A \cdot B}{\lVert A \rVert \lVert B \rVert}$$

Noise Contrastive Estimation (NCE) 函数定义为：

![](https://pic1.zhimg.com/80/v2-4564d7b4bb1f1e5658885a3ed6bf63e0_720w.png)

如果负样本的数量很多，NCE 的一个变种 InfoNCE 定义为：

![](https://pic3.zhimg.com/80/v2-9106987a2429f009f2d6ce8387564f2e_720w.png)

与其他深度学习模型类似，对比学习应用了许多*训练优化算法*。训练的过程包括*最小化损失函数*来学习模型的参数。
常见的优化算法包括 SGD 和 Adam 等。
训练大的 batch 的网络有时需要*特殊设计的优化算法*，例如 LARS。

### 如何构造负例

正例。平移、旋转、删除某个特征等
负例。batch 内随机采样、全局负采样等

对比学习希望习得某个表示模型，它能够将图片映射到某个投影空间，并在这个空间内拉近正例的距离，推远负例距离。
也就是说，迫使表示模型能够忽略表面因素，学习图像的内在一致结构信息，即学会某些类型的不变性，比如遮挡不变性、旋转不变性、颜色不变性等。

## 对比损失【重要*数学警告】

- [再谈 softmax 和对比学习](https://zhuanlan.zhihu.com/p/441985870)

## 相关研究

###  噪音对比估计（NCE）

- [论文地址](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)
- [噪音对比估计（NCE）| 基础知识以及推导](https://blog.csdn.net/littlely_ll/article/details/79252064)
- [Noise Contrastive Estimation 学习 | 很好的讲解](https://zhuanlan.zhihu.com/p/58369131)

噪音对比估计（NCE, Noise Contrastive Estimation）是一种新的统计模型估计方法，用来解决神经网络的复杂计算问题，因此在图像处理和自然语言处理中得到广泛应用。
word2vec 中 的 negtive sample 是 NCE 的一种特殊情况。
利用 Noise-Contrastive Estimation 的确可以有效地回避 Softmax 的运算，因为 MNIST 数据集只有 10 种分类，效果不明显。
但是对于自然语言处理，比如词向量的生成，如果要把整个词汇表上的概率进行 softmax 运算，开销就十分巨大了，因此 NCE 就显得十分必要。

**One Sentence Summary**: 
给现有的样本加上噪声，作为负样本，模型对原本真实的样本和新增的负样本进行区分，从而将 softmax 这种低效的计算转化成二分类的 logloss，计算效率更高.
但是这里会出现一个问题，如果我们的 noise 分布选的不好，效果可能并不一定会好，所以在这篇 paper 里作者也提到了在增加 noise 的时候，
尽量和现有的数据分布相似，这样能最大程度上提高训练的效果.

### InfoNCE

Representation Learning with Contrastive Predictive Coding (InfoNCE)

- [对 CPC (对比预测编码) 的理解](https://zhuanlan.zhihu.com/p/317711322)

One Sentence Summary：提出 CPC 框架，将 representation 的学习从一个生成模型，转换成了分类模型，用于下游的其他任务。

文章比较有意思的点：证明了互信息的下界和总样本数是有关的，N越大，互信息越大.

### Deep InfoMax

- [对 Deep InfoMax（DIM）的理解](https://zhuanlan.zhihu.com/p/521614413)

无监督学习一个重要的问题就是学习有用的 representation，
本文的目的就是训练一个 representation learning 函数（即编码器encoder） ，
其通过最大编码器输入和输出之间的互信息(MI)来学习对下游任务有用的 representation，而互信息可以通过 MINE 的方法进行估算。

作者还提到，直接最大化全部输入和编码器的输出（即全局的 MI）更适合于重建性的任务，而在分类的下游任务上效果不太好。
而最大化输入的局部区域（例如不是完整的图片，而是图片中的一块）和输出（即学到的 representation）的平均互信息在下游任务（如图像分类）上的效果更好。

因为这种方法类似于对抗自动编码器(AEE, adversarial autoencoders) 的，将 MI 最大化和先验匹配结合起来，根据期望的统计特性约束 representation，
并且还与 infomax 的优化规则密切相关，因此作者称其为 Deep InfoMax(DIM).

### Prototypical Contrastive Learning of Unsupervised Representations

作者提出了原型对比学习（PCL），它是无监督表示学习的一种新方法，综合了对比学习和聚类学习的优点。

### SimCLR

对比学习的概念很早就有了，但真正成为热门方向是在 2020 年的 2 月份，Hinton 组的 Ting Chen 提出了 SimCLR，
用该框架训练出的表示以 7% 的提升刷爆了之前的 SOTA，甚至接近有监督模型的效果。
我们首先以 SimCLR 为例来介绍一个比较“标准”的对比学习模型，其实，在 SimCLR 之前已经提出不少对比学习模型，比如 Moco V1 出现就比 SimCLR 早。

SimCLR 模型由对称的上下两个分枝（Branch）构成，说白了就是双塔模型。
随机从无标训练数据中取 N 个构成一个 Batch，对于 Batch 里的任意图像，根据前面样本构造方法形成两个图像增强视图：Aug1和Aug2。
Aug1 和 Aug2 各自包含 N 个增强数据（1个正，其余为负例），并分别经过上下两个分枝，对增强图像做非线性变换，
这两个分枝就是 SimCLR 设计出的表示学习所需的投影函数，负责将图像数据投影到某个表示空间。

### 自我引导的对比学习（一个BERT不够，那就两个）

Self-Guided Contrastive Learning for BERT Sentence Representations

来自首尔大学，讨论的问题是如何在不引入外部资源或者显示的数据增强的情况下，利用 BERT 自身的信息去进行对比，从而获得更高质量的句子表示？
文中对比的是：BERT 的中间层表示和最后的 CLS 的表示。
模型包含两个 BERT，一个 BERT 的参数是固定的，用于计算中间层的表示，其计算分两步：
1. 使用 MAX-pooling 获取每一层的句子向量表示
2. 使用均匀采样的方式从 N 层中采样一个表示
   - 另一个 BERT 是要 fine-tune 的，用于计算句子 CLS 的表示。
   - 同一个句子的通过两个 BERT 获得两个表示，从而形成正例，负例则是另一个句子的中间层的表示或者最后的 CLS 的表示。

![](https://pic1.zhimg.com/80/v2-6c05bc42682230595d9e452223930c50_720w.jpg)

文中还对比了不同负例组合的方式，最后发现只保留 CLS 的表示和隐藏层的表示之间的对比，忽略 CLS 和 CLS 以及中间层和中间层之间的对比是最优的，即保留(1)(3)。

![](https://pic2.zhimg.com/80/v2-3b7c9a55967072fb299cbaba84c48411_720w.jpg)

这篇论文没有选择直接从底层数据增强角度出发，是稍微偏模型方法的改进的，侧重挖掘模型内部的信息。
主实验是在 STS 和 SentEval 任务上测试的，从结果来看的话，仍然是 SimCSE 要好很多，而且 SimCSE 操作起来是更简单的。
不过本文也是提供了一个不一样的思路。

### 花式数据增强

ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer

来自北邮的工作，也是研究如何在无监督的模式下，学习更好的句子表示。
该工作主要对比了使用 4 种不同的数据增强方式进行对比对句子表示的作用。

![](https://pic2.zhimg.com/80/v2-c4198c4dd4f38ea9788947e09a3cd91d_720w.jpg)

模型是在 STS 任务上进行评估的。和 SimCSE 一样也用了 NLI 做监督，整体性能比 SimCSE 低 1-2 个点。

### 无监督文本表示

DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations

DeCLUTR 来自多伦多大学，是 NLP 领域使用对比学习中较早的一篇，去年 6 月份就放到 arxiv 上面了。
文章研究的问题同样是：如何利对比学习从大量无标注数据学习更好的通用句子表示？文中的对比体现在两个方面：
1. 对比来自不同文档的文本片段(span)的语义。如果两个文本片段(span)来自同一个文档，那么他们的语义表示的距离应该相对较近，否则距离远；
2. 对比来自同一文档的文本span。当两个文本片段都来自同一个文档，如果他们在文档中的位置距离比较近，他们的语义表示距离近，否则远。

在采样正例的时候有些讲究。
具体来讲是先从一个文档中采样 N(>=1) 个原始文本片段 (anchor span)，然后从每个锚点span周围采样，作为正例 span。
采样规则是正例 span 可以与锚点 span 交叠、相邻、从属。
负例是从一个 batch 中随机采样得到的。
对比学习的损失函数是 InfoNCE。模型整体的损失函数是 InfoNCE 和 MLM 的加和。

实验是在 SenEval benchmark(28个数据集）上进行测试的，包含有/半监督任务和无监督任务。
有/半监督任务的 baseline 有 InferSent，Universal Sentence Encoder 和 Sentence Transformers；无监督任务的 baseline 有 QuickThoughts。
最显著的实验结果是 DeCLUTR 在大部分的数据集上取得了 SOTA，并且在无监督任务上取得了和有监督任务相当的结果。

接下来两篇文章是关于如何利用对比学习提升自然语言理解任务的性能。

### 论负例对对比学习的重要性

CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding

来自清华大学，文章探讨的是如何利用对比学习提高模型的*鲁棒性*。
在初步实验中发现**用探针对句子语义进行轻微扰动**，模型就会预测错误。
之前的对抗训练确实能够从扰动的样本中学，但是主要侧重于习语义相似的扰动，忽略了语义不同或者相反的扰动。
这样的语义改变无法被对抗学习无法检测到。
**本文提出 CLINE，使用无监督的方法构造负样本。**
通过同时利用语义相似和相反的样例和原始样例进行对比，模型可以侦测到扰动导致的语义的改变。

正负例的构造：**正例是将句子中的词(名词、动词、形容词）替换为其同义词. 负例是将句子中的词替换为其反义词或者随机选择的词。**

文中的损失函数由三部分构成：
1. 掩码语言模型的 MLM 损失
2. 检测当前词是否是被替换的词的损失 RTD
3. InfoNCE对比正例和负例。

有个小细节不太一样的是对比 InfoNCE 中并没有引入温度参数 τ。

实验是在NLU任务上进行的，包括NLI(SNLI, PERSPECTRUM,) 情感分析(IMDB,MB) 阅读理解 （BoolQ）, 新闻分类(AG)。
实验结果表明使用CLINE训练的模型可以同时在对抗测试集和对比测试集上提升性能。

### 对比实例学习+远距离监督关系抽取

CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction

来自阿里巴巴-浙江大学前沿技术联合研究中心，研究**如何利用对比学习提高远距离监督的关系抽取任务的性能**。

从对比角度讲，正例是同一关系下的*实例对*，负例是不同关系的*实例对*。
**文中的重点是在有噪声的情况下，如何构造正负例。**
CIL 的 baseline 是多实例对比学习，是将多个属于同一关系的实例放在一个 bag 中，一起训练得到一个关系的表示。
每个实例都被假设是表达了一个实体对之间的关系。

模型的损失函数是 InfoNCE 对比损失和 MLM 损失的加权和。
CIL 在 NYT10，GDS 和 KBP 三个数据集上取得较大提升。

### Post-training 中使用对比学习

Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene

来自中山大学，本文主要针对样本量稀少的场景，如何使用对比学习先在无标注数据集进行 post-training， 然后再在有标注数据集上 fine-tuning。

对比方法：互补的 mask 方法，将一个输入进行多次 mask，第一次的 mask 的比例是 $p_c$ ,
第二次mask的时候只针对第一次mask中没被选择的token以 $p_m$ 的比例进行mask,所以两个句子被mask的部分是互补的，第三次以此类推。
对比是在多个被mask的输入上进行的。这样做的好处是既可以避免 $p_m$ 太小时，两个句子太相似导致对比损失迅速降到0，也可以避免 $p_m$ 太大而导致模型无法恢复mask的内容。
（和SimCSE的直接两次dropout相比复杂了点，但有异曲同工之妙）。

实验是在少样本 GLUE 上进行的，只有 20 个样例的时候提升不是很明显，样本 100 和 1000 的时候相比之前 SOTA 有轻微提升。

## 对比学习的应用

- 在召回阶段：
  - 负采样逻辑优化
    - 现在很多推荐系统里的负采样都是随机采样的，虽然随机采样已经能拿到不错的效果了，但是一定还有优化空间
    - 比如 InfoNce 文章中提到的增大负采样的个数能够提升互信息的下界
    - 同时在工业界的 best practice 中，往往也能发现 batch_softmax 的效果也很不错
- i2i召回
  - 基于item2item 召回
    - 往往是基于 item1 的 embedding 去召回与 item1 相似的 items，这时候 embedding 的相似性就显得尤为重要，但是这里有个问题
    - 如何选择构造与 item1 相似的 item 作为正样本？
    - （用户点击过的作为 item 作为相似的样本也不合理，毕竟用户的兴趣是多维的，点过的每个样本不可能都属于同一个类别或者相似的，虽然通过推荐的大量数据，
    - 协同过滤可能可以在全局找到一个还不错的结果，但是直接拿用户序列作为正样本来做一定是不合理的） 

### 对比学习与 NLP



## 发展历程

第一阶段：百花齐放

InstDisc(Instance Discrimination)
意义：对比学习里程碑
代理任务：个体判别
Methods: memory bank, image2vec, 对比学习
技巧：proximal regularization 类似于动量更新

InvaSpread
Invariant Spreading
Methods: 端到端流派，单个编码器
缺点：字典太小，作者没有 TPU，SimCLR 的前身

CPC Contrastive Predictive Coding
意义：通用结构：图片音频文字甚至强化学习

CMC Contrastive Multiview Coding
意义：更广泛
方法：增大各视角的互信息，视角可以是听到的，看到的，闻到的等等
数据：同一物体的多模态数据，非同物体为负样本。

第二阶段：CV双雄

MoCo
提出队列，动量编码器 -> 又大又好的字典 -> 更好地进行对比学习。
队列作为额外数据结构，代替 InstDisc 的 Memory bank 保存负样本。动量编码器更新编码器，而不是特征。

SimCLR
Simple Contrastive Learning of Visiual Representations
概念和方法都比较简单
batch size 太大，一般人不好上手。
共享权重的两个编码器，其实就是一个编码器
非线性变换：对隐藏特征应用一个 project，就是 mlp（含 relu），给模型提了将近 10 个点
normalized + templature-scaled: L2 Norm + T 缩放因子，和 infoNCE 类似
很多优秀的数据增强方法。
对比学习的维度不需要太高，这里是 128.

MoCov2
更多数据增强 + MLP 层 + cosine leraning rate schedule + 更长的 epochs

SimCLRv2

SWaV
对比学习 + 聚类的方法
加入先验知识（聚类的中心），减少大量负样本的近似

三阶段：不用负样本 52:30 BYOL 58:23 针对 BYOL 的博客和他们的回应 69:31 SimSiam

第四阶段：基于 Transformer 77:23 MoCov3 83:10 DINO


## 参考

- [对比学习论文综述【论文精读】](https://www.bilibili.com/video/BV19S4y1M7hm/)
- [对比学习（Contrastive Learning）综述](https://zhuanlan.zhihu.com/p/346686467)
- [对比学习（Contrastive Learning）:研究进展精要 | 长文](https://zhuanlan.zhihu.com/p/367290573)
- [对比学习（Contrastive Learning）最新综述](https://zhuanlan.zhihu.com/p/410442591)
- [ConSERT｜用对比学习做 NLP 都有哪些坑？](https://zhuanlan.zhihu.com/p/378544839)
- [一文梳理 2020 年大热的对比学习模型](https://mp.weixin.qq.com/s/6qqFAQBaOFuXtaeRSmQgsQ)
- [我分析了 ACL21 论文列表，发现对比学习已经...](https://zhuanlan.zhihu.com/p/393412001)

