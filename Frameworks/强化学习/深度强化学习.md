---
title: 深度强化学习
---

## RL 算法们

通过行为的价值来选取特定行为的方法, 包括使用表格学习的 q learning, sarsa, 使用神经网络学习的 deep q network, 还有直接输出行为的 policy gradients, 又或者了解所处的环境, 想象出一个虚拟的环境并从虚拟的环境中学习 等等

Model-free
Q learning
Sarsa
Policy Gradients

model-based RL
多了一道程序, 为真实世界建，模多出了一个虚拟环境
最终 model-based 还有一个杀手锏是 model-free 超级羡慕的. 那就是想象力

### 基于概率 和 基于价值

而基于价值的方法输出则是所有动作的价值, 我们会根据最高价值来选着动作, 相比基于概率的方法, 基于价值的决策部分更为铁定, 毫不留情, 就选价值最高的, 而基于概率的, 即使某个动作的概率最高, 但是还是不一定会选到他.
我们现在说的动作都是一个一个不连续的动作, 而对于选取连续的动作, 基于价值的方法是无能为力的. 我们却能用一个概率分布在连续动作中选取特定动作, 这也是基于概率的方法的优点之一.

比如在基于概率这边, 有 Policy Gradients, 在基于价值这边有 Q learning, Sarsa 等. 而且我们还能结合这两类方法的优势之处, 创造更牛逼的一种方法, 叫做 Actor-Critic, actor 会基于概率做出动作, 而 critic 会对做出的动作给出动作的价值, 这样就在原有的 policy gradients 上加速了学习过程.

## 回合更新 和 单步更新
## 在线学习 和 离线学习
在线学习, 就是指我必须本人在场, 并且一定是本人边玩边学习
离线学习是你可以选择自己玩, 也可以选择看着别人玩, 通过看别人玩来学习别人的行为准则

离线学习 同样是从过往的经验中学习, 但是这些过往的经历没必要是自己的经历, 任何人的经历都能被学习. 或者我也不必要边玩边学习, 我可以白天先存储下来玩耍时的记忆, 然后晚上通过离线学习来学习白天的记忆.那么每种学习的方法又有哪些呢?

最典型的在线学习就是 Sarsa 了, 还有一种优化 Sarsa 的算法, 叫做 Sarsa lambda, 最典型的离线学习就是 Q learning, 后来人也根据离线学习的属性, 开发了更强大的算法, 比如让计算机学会玩电动的 Deep-Q-Network.


优点：

- 从积极的角度看，强化学习（RL）称得上是万金油，而且它的效果好得令人难以置信，理论上讲，一个强大的、高性能的RL系统应该能解决任何问题。

- 就当前的情况看，Deep RL是最接近AGI的事物之一，它也为吸引了数十亿美元投资的“人工智能梦”提供了助燃剂。

缺点：

- 相当困难，入门困难，操作困难：头部公司养得起小团队
- 规划谬误
- DeepMind的成果很棒，这个视频刚发布时，我还因强化学习能让机器人学会跑步惊讶了许久。但在看过论文后，6400小时的CPU耗时还是有点令人沮丧。这不是说我认为它耗时过久，而是Deep RL的实际采样效率还是比预想的高了几个数量级，这一点更让人失望。
- 同样的，现成的蒙特卡洛树搜索在雅达利游戏中也能轻松超越 DQN。

## 历史

![img](https://pic1.zhimg.com/80/v2-bf578b4710c64fb2c93009b3da73c06c_720w.jpg)

## 内容

Deep Q-Networks，DQN

雅达利游戏是深度强化学习最著名的一个基准。