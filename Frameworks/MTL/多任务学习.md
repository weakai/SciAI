---
title: 多任务学习
---

多任务学习（Multitask Learning）是[迁移学习](https://aitechtogether.com/tag/迁移学习)的一种方式，通过共享表示信息，同时学习多个相关任务，使这些任务取得比单独训练一个任务更好的效果，模型具有更好的泛化性。

在[深度学习](https://aitechtogether.com/tag/深度学习)模型中，多任务学习的最直接实现方法是多个 Task 共享底层的多层网络参数，同时在模型输出层针对不同任务配置基层 Task-specific 的参数。

这样，底层网络可以在学习多个 Task 的过程中从不同角度提取样本信息。然而，这种 Hard Parameter Sharing 的方法，往往会出现跷跷板现象。

不同任务之间虽然存在一定的关联，但是也可能存在冲突。联合训练导致不相关甚至冲突的任务之间出现负迁移的现象，影响最终效果。为了解决Hard Parameter Sharing的弊端，学术界涌现了如多专家网络（Multi-expert  Network，MoE）等多种解决深度学习中多任务学习问题的方法，是学术界一直以来研究的热点，在工业界也有诸多应用。本文从最基础的多任务学习开始，梳理了近几年来7篇多任务学习顶会相关工作，包括Hard/Soft Parameter Sharing、参数共享+门控、学习参数共享方式等建模方式。

#### Hard/Soft Parameter Sharing

Hard Parameter Sharing 通过共享参数的方法对多个任务联合建模，但是从哪一层开始共享、哪一层非共享没有有效的指导信息。

### 为什么要用多任务学习？

#### 方便

在推荐任务中，往往不仅要预测用户的 engagement（例如CTR），还要预测用 satisfaction（例如评分、CVR、观看时长）。如果用多个模型预测多个目标，参数量会很大，而且在线上也不好维护。因此需要使用一个模型来预测多个目标，这点对工业界来说十分友好。

#### 效果更好

针对很多数据集比较**「稀疏」**的任务，比如短视频转发，大部分人看了一个短视频是不会进行转发这个操作的，这么稀疏的行为，模型是很难学好的（过拟合问题严重），那我们把预测用户是否转发这个稀疏的事情和用户是否点击观看这个经常发生事情放在一起学，通过参数共享，一定程度上会缓解模型的过拟合，提高了模型的泛化能力。这其实是regularization和transfer  learning。也可以理解为，其他任务的预测loss对于"转发"事件预测来说是辅助loss。从另一个角度来看，对于数据很少的新任务，这样也解决了**「冷启动问题」**。

## 参考

- [【综述专栏】《Multitask Learning》多任务学习发展的关键节点](https://toutiao.io/posts/9hqwlro/preview)