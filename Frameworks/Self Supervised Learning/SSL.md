---
title: 自监督学习
---

## 0 基础

### 0.1 学习的范式

我们首先来回顾下机器学习中两种基本的学习范式，一种是监督学习，一种是无监督学习。

监督学习利用大量的标注数据来训练模型，模型的预测和数据的真实标签产生损失后进行反向传播，通过不断的学习，最终可以获得识别新样本的能力。而无监督学习不依赖任何标签值，通过对数据内在特征的挖掘，找到样本间的关系，比如聚类相关的任务。有监督和无监督最主要的区别在于模型在训练时是否需要人工标注的标签信息。

无监督学习中被广泛采用的方式是自动编码器。对于如何学习隐层向量的研究，可以称之为表征学习。于自编码器，可能仅仅是做了维度的降低而已，我们希望学习的目的不仅仅是维度更低，还可以包含更多的**语义**特征，让模型懂的输入究竟是什么，从而帮助下游任务。而自监督学习最主要的目的就是学习到更丰富的语义表征。

### 0.2 人工智能最火的技术

AI 未来最火的技术是自监督学习。Yann Lecun在他的演讲中引入了“**蛋糕类比**”来说明**自监督学习的重要性**。如果人工智能比作一块蛋糕，那么蛋糕的**大部分**是自监督学习，蛋糕上的**糖衣**是监督学习，蛋糕上的**樱桃**是强化学习。

### 0.3 自监督学习的术语

术语“自监督学习”最早是在机器人技术中引入的，其中通过查找和利用不同输入传感器信号之间的关系来自动标记训练数据。然后，它被机器学习领域借用。在AAAI 2020 的演讲中，Yann LeCun 将自监督学习描述为 *the machine predicts any parts of its  input for any observed part*, 我们遵循 LeCun, 可以将它们概括为两个经典定义：

- 通过“半自动”过程从数据本身获取“标签”。
- 用数据的其他部分预测数据的一部分。

具体而言，此处的“其他部分”可能是不完整的，变形的，变形的或损坏的。换句话说，机器学会了“恢复”原始输入的全部，部分或一些特征。

监督,无监督,自监督的区别如下

![](https://zhengwen.aminer.cn/image-Oyvkw8Qx2w.png)

## 1 概述

### 1.1 自监督学习定义

自监督学习主要是利用辅助任务（pretext）从大规模的无监督数据中挖掘自身的监督信息，通过这种构造的监督信息对网络进行训练，从而可以学习到对下游任务有价值的表征。

所以对于自监督学习来说，存在三个挑战：

1. 对于大量的无标签数据，如何进行表征学习？
2. 从数据的本身出发，如何设计有效的辅助任务 pretext？
3. 对于自监督学习到的表征，如何来评测它的有效性？

对于第三点，评测自监督学习的能力，主要是通过 Pretrain-Fintune 的模式。 

![](https://pic1.zhimg.com/80/v2-8d077a997287e6fc7f9b5576b3e16f00_720w.jpg)

### 1.2 监督学习的意义

监督学习正在解决其瓶颈。它不仅严重依赖昂贵的手动标记，而且还存在泛化错误，虚假相关性和对抗性攻击。我们希望神经网络通过更少的标签，更少的样本或更少的试验来学习更多。

## 2 自监督学习的主要方法

自监督学习的主流方法分成三大类

1. 基于上下文（Context based）
2. 基于时序（Temporal Based）
3. 基于对比（Contrastive Based）

另一种分法是:

1. Generative Methods
2. Contrastive Methods
3. 生成对比式（对抗式）

### 2.1 基于上下文（Context based）

基于数据本身的上下文信息，我们其实可以构造很多任务，比如在 NLP 领域中最重要的算法 **Word2vec**。
Word2vec 主要是利用语句的顺序，例如 CBOW 通过前后的词来预测中间的词，而 Skip-Gram 通过中间的词来预测前后的词。

### 2.2 基于时序（Temporal Based）

之前介绍的方法大多是基于样本自身的信息，比如旋转、色彩、裁剪等。而样本间其实也是具有很多约束关系的，这里我们来介绍利用时序约束来进行自监督学习的方法。最能体现时序的数据类型就是视频了（video）。

### 2.3 基于对比（Contrastive Based）

第三类自监督学习的方法是基于对比约束，它通过学习对两个事物的相似或不相似进行编码来构建表征，这类方法的性能目前来说是非常强的，从最近的热度就可以看出，很多大牛的精力都放在这个方向上面。

### 2.4 Generative Methods

Generative Methods 是可以通过构建代理任务（proxy task）来进行自监督学习，其做法是将模型最后的输出依然是一个和原图相似大小的结果（模型本来的输出就是这个尺度或者通过GAN的方法还原），然后构建一个关于 pixel 的 loss 来完成自监督学习。

#### 2.4.1 自回归模型 (AR model)

自回归模型可被视作贝叶斯网络结构（有向图模型）。联合分布可以被分解为条件分布的乘积。其中，每个变量的概率取决于之前的变量。

自回归模型的优点是可以很好地对上下文依赖性进行建模。但是，缺点是每个位置只能从一个方向访问其上下文。

#### 2.4.2 流模型

流模型的目标是从数据估计复杂的高维密度 $p(x)$。直观地讲，直接确定密度是困难的。为了获得复杂的密度，我们希望通过堆叠一系列分别描述不同数据特征的转换函数来“逐步”生成它。

流模型的优点是 x 和 z 之间的映射是可逆的。

#### 2.4.3 自编码（AE）模型

自编码模型的目标是从（损坏的）输入中重建（部分）输入。 由于其灵活性，AE模型可能是最受欢迎的生成模型, 具有许多变体。

- 基本 AE 模型
- 上下文预测模型（CPM）
- 去噪 AE 模型
- 变分自编码器

#### 2.4.4 混合生成模型

- 结合 AR 和 AE 模型
- 结合 AE 和流模型

### 2.5 生成-对比（对抗）

生成模型在自监督学习中取得成功的原因是它具有适应数据分布的能力，可以根据该数据分布执行各种下游任务。 生成式自监督学习的目标通常被表述为最大似然函数。

## 3 挑战

## Links

- [Self-Supervised Learning 再次入门](https://zhuanlan.zhihu.com/p/108906502)
- [推荐 | 自监督学习综述 | Self-supervised Learning: Generative or Contrastive](https://www.aminer.cn/research_report/60af5aa430e4d5752f50dab4?download=false)