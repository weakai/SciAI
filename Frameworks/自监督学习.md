---
title: 自监督学习
---

自监督学习实际上与监督学习、非监督学习、半监督学习并没有本质上的鸿沟。

手动数据注释是监督学习中必不可少的步骤，这是耗时，费力且有噪声的。与有监督的方法不同，无监督的方法不依赖于人类注释，并且通常集中在数据良好表示（例如平滑度，稀疏性和分解）的预设先验上。无监督方法的经典类型是聚类方法，例如高斯混合模型，它将数据集分解为多个高斯分布式子数据集。然而，非监督学习由于预设先验的一般性较差而不太值得信赖，在某些数据集（例如非高斯子数据集）上选择将数据拟合为高斯分布可能是完全错误的。

自我监督方法可以看作是一种具有监督形式的特殊形式的非监督学习方法，这里的监督是由自我监督任务而不是预设先验知识诱发的。与完全不受监督的设置相比，自监督学习使用数据集本身的信息来构造伪标签。**在表示学习方面，自监督学习具有取代完全监督学习的巨大潜力。**人类学习的本质告诉我们，大型注释数据集可能不是必需的，我们可以自发地从未标记的数据集中学习。更为现实的设置是使用少量带注释的数据进行自学习。这称为 Few-shot Learning。

## 自监督学习的主要流派

在自监督学习中，如何自动获取伪标签至关重要。 根据伪标签的不同类型，我将自我监督的表示学习方法分为 4 种类型：基于数据生成(恢复)的任务，基于数据变换的任务，基于多模态的任务，基于辅助信息的任务。这里简单介绍第一类任务。事实上，所有的非监督方法都可以视作第一类自监督任务，在我做文献调研的过程中，我越发的感觉到事实上非监督学习和自监督学习根本不存在界限。

所有的非监督学习方法，例如数据降维(PCA:在减少数据维度的同时最大化的保留原有数据的方差)，数据拟合分类(GMM: 最大化高斯混合分布的似然)， 本质上都是为了得到一个良好的数据表示并希望其能够生成(恢复)原始输入。这也正是目前很多的自监督学习方法赖以使用的监督信息。基本上所有的 encoder-decoder 模型都是以数据恢复为训练损失。

## 术语

Hand-crafted features: 人工设计的数据特征
Few-shot Learning: 使用少量带注释的数据进行自学习