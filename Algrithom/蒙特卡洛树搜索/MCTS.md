---
title: MCTS
date: 2022-7-21
tags: [MCTS]
---

## 序言

相信许多人和我一样，对人工智能的认识始于2016年，Deepmind推出的AlphaGo与围棋大师李世石的惊世一战，让“人工智能”这个名词，被千家万户所熟知。尽管深度神经网络早在很多年前凭借其在图像识别等方面一骑绝尘的表现成为科研机构研究的热点，自此之后，人工智能才变成了一股时代的浪潮，浩浩荡荡裹挟着我们向着陌生未知的领域奔涌而去。

最近一直在做棋类博弈相关方面的探究，各种算法千变万化，都离不开对蒙特卡洛搜索算法（Monte Carlo Tree Search）这一经典搜索算法的使用，本文将以讲清基本的蒙特卡洛搜索算法（后文简称为 MCTS）为目的，介绍这一算法涉及的方方面面的知识点，力求让读者以及自己能彻底琢磨透这一算法。

关于蒙特卡洛树搜索，国内真的很难找到特别好的入门资料，很多还是错的，本文是前段时间为了实现自己的一个  AI，在阅读了几十篇国内外文章之后根据自己的理解整合写的，主要参照 INT8 的一篇英语博文 Monte Carlo Tree Search - beginners guide Machine learning  。不过虽然已经尽力写得通俗，但是蒙特卡洛搜索树比博弈树要难两个等级，没太多算法基础的可以不用搞懂每一个细节。

在很长一段时间以来，学术界普遍认为，机器在围棋领域击败人类是完全不现实的，它被认为是人工智能的“圣杯”，尽管“深蓝”在二十年前已经击败了卡斯帕罗夫，但是在围棋上人工智能的表现还很弱。
2016 年 3 月，由 google Deepmind 研发的 ALphaGo 程序 4-1 击败了李世石，并且在一年后 AlphaGo Zero 又以 100-0 击败了它的前辈——毫无疑问，人类世界已经没有他的对手，柯洁也差得远。

Alpha Go / Zero系统将几种方法组合成一个伟大的工程:

- 蒙特卡罗树搜索
- 残余卷积神经网络 - 用于游戏评估和移动先验概率估计的策略和价值网络
- 用于通过自我游戏训练网络的强化学习

## 蒙特卡洛算法的前身今世

人们对游戏AI的探索始于完美信息博弈游戏（perfect information games），这类游戏的特点就是游戏参与者没有隐藏的信息（例如英雄联盟等moba类游戏的战争迷雾），也没有任何不确定因素（比如玩大富翁的时候要掷色子）。这类游戏显然是最好研究的（像星际争霸这样的游戏AI至今还没有战胜人类）。因为在游戏中的每一步都是确定的，所以理论上，我们是可以穷举所有的情况从而构建一颗“游戏树”（如下图井字棋的游戏树），我们可以按照依次取最大值最小值的原则不断向下搜索（因为敌我双方的目的是相反的），这种方法就叫做 Minmax 方法。

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9nc3MyLmJkc3RhdGljLmNvbS85Zm8zZFNhZ194STRraEdrcG9XSzFIRjZoaHkvYmFpa2UvdyUzRDI2OC9zaWduPTM1MzJkYzk4ODAyNmNmZmM2OTJhYjhiNDgxMDA0YTdkLzFjOTUwYTdiMDIwODdiZjQ4OGE2ZmRlOGYwZDM1NzJjMTFkZmNmNmQuanBn?x-oss-process=image/format,png)

然而，这种方法要求我们要穷举所有可能的情况，这在绝大多数的游戏中显然是不现实的，因此人们又想到寻求一个函数对当前局面进行判断，在树扩展的过程中及时对这棵树进行剪枝，这种方法在国际象棋这个领域取得了一定成果，但是对大部分游戏，依然不行。

而在此类领域，MCTS体现其强大的能力。我们想象，有两个对围棋一无所知的人，小明和小白，他们虽然不知道怎么下才能赢，但是他们依然遵守规则坚持不懈的下棋。经过一番激烈的菜鸡互啄，小明最终赢了小白，这个时候小明对围棋的认识加深了一步——“原来第一步要下天元才能赢啊！”（这不正确！），小白对围棋的认识也加深了一步——“第一步下在棋盘边缘上会输啊！”，小明和小白继续下棋，下到天荒地老，他们二人对围棋的认识都在不断加深，好多好多年过去了，小明赢了 6000 盘，小白赢了4000 盘，那么我们就可以认为，在这种情况下小明胜率 60%，小白胜率 40%。这个例子告诉我们什么呢？大力出奇迹啊！通过大量的模拟，我们可以提高对某一种事物的认识，并对当前局面有一个更好的估计，这就是MCTS的基本思想，下面我们对其原理和步骤进行进一步介绍。

在此之前，棋类 AI 基本上使用的是博弈树算法——但是用在围棋上，效果很差，原因有两个：

1. 棋局评判能力要求更高
   棋局的评判一般使用估值函数来评估，国际象棋的棋局局面特征比较明显，最容易想到的是可以给每个棋子和位置设置不同的分值，如果棋子之间的保护关系等特征，对局面的评价就已经很靠谱了。而对于围棋上述方法基本不起任何作用。
2. 计算能力要求更高
   首先，国际象棋的棋盘大小为 64，围棋的大小为 361。由于棋盘大小的不同，每走一步国际象棋和围棋的计算量的要求是不一样的，围棋明显要求更高。这在博弈论中一般称之为分支因子，即平均每个落子后的合法走法，国际象棋的分支因子约为 35，而围棋大约是 250。另外一个可以说明计算能力要求不同的指标是搜索空间，在该指标上两者也存在指数级的差异，国际象棋是  10^50，而围棋是 10^171。 我们知道宇宙中的原子总数总共大约也才10^80，因此围棋的搜索空间绝对算是天文数字，  已经不能用千千万来形容了。
   不过说到千千万，我就想起孙悟空的头发…下半年…

### 蒙特卡洛方法

首先我们聊聊「蒙特卡洛方法」，注意！这和蒙特卡洛树搜索不是同一种算法，很多科普文章都搞混了这两个概念，声称 AlphaGO 使用的是蒙特卡洛方法。
蒙特卡洛法方法是什么呢，它是评判棋盘局面的一种方法，我们上面说到，围棋很难写出好的估值函数，于是上世纪有人提出了一种神奇的方法：双方在某个局面下「随机」走子，注意是「随机」走，走到终局或者残局为止，随机很多次（比如一万盘），计算胜率，胜率越高的局面就越好。
这个方法是我高三的时候在「围棋世界」上看到的，那里有篇文章讲 alphago 用的就是这种方法，我当时如获至宝，因为这种方法也太太太美妙了，不用写极其复杂的估值函数，直接随机很多很多盘棋，多美妙！然后我们就套用前一篇文章我们将的博弈树的最大最小算法，完美!
但其实这是个**伪算法**，就举个极端的例子，比如说我下某步棋之后，对方有 100 种应对—— 99 种会导致劣势，但是有 1 种必胜下法，我就绝对不能下这步棋。
但是「蒙特卡洛树搜索」是个真算法，并且它其实在 alphago 之前早就有了，而且能胜业余的段级选手，在当时是很大的突破。



## 蒙特卡洛搜索算法的原理

蒙特卡洛树搜索（简称 MCTS）是 Rémi Coulom 在 2006 年在它的围棋人机对战引擎 「Crazy Stone」中首次发明并使用的的 ，并且取得了很好的效果。我们先讲讲它用的原始 MCTS 算法（ALphago 有部分改进） 蒙特卡洛树搜索，首先它肯定是棵搜索树。

### Exploration and Exploitation（探索与利用）

这个在强化学习经常遇到的一个困境和难题。回到我们上面提到的小明和小白下围棋的例子，什么是“Exploration”？
探索就是向未知的领域勇敢的进发。
如果小明不尝试新的招法，他永远不会发现，开局最好的下法不是下天元，是四个角星位附近。
什么是“Exploitation”？利用就是经验万岁。
经验有时候虽然不靠谱，但大多数时候还是管用的，而且你下的越多，你的经验就越靠谱。
探索也好，利用也罢，怎么去确定他们的分配比例呢？
这就是难题所在了，我们可以设置一个概率参数 p，以 p 的概率探索，以 1-p 的概率利用。
还有很多种方法，我们下面要介绍的 UCB 也是其中的一种。

### Upper Confidence Bounds（UCB）

在本文中，我们采用 UCB 方法来确定何时进行探索和利用。

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy80Mjc0MS01Mzk5ZTE0YzUxY2IxNTIxLnBuZw?x-oss-process=image/format,png)

其中 $v_i$ 是节点估计的值（比如胜率），$n_i$ 是节点被访问的次数，而 $N$ 则是其父节点已经被访问的总次数。$C$ 是可调整参数。

前者代表我们的经验，而后者代表我们的勇气。我们重点看一下我们的“勇气”。后面的值越大，代表着相对父节点的访问，我们访问当前这个子节点的次数偏少，因此我们要多多关注它，反之，则正好相反。

### 蒙特卡罗搜索的基本操作

![](https://pic1.zhimg.com/80/v2-b958662a0be8daf52bea1cd735a7575c_720w.jpg)

算法的每次迭代分为四步——**选择**，**扩展**，**模拟**和**反向传播**。我们先简单介绍一下这四步，后文我们会给一个详细迭代的例子。

#### 选择

从根节点开始，我们选择采用 UCB 计算得到的最大的值的孩子节点，如此向下搜索，直到我们来到树的底部的叶子节点（没有孩子节点的节点），等待下一步操作。

#### 扩展

到达叶子节点后，如果还没有到达终止状态（比如五子棋的五子连星），那么我们就要对这个节点进行扩展，扩展出一个或多个节点（也就是进行一个可能的 action 然后进入下一个状态）。

#### 模拟

之后，我们基于目前的这个状态，根据某一种策略（例如 random policy）进行模拟，直到游戏结束为止，产生结果，比如胜利或者失败。

#### 反向传播

根据模拟的结果，我们要自底向上，反向更新所有节点的信息，具体更新哪些信息在后面示例和实现中会讲。

#### 算法什么时候可以终止

取决于你什么时候想让他停止，比如说你可以设定一个时间，比如五秒后停止计算。
一般来说最佳走法就是**具有最高访问次数**的节点，这点可能稍微有点反直觉。这样评估的原因是因为蒙特卡洛树搜索算法的核心就是，**越优秀的节点，越有可能走**，反过来就是，**走得越多的节点，越优秀**。

### 蒙特卡洛搜索算法的流程图

![](https://img-blog.csdn.net/20171024211039397?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGp5dDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## Reference

- [面向初学者的蒙特卡洛树搜索MCTS详解及其实现](https://blog.csdn.net/caozixuan98724/article/details/103213795)
- [蒙特卡洛树搜索最通俗入门指南](https://zhuanlan.zhihu.com/p/53948964)