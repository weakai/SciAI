# 学习率

#### [从动力学角度看优化算法（五）：为什么学习率不宜过小？](https://kexue.fm/archives/7787)

有限的学习率隐式地给优化过程带来了梯度惩罚项，而这个梯度惩罚项对于提高泛化性能是有帮助的，因此哪怕不考虑算力和时间等因素，也不应该用过小的学习率。

藏在学习率中的正则

学习率不宜过小，较大的学习率不仅有加速收敛的好处，还有提高模型泛化能力的好处
当然，可能有些读者会想，我直接把梯度惩罚加入到 loss 中，是不是就可以用足够小的学习率了？理论上确实是的，原论文将梯度惩罚加入到 loss 中的做法，称为“显式梯度惩罚”。

