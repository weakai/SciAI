---
title: 知识蒸馏
---

蒸馏训练？

目前主流的压缩方法有裁剪、量化、知识蒸馏等。
其中知识蒸馏这一概念是由Hinton等人在2015年发表的《Distilling the Knowledge in a Neural Network》论文中提出的一个黑科技，一种非常经典的模型压缩技术，是将知识从一个复杂模型(Teacher)迁移到另一个轻量级模型(Student)上的方式来实现模型压缩。

其实所谓知识的迁移，其实可以理解为一种训练过程，就是使用Teacher模型来训练Student模型，这种训练方法就是**蒸馏训练**。在训练出一个效果良好的Student模型后，这个Student模型就可以被用于实际部署了。

![知识蒸馏架构图](http://p26-tt.byteimg.com/large/pgc-image/da223e2319a64621a5755e3cae4c6d0b)

所谓Teacher模型的知识是指Teacher模型的推理结果，我们称之为soft label，这个soft label将作为Student网络的训练目标，Student的推理结果需要尽可能接近Teacher的推理结果。与soft label相对应的是hard label，hard label就是真实训练数据的标签。相比于hard label，soft label所含的信息量更大。

举个例子，比如做区分驴和马的分类任务的时候，soft label不会像hard label那样只给马的index值为1，其余类别为0，而是在驴的部分也会提供一个概率值（例如0.3或0.4之类），这样的优势在于使soft label包含了不同类别之间的相似性信息。显而易见，使用soft label训练出来的模型肯定要比单独使用hard label训练出来的模型学习到更多的知识，也就更加的优秀。

知识蒸馏训练的目标函数可由distillation loss（对应teacher soft label）和student loss（对应标注的hard label）加权得到。公式如下，其中p表示Student模型的推理结果，q为teacher的推理结果，y为hard label。

$$L=\alpha L_{soft}+\beta L_{hard}=\alpha CrossEntropy(p,q) + \beta CrossEntropy(p,y)$$

## 什么是服务型蒸馏训练？




## 参考

[关于teacher-student(知识蒸馏)的一些思考与总结](https://blog.csdn.net/guoyuhaoaaa/article/details/81433830)

[模型也可以上网课？！一文看懂服务型蒸馏训练方案](https://www.qbitai.com/2020/07/16178.html)