---
title: bleu
---

[浅谈BLEU评分](https://coladrill.github.io/2018/10/20/%E6%B5%85%E8%B0%88BLEU%E8%AF%84%E5%88%86/)

双语评估替换 Bilingual Evaluation Understudy

是一种对`生成语句`进行`评估的指标`，用于比较候选文本翻译与其他一个或多个参考翻译的评价分数，评估自动机器翻译系统的预测结果

[2002《BLEU: a Method for Automatic Evaluation of Machine Translation》](http://www.aclweb.org/anthology/P02-1040.pdf)

- 优点：计算速度快、计算成本低、容易理解、与具体语言无关、和人类给的评估高度相关。
- 缺点：不考虑语言表达（语法）上的准确性；测评精度会受常用词的干扰；短译句的测评精度有时会较高；没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定。

除了翻译之外，BLEU评分结合深度学习方法可应用于其他的语言生成问题，例如：语言生成、图片标题生成、文本摘要、语音识别。

## n-gram precision

a sentence has 18 words means that it has 18 1-gram and 17 2-gram

precision: 指 Candidate 语句里面的 n-gram 在所有 Reference 语句里面出现的概率

modified n-gram precision: Reference 里的单词会被重复使用，故需要排除修正

## modified n-gram recision

主要思路是Reference语句里面如果一个单词片段已经被匹配，那么这个片段就不能再次被匹配，并且一个单词片段只能取一个Reference语句中出现次数的最大值，比如7个the分别在Reference 1 和 2中出现2和1次，所以取2而不是两者相加的3。

利用以上方法，每一个句子都可以得到一个 modified n-gram recision，一个句子不能代表文本翻译的水平高低，于是把一段话或者所有翻译句子的结果综合起来可以得到 $n_p$

![pn](https://coladrill.github.io/img/post/20181020/1.png)

简而言之，就是把所有句子的modified n-gram precision的分子加起来除以分母加起来。

## BP值和BLEU值

对于不同的长度n都会有一个pn，那么如何将不同n的pn结合起来得到最终的Bleu值。研究者们还考虑到一种情况，就是待测译文翻译不完全不完整的情况，这个问题在机器翻译中是不能忽略的，而简单的pn值不能反映这个问题

引入BP值(`Brevity Penalty`)，作者指定当待评价译文同任意一个参考译文长度相等或超过参考译文长度时，BP值为1，当待评价译文的长度较短时，则用一个算法得出BP值。

以c来表示待评价译文的长度，r来表示参考译文的文字长度，则

![bp](https://coladrill.github.io/img/post/20181020/2.png)

BLEU值(`Bilingual Evaluation Understudy`)计算为

![bleu](https://coladrill.github.io/img/post/20181020/3.png)

在对数情况下，计算变得更加简便

![logblue](https://coladrill.github.io/img/post/20181020/4.png)

通常这个N取4，wn=1/4，这就是很多论文里面的一个经典指标Bleu4

## Chrf

[聊聊机器翻译界的“灌水与反灌水之战”！](https://posts.careerengine.us/p/613cd5bdc25eb56b1397f501)

[ACL2021 Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://aclanthology.org/2021.acl-long.566.pdf)

过去10年间涌现了大量提升机器翻译性能的算法，这些算法通过与前人的模型对比自动评测指标，比如BLEU分数值，来凸显其性能。随着论文报告的分数值越来越高，我们不禁要问，这些论文的评测方式真的有说服力吗？算法真的有效吗？整个社区是朝着良性的方向发展，还是灌水之风日渐严重？

正在消失的统计显著性检验

在机器翻译中，统计显着性检验已用于自动度量分数，更具体地说，用于评估两个 MT 系统之间度量分数的特定差异是否不是巧合。

Dror 等人 (2018) 指出，虽然 NLP 社区非常重视实验结果，但很少使用统计显着性检验。团队根据对 A3 的分析验证了这是否适用于 MT 评估。

通过对769篇论文的评估，作者揭露了当前机器翻译评测中令人担忧的**4个陷阱**。

1. 称霸机器翻译的BLEU

2. 被遗忘的统计显著性检验
   - 人们越来越不喜欢使用这个检验，即使它可以显著的提升论文可信度
3. 一直copy一直爽
   - 越来越多的论文更加倾向于直接复制实验结果而不是复现相关实验，这在2015年以后显得尤为明显；拷贝结果的确可以省时省力，但引发的问题是：那些论文是否提供了足够信息，以确保它取得的分值和前人报告的结果具有可比性
   - 针对模型输出结果进行后处理的操作包括：是否完全小写化、是否标点规范化、是否进行tokenize处理。实验结果如上表所示，不同的后处理方式对自动评测结果有很大的影响
4. 评测中数据的"艺术"
   - 机器翻译领域论文大多提出新算法以提高翻译准确度（因变量），而评测新的算法对因变量的影响时，需要保持其他所有自变量（例如数据集）不变，否则无法保证算法性能的可信度。
   - **越来越多的论文在进行对比实验时使用了不一致数据**，在这种设定下，我们无法判断出性能的提升到底是因为算法的优越还是数据的"艺术"。

## 反击灌水的攻与防

- 无论自动评测指标分值提高有多大，都应该尽量进行统计显著性检验
- 不应该仅使用BLEU作为评测指标，也需要结合其他更加合适的自动评测指标及人工评测
- 尽量不要直接拷贝别人的实验结果，如果不可避免，要保证结果具有可比性
- 要保证所有的数据集以及预处理方式一致